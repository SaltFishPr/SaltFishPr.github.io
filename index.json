[{"categories":["micro service"],"content":"Consul, æœåŠ¡ç½‘æ ¼çš„æ§åˆ¶å¹³é¢","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"æœåŠ¡ç½‘æ ¼ æœåŠ¡ç½‘æ ¼æ˜¯ä¸ºæœ¬åœ°ã€äº‘æˆ–æ··åˆäº‘åŸºç¡€è®¾æ–½æä¾›å®‰å…¨çš„æœåŠ¡åˆ°æœåŠ¡é€šä¿¡çš„ä¸“ç”¨å±‚ã€‚æœåŠ¡ç½‘æ ¼é€šå¸¸ä¸å¾®æœåŠ¡æ¶æ„æ¨¡å¼ä¸€èµ·ä½¿ç”¨ï¼Œä½†å¯ä»¥åœ¨æ¶‰åŠå¤æ‚ç½‘ç»œçš„ä»»ä½•åœºæ™¯ä¸­æä¾›ä»·å€¼ã€‚ æœåŠ¡ç½‘æ ¼é€šå¸¸ç”±æ§åˆ¶å¹³é¢å’Œæ•°æ®å¹³é¢ç»„æˆã€‚æ§ä»¶å¹³é¢ç»´æŠ¤ä¸€ä¸ªä¸­å¤®æ³¨å†Œè¡¨ï¼Œå¯è·Ÿè¸ªæ‰€æœ‰æœåŠ¡åŠå…¶å„è‡ªçš„ IP åœ°å€ï¼Œè¿™è¢«ç§°ä¸ºæœåŠ¡å‘ç°ã€‚åªè¦åº”ç”¨ç¨‹åºåœ¨æ§åˆ¶å¹³é¢æ³¨å†Œï¼Œæ§åˆ¶å¹³é¢å°±å¯ä»¥ä¸æœåŠ¡ç½‘æ ¼çš„å…¶ä»–æˆå‘˜å…±äº«å¦‚ä½•ä¸åº”ç”¨ç¨‹åºé€šä¿¡å¹¶å¼ºåˆ¶è§„å®šè°èƒ½å½¼æ­¤é€šä¿¡ã€‚ æ§åˆ¶å¹³é¢è´Ÿè´£ä¿æŠ¤ç½‘æ ¼ï¼Œä¿ƒè¿›æœåŠ¡å‘ç°ã€å¥åº·æ£€æŸ¥ã€ç­–ç•¥æ‰§è¡Œå’Œå…¶ä»–ç±»ä¼¼çš„æ“ä½œé—®é¢˜ã€‚æ•°æ®å¹³é¢å¤„ç†æœåŠ¡ä¹‹é—´çš„é€šä¿¡ã€‚è®¸å¤šæœåŠ¡ç½‘æ ¼è§£å†³æ–¹æ¡ˆé‡‡ç”¨ sidecar ä»£ç†æ¥å¤„ç†æ•°æ®å¹³é¢é€šä¿¡ï¼Œå› æ­¤é™åˆ¶äº†æœåŠ¡å¯¹ç½‘ç»œç¯å¢ƒçš„æ„ŸçŸ¥æ°´å¹³ã€‚ ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:1:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"æœåŠ¡å‘å±• ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:2:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"å•ä½“æœåŠ¡ æ¨¡å—é—´å‡½æ•°è°ƒç”¨ï¼Œä»¥çº³ç§’ä¸ºå•ä½ å‚æ•°ä¼ é€’ï¼Œæ•°æ®æ— éœ€åŠ å¯† å¯¼å…¥æ¨¡å—ï¼Œå¢åŠ åŠŸèƒ½ å­ç³»ç»Ÿå‡ºç°é—®é¢˜ï¼Œå¾—é‡æ–°å‘å¸ƒæ•´ä¸ªç³»ç»Ÿ ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:2:1","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"å¾®æœåŠ¡ ä¸ºäº†æå‡å¼€å‘æ•ˆç‡ï¼Œé™ä½ç‰¹æ€§å‘å¸ƒå‘¨æœŸï¼Œå¼€å§‹èµ°å‘å¾®æœåŠ¡æ¶æ„ æ¨¡å—é—´ç½‘ç»œè°ƒç”¨ï¼Œä»¥æ¯«ç§’ä¸ºå•ä½ æ•°æ®åœ¨ç½‘ç»œä¸­ä¼ è¾“ï¼Œéœ€è¦åŠ å¯† è®¿é—® IP:Port è·å–æœåŠ¡ æŒ‘æˆ˜ï¼š æœåŠ¡å‘ç° æœåŠ¡é…ç½® æœåŠ¡åˆ†ç•Œï¼šå¤–éƒ¨ï¼Œä¸šåŠ¡ï¼Œæ•°æ® æœåŠ¡åœ°å€ç”±è´Ÿè½½å‡è¡¡å™¨ï¼ˆå¦‚ nginxï¼‰ç¡¬ç¼–ç ï¼Œå¾®æœåŠ¡é€šè¿‡è´Ÿè½½å‡è¡¡å™¨åœ°å€è°ƒç”¨å…¶ä¾èµ–çš„æœåŠ¡ ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:2:2","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"æœåŠ¡ç½‘æ ¼ æœåŠ¡å‘ç°ï¼šå¾®æœåŠ¡æœåŠ¡åœ¨ä¸­å¤®æœåŠ¡è¿›è¡Œæ³¨å†Œ æœåŠ¡é…ç½®ï¼šä¸­å¤®æœåŠ¡ K/V æ•°æ®åº“å­˜å‚¨é…ç½® æœåŠ¡åˆ†ç•Œï¼šä¸­å¤®æœåŠ¡å®šä¹‰å¾®æœåŠ¡ä¹‹é—´çš„é€šä¿¡è§„åˆ™ï¼ˆService Graphï¼‰ æœåŠ¡é‰´æƒï¼šä¸åŒçš„æœåŠ¡æœ‰ä¸åŒçš„ TLS è¯ä¹¦ï¼ŒæœåŠ¡é—´è¿›è¡ŒåŒå‘ TLS é‰´æƒ sidecar proxy ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:2:3","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"éƒ¨ç½² Consul å‰ç½®è¦æ±‚ï¼š docker minikube helm å…‹éš†ç¤ºä¾‹å­˜å‚¨åº“ git clone https://github.com/hashicorp/learn-consul-kubernetes.git --depth=1 åˆ‡æ¢åˆ°æ•™ç¨‹ç›®å½• cd learn-consul-kubernetes/service-mesh/deploy æ·»åŠ  chart ä»“åº“ helm repo add hashicorp https://helm.releases.hashicorp.com æ›´æ–° chart ä»“åº“ helm repo update éƒ¨ç½² consul helm install -f config.yaml consul hashicorp/consul --create-namespace -n consul --version \"0.39.0\" éªŒè¯å®‰è£… kubectl get pods --namespace consul --selector app=consul Output: NAME READY STATUS RESTARTS AGE consul-bz8xw 1/1 Running 0 23m consul-connect-injector-webhook-deployment-8d4f5c97b-56vk5 1/1 Running 1 (22m ago) 23m consul-connect-injector-webhook-deployment-8d4f5c97b-lp9zh 1/1 Running 0 23m consul-controller-7b588d978-2thrf 1/1 Running 0 23m consul-server-0 1/1 Running 0 23m consul-webhook-cert-manager-78669db499-wnlq8 1/1 Running 0 23m è½¬å‘ UI ç«¯å£ kubectl --namespace consul port-forward service/consul-ui 18500:80 --address 0.0.0.0 è¿™é‡Œæ¨èä½¿ç”¨ VSCode Remote SSHï¼Œæä¾›çš„ç«¯å£è½¬å‘åŠŸèƒ½å¯ä»¥è½»æ˜“çš„è®¿é—®æœåŠ¡å™¨çš„æœåŠ¡ã€‚ ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:3:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"æ›´æ–° consul é…ç½® ä¿®æ”¹ config.yaml æ–‡ä»¶ global:name:consuldatacenter:dc1metrics:enabled:trueenableAgentMetrics:trueacls:manageSystemACLs:truegossipEncryption:autoGenerate:truetls:enabled:trueenableAutoEncrypt:trueverify:trueserver:replicas:1ui:enabled:trueservice:type:\"NodePort\"connectInject:enabled:truecontroller:enabled:trueprometheus:enabled:true ä½¿ç”¨ helm å‡çº§ helm upgrade consul hashicorp/consul --namespace consul --version \"0.39.0\" --values ./config.yaml --wait ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:4:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"éƒ¨ç½²å¾®æœåŠ¡ éƒ¨ç½²ç¤ºä¾‹æœåŠ¡ ä¾ç„¶æ˜¯åœ¨åˆšåˆšçš„ç›®å½•ä¸‹ï¼Œåº”ç”¨ hashicups æ–‡ä»¶å¤¹ä¸­çš„é…ç½®æ–‡ä»¶ kubectl apply -f hashicups/ Output: service/frontend created serviceaccount/frontend created servicedefaults.consul.hashicorp.com/frontend created configmap/nginx-configmap created deployment.apps/frontend created service/postgres created serviceaccount/postgres created servicedefaults.consul.hashicorp.com/postgres created deployment.apps/postgres created service/product-api created serviceaccount/product-api created servicedefaults.consul.hashicorp.com/product-api created configmap/db-configmap created deployment.apps/product-api created service/public-api created serviceaccount/public-api created servicedefaults.consul.hashicorp.com/public-api created deployment.apps/public-api created æŸ¥çœ‹ pods kubectl get pods --selector consul.hashicorp.com/connect-inject-status=injected Output: NAME READY STATUS RESTARTS AGE frontend-699cb4546-vj78k 2/2 Running 0 25m postgres-54966b4458-7gdk2 2/2 Running 0 25m product-api-688d79df6c-fk7zw 2/2 Running 0 25m public-api-5975bd4f4c-vkhrs 2/2 Running 0 25m è½¬å‘æœåŠ¡ç«¯å£ kubectl port-forward service/frontend 18080:80 --address 0.0.0.0 æµè§ˆå™¨æ‰“å¼€ localhost:18080 ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:5:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"é…ç½®é›¶ä¿¡ä»»ç½‘ç»œ æ‹’ç»æ‰€æœ‰è¯·æ±‚ è¿›å…¥ zero-trust-network æ–‡ä»¶å¤¹ cd service-mesh/zero-trust-network åº”ç”¨è§„åˆ™ kubectl apply -f deny-all.yaml æŸ¥çœ‹ç®¡ç†ç•Œé¢ è®¿é—® http://127.0.0.1:18080/ å¾—åˆ° Error :( å…è®¸ç‰¹å®šæœåŠ¡é—´çš„é€šä¿¡ kubectl apply -f service-to-service.yaml è¯¥æ–‡ä»¶å…è®¸ä¸‹åˆ—è¯·æ±‚çš„å‘ç”Ÿ frontend -\u003e public-api public-api -\u003e product-api product-api -\u003e postgres æŸ¥çœ‹ç®¡ç†ç•Œé¢ intention intention åŒ…å«å››ä¸ªéƒ¨åˆ† æºæœåŠ¡ - æŒ‡å®šå‘å‡ºè¯·æ±‚çš„æœåŠ¡ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªæœåŠ¡çš„å…¨ç§°ï¼Œä¹Ÿå¯ä»¥*æŒ‡ä»£æ‰€æœ‰çš„æœåŠ¡ã€‚ ç›®æ ‡æœåŠ¡ - æŒ‡å®šæ¥æ”¶è¯·æ±‚çš„æœåŠ¡ã€‚å³åœ¨æœåŠ¡å®šä¹‰ä¸­é…ç½®çš„ä¸Šæ¸¸ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªæœåŠ¡çš„å…¨ç§°ï¼Œä¹Ÿå¯ä»¥*æŒ‡ä»£æ‰€æœ‰çš„æœåŠ¡ã€‚ æƒé™ - å®šä¹‰æ˜¯å¦å…è®¸æºå’Œç›®æ ‡ä¹‹é—´çš„é€šä¿¡ã€‚è¿™å¯ä»¥è®¾ç½®ä¸º allow æˆ– denyã€‚ æè¿° - å¯é€‰ï¼Œæè¿° intention çš„å­—æ®µã€‚ apiVersion:consul.hashicorp.com/v1alpha1kind:ServiceIntentionsmetadata:name:product-api-to-postgresspec:destination:name:postgressources:- name:product-apiaction:allow ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:6:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["linux"],"content":"æŸ¥çœ‹ æŸ¥çœ‹ sh è·¯å¾„ which sh /usr/bin/sh æŸ¥çœ‹é»˜è®¤ sh ll /usr/bin/sh lrwxrwxrwx 1 root root 4 Feb 16 21:03 /usr/bin/sh -\u003e dash* ","date":"2022-02-16","objectID":"/posts/2022-02-16-ubuntu-change-sh/:1:0","tags":null,"title":"Ubuntu åˆ‡æ¢é»˜è®¤ sh","uri":"/posts/2022-02-16-ubuntu-change-sh/"},{"categories":["linux"],"content":"åˆ‡æ¢ sudo dpkg-reconfigure dash é€‰æ‹© [æ˜¯] è®¾ç½® sh ä¸º dash é€‰æ‹© [å¦] è®¾ç½® sh ä¸º bash æŸ¥çœ‹ shï¼š ll /usr/bin/sh lrwxrwxrwx 1 root root 4 Feb 16 21:03 /usr/bin/sh -\u003e bash* ","date":"2022-02-16","objectID":"/posts/2022-02-16-ubuntu-change-sh/:2:0","tags":null,"title":"Ubuntu åˆ‡æ¢é»˜è®¤ sh","uri":"/posts/2022-02-16-ubuntu-change-sh/"},{"categories":["golang"],"content":"Go è¯­è¨€æ–‡æ¡£ï¼Œè§„èŒƒï¼Œå¼€æºç»„ä»¶","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/","tags":null,"title":"Go è¯­è¨€èµ„æºæ•´åˆ","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"ç¼–ç¨‹è§„èŒƒ uber-guide: uber go guide clean-go-article: go clean code ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:1","tags":null,"title":"Go è¯­è¨€èµ„æºæ•´åˆ","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"å¸¸ç”¨åŒ… viper: é…ç½®è¯»å– zap: æ—¥å¿—è¾“å‡º lumberjack: æ—¥å¿—æ»šåŠ¨è®°å½•å™¨ validator: æ•°æ®æ ¡éªŒ cast: ç±»å‹è½¬æ¢ lo: å·¥å…·åº“ï¼Œåˆ‡ç‰‡ï¼Œæ˜ å°„ï¼Œå…ƒç»„ï¼Œé›†åˆâ€¦ carbon: æ—¶é—´åº“çš„æ‰©å±• cron: å®šæ—¶ä»»åŠ¡ ants: goroutine æ±  wire: ä¾èµ–æ³¨å…¥ cobra: å‘½ä»¤è¡Œç¨‹åº resty: HTTP/REST å®¢æˆ·ç«¯ testify: æµ‹è¯•åº“ï¼Œæ–­è¨€ã€mock go-funk: Go å®ç”¨å·¥å…·åº“ï¼ˆmapã€findã€containsã€filterã€chunkã€reverseï¼Œâ€¦ï¼‰ jsoniter: æ ‡å‡†åº“ encoding/json çš„å‡çº§ï¼Œå¯ç›´æ¥æ›¿æ¢ï¼Œé«˜æ€§èƒ½ gjson: å¿«é€Ÿä» json ä¸­è·å–å€¼ carbon: ä¸€ä¸ªè½»é‡çº§ã€è¯­ä¹‰åŒ–ã€å¯¹å¼€å‘è€…å‹å¥½çš„ golang æ—¶é—´å¤„ç†åº“ï¼Œæ”¯æŒé“¾å¼è°ƒç”¨ afero: Go çš„æ–‡ä»¶ç³»ç»ŸæŠ½è±¡ fsm: æœ‰é™çŠ¶æ€æœºåº“ fasthttp: æ ‡å‡†åº“ net/http çš„å‡çº§ï¼Œæ›´å¿« agollo: è¿æ¥ apollo é…ç½®ä¸­å¿ƒ msgp: MessagePack åºåˆ—åŒ–ï¼Œæ¯” json æ›´å¿«ï¼Œæ•°æ®é‡æ›´å° redsync: Redis åˆ†å¸ƒå¼é” sqlx: æ ‡å‡†åº“ database/sql çš„æ‰©å±• kafka-go: kafka åº“ ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:2","tags":null,"title":"Go è¯­è¨€èµ„æºæ•´åˆ","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"æ¡†æ¶ rpcx: rpc æ¡†æ¶ gin: web æ¡†æ¶ fiber: web æ¡†æ¶ iris: HTTP/2 web æ¡†æ¶ cooly: çˆ¬è™«æ¡†æ¶ watermill: äº‹ä»¶é©±åŠ¨æ¡†æ¶ ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:3","tags":null,"title":"Go è¯­è¨€èµ„æºæ•´åˆ","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"å…¶å®ƒ fync: GUI bubbletea: ç»ˆç«¯åº”ç”¨ TUI plot: ç»˜å›¾ smocker: HTTP mock server primitive: ä½¿ç”¨å‡ ä½•å½¢çŠ¶å°†å›¾ç‰‡å˜ä¸ºæŠ½è±¡ç”» ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:4","tags":null,"title":"Go è¯­è¨€èµ„æºæ•´åˆ","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"å·¥å…· golines: ä»£ç æ ¼å¼åŒ–ï¼Œå¤„ç†è¡Œè¿‡é•¿çš„è¡Œ å®‰è£…ï¼šgo install github.com/segmentio/golines@latest ä½¿ç”¨ï¼šgolines -w . ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:5","tags":null,"title":"Go è¯­è¨€èµ„æºæ•´åˆ","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"Go è¯­è¨€ä»‹ç»ä¸å…¥é—¨","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"ç®€ä»‹ Go æ˜¯ Google å¼€å‘çš„ä¸€ç§é™æ€å¼ºç±»å‹ã€ç¼–è¯‘å‹ã€å¹¶å‘å‹ï¼Œå¹¶å…·æœ‰åƒåœ¾å›æ”¶åŠŸèƒ½çš„ç¼–ç¨‹è¯­è¨€ã€‚äº 2009 å¹´ 11 æœˆæ­£å¼å®£å¸ƒæ¨å‡ºï¼Œ 1.0 ç‰ˆæœ¬åœ¨ 2012 å¹´ 3 æœˆå‘å¸ƒã€‚ä¹‹åï¼ŒGo å¹¿æ³›åº”ç”¨äº Google çš„äº§å“ä»¥åŠè®¸å¤šå…¶ä»–ç»„ç»‡å’Œå¼€æºé¡¹ç›®ã€‚Go ç¨‹åºå‘˜å¸¸å¸¸è¢«ç§°ä¸ºåœ°é¼ ï¼ˆgopherï¼‰ï¼Œå› æ­¤åœ°é¼ ä¹Ÿæ˜¯ Go çš„å‰ç¥¥ç‰©ã€‚ åœ¨ Go è¯­è¨€å‡ºç°ä¹‹å‰ï¼Œå¼€å‘è€…ä»¬æ€»æ˜¯é¢ä¸´éå¸¸è‰°éš¾çš„æŠ‰æ‹©ï¼Œç©¶ç«Ÿæ˜¯ä½¿ç”¨æ‰§è¡Œé€Ÿåº¦å¿«ä½†æ˜¯ç¼–è¯‘é€Ÿåº¦å¹¶ä¸ç†æƒ³çš„è¯­è¨€ï¼ˆå¦‚ï¼šC++ï¼‰ï¼Œè¿˜æ˜¯ä½¿ç”¨ç¼–è¯‘é€Ÿåº¦è¾ƒå¿«ä½†æ‰§è¡Œæ•ˆç‡ä¸ä½³çš„è¯­è¨€ï¼ˆå¦‚ï¼š.NETã€Javaï¼‰ï¼Œæˆ–è€…è¯´å¼€å‘éš¾åº¦è¾ƒä½ä½†æ‰§è¡Œé€Ÿåº¦ä¸€èˆ¬çš„åŠ¨æ€è¯­è¨€å‘¢ï¼Ÿæ˜¾ç„¶ï¼ŒGo è¯­è¨€åœ¨è¿™ 3 ä¸ªæ¡ä»¶ä¹‹é—´åšåˆ°äº†æœ€ä½³çš„å¹³è¡¡ï¼šå¿«é€Ÿç¼–è¯‘ï¼Œé«˜æ•ˆæ‰§è¡Œï¼Œæ˜“äºå¼€å‘ã€‚ ç‰¹æ€§ï¼š ä¸ºå¹¶å‘è®¾è®¡ï¼šåç¨‹ï¼ˆgoroutineï¼‰ã€é€šé“ï¼ˆchannelï¼‰ã€åŒæ­¥åŸè¯­ï¼ˆæ ‡å‡†åº“ sync åŒ…ï¼‰ï¼š èŠ‚çœå†…å­˜ã€ç¨‹åºå¯åŠ¨å¿«å’Œä»£ç æ‰§è¡Œé€Ÿåº¦å¿« è‰¯å¥½çš„è·¨å¹³å°æ€§ å¼€æº å…¶å®ƒå¸å¼•æˆ‘çš„åœ°æ–¹ï¼š 25 ä¸ªä¿ç•™å…³é”®å­—ï¼Œå­¦ä¹ å¿«ï¼Œä¸Šæ‰‹å¿« go å·¥å…·é“¾ï¼Œæä¾›æ„å»ºã€æ‰§è¡Œã€ä¾èµ–ç®¡ç†ã€ä»£ç æ£€æŸ¥ã€ä»£ç æ ¼å¼åŒ–ç­‰åŠŸèƒ½ ä»£ç å¯è¯»æ€§é«˜ï¼Œå¤§å®¶ä»£ç é£æ ¼ä¸€è‡´ ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:1:0","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"Hello World åªä½¿ç”¨æ ‡å‡†åº“æ„å»º http æœåŠ¡ã€‚ Hello World å‘æ¯ä¸ªè®¿é—®çš„å®¢æˆ·ç«¯æ‰“å°ä¸­æ–‡çš„â€œä½ å¥½, ä¸–ç•Œ!â€å’Œå½“å‰çš„æ—¶é—´ä¿¡æ¯ã€‚ package main import ( \"fmt\" \"log\" \"net/http\" \"time\" ) func main() { fmt.Println(\"Please visit http://127.0.0.1:12345/\") http.HandleFunc(\"/\", func(w http.ResponseWriter, req *http.Request) { s := fmt.Sprintf(\"ä½ å¥½, ä¸–ç•Œ! -- Time: %s\", time.Now().String()) fmt.Fprintf(w, \"%v\\n\", s) log.Printf(\"%v\\n\", s) }) if err := http.ListenAndServe(\":12345\", nil); err != nil { log.Fatal(\"ListenAndServe: \", err) } } è¿è¡Œ Hello Worldï¼š ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:2:0","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"æ€§èƒ½æ¯”è¾ƒ CPU å¯†é›†å‹å·¥ä½œè´Ÿè½½ï¼šæ­£åˆ™è¡¨è¾¾å¼è®¡ç®— å››æ ¸ 3.0GHz IntelÂ®i5-3330Â® CPU 15.8 GiB RAM 2TB SATA ç£ç›˜é©±åŠ¨å™¨ Ubuntuâ„¢ 21.04 x86_64 GNU/Linux 5.11.0-18-generic è¯­è¨€ è¿è¡Œæ—¶é—´ ä½¿ç”¨å†…å­˜(byte) æºç å¤§å°(byte) æ€»æ‰§è¡Œæ—¶é—´ cpu è´Ÿè½½ Go 3.85 324,200 810 6.01 27% 19% 20% 91% Java 5.31 793,572 929 17.50 79% 78% 83% 89% web æ¡†æ¶å¯¹æ¯”ï¼šJava(Spring) vs Go(Fiber) æµ‹è¯•å†…å®¹åŒ…æ‹¬ JSON åºåˆ—åŒ–(JSON)ã€å•æŸ¥è¯¢(1-query)ã€æ•°æ®æ›´æ–°(Updates)å’Œçº¯æ–‡æœ¬è¿”å›(Plaintext)çš„å³°å€¼æ€§èƒ½ï¼Œå•ä½ response/second ã€‚ä»¥åŠå“åº”æ—¶å»¶ã€‚ ç‰©ç†æœºå™¨ï¼š IntelÂ® Xeon Gold 5120 CPU 32 GB å†…å­˜ ä¼ä¸šçº§ SSD æ€ç§‘ä¸‡å…†ä»¥å¤ªç½‘äº¤æ¢æœº Framework è¿”å› json å•æŸ¥è¯¢ æ•°æ®æ›´æ–° è¿”å›çº¯æ–‡æœ¬ å¹³å‡æ—¶å»¶ æœ€å¤§æ—¶å»¶ fiber 1,317,695 395,902 11,806 6,413,651 0.8ms 20.3ms spring 150,259 127,114 10,498 183,737 14.3ms 128.1ms äº‘ä¸Šéƒ¨ç½²ï¼š Microsoft Azure D3v2 å®ä¾‹ åƒå…†ä»¥å¤ªç½‘ Framework è¿”å› json å•æŸ¥è¯¢ æ•°æ®æ›´æ–° è¿”å›çº¯æ–‡æœ¬ å¹³å‡æ—¶å»¶ æœ€å¤§æ—¶å»¶ fiber 167,451 52,125 1,888 1,135,808 0.7ms 17.2ms spring 20,140 18,899 1,420 28,780 11.0ms 248.9ms ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:3:0","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"Go å·¥å…·é“¾å®‰è£…ä¸ç®€ä»‹ windows ç¯å¢ƒä¸‹ï¼šä¸‹è½½å®‰è£…æ–‡ä»¶å¹¶è¿è¡Œã€‚ ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:4:0","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"æ£€æµ‹å®‰è£… è¾“å…¥ï¼š go version è¾“å‡ºï¼š go version go1.17.2 linux/arm64 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:4:1","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"å·¥å…·ä»‹ç» go run: ç¼–è¯‘å¹¶è¿è¡Œæ–‡ä»¶/åŒ… go build: ç¼–è¯‘ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶ï¼ˆé»˜è®¤å½“å‰å¹³å°å½“å‰æ¶æ„ï¼‰ go get: ä»è¿œç¨‹æ‹‰å–åŒ… go modï¼šä¾èµ–ç®¡ç† go install: ç¼–è¯‘ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶å¹¶å°†å…¶ç§»åŠ¨åˆ° $GOPATH/bin ä¸­ï¼Œå¯ä»¥æ˜¯è¿œç¨‹åŒ… go test: æ‰§è¡Œæµ‹è¯• go fmt: ä»£ç æ ¼å¼åŒ– go vet: ä»£ç æ£€æŸ¥ go env: æŸ¥çœ‹/è®¾ç½® Go ç¯å¢ƒå˜é‡ ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:4:2","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"Go åŸºç¡€ ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:0","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"25 ä¸ªå…³é”®å­— package, import, const, var, type, func ç”¨æ¥å£°æ˜å„ç§ä»£ç å…ƒç´  interface, chan, map å’Œ struct ç”¨åš ä¸€äº›ç»„åˆç±»å‹çš„å­—é¢è¡¨ç¤ºä¸­ break, case, continue, default, else, fallthrough, for, goto, if, range, return, select å’Œ switch ç”¨åœ¨æµç¨‹æ§åˆ¶è¯­å¥ä¸­ defer å’Œ go ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯æµç¨‹æ§åˆ¶å…³é”®å­— ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:1","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"ç±»å‹ç³»ç»Ÿ åŸºæœ¬ç±»å‹ å­—ç¬¦ä¸²ï¼šstring å¸ƒå°”ï¼šbool æ•°å€¼ï¼š uint8(byte), uint16, uint32, uint64, int8, int16, int32(rune), int64 float32, float64 complex64, complex128 uintptrã€int ä»¥åŠ uint ç±»å‹çš„å€¼çš„å°ºå¯¸ä¾èµ–äºå…·ä½“ç¼–è¯‘å™¨å®ç°ã€‚åœ¨ 64 ä½çš„æ¶æ„ä¸Šï¼Œint å’Œ uint ç±»å‹çš„å€¼æ˜¯ 64 ä½çš„ï¼›åœ¨ 32 ä½çš„æ¶æ„ä¸Šï¼Œå®ƒä»¬æ˜¯ 32 ä½çš„ã€‚ç¼–è¯‘å™¨å¿…é¡»ä¿è¯ uintptr ç±»å‹çš„å€¼çš„å°ºå¯¸èƒ½å¤Ÿå­˜ä¸‹ä»»æ„ä¸€ä¸ªå†…å­˜åœ°å€ã€‚ ç»„åˆç±»å‹ æŒ‡é’ˆç±»å‹ ç»“æ„ä½“ç±»å‹ å‡½æ•°ç±»å‹ å®¹å™¨ç±»å‹ æ•°ç»„ï¼šå®šé•¿å®¹å™¨ åˆ‡ç‰‡ï¼šåŠ¨æ€é•¿åº¦å®¹å™¨ æ˜ å°„ï¼šåˆç§°å­—å…¸ç±»å‹ï¼Œåœ¨æ ‡å‡†ç¼–è¯‘å™¨ä¸­æ˜ å°„æ˜¯ä½¿ç”¨å“ˆå¸Œè¡¨å®ç°çš„ é€šé“ç±»å‹ æ¥å£ç±»å‹ ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:2","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"ä¸€ä¸ªç®€å•çš„æ —å­ æ•°æ®ç±»å‹ï¼Œå¸¸é‡ã€å˜é‡å£°æ˜ï¼Œæµç¨‹æ§åˆ¶è¯­å¥ // å•è¡Œæ³¨é‡Š /* å¤šè¡Œ æ³¨é‡Š */ // å¯¼å…¥åŒ…çš„å­å¥åœ¨æ¯ä¸ªæºæ–‡ä»¶çš„å¼€å¤´ã€‚ // mainæ¯”è¾ƒç‰¹æ®Šï¼Œå®ƒç”¨æ¥å£°æ˜å¯æ‰§è¡Œæ–‡ä»¶ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªåº“ã€‚ package main // importè¯­å¥å£°æ˜äº†å½“å‰æ–‡ä»¶å¼•ç”¨çš„åŒ…ã€‚ import ( \"fmt\" // Goè¯­è¨€æ ‡å‡†åº“ä¸­çš„åŒ… \"os\" // ç³»ç»Ÿåº•å±‚å‡½æ•°ï¼Œå¦‚æ–‡ä»¶è¯»å†™ ) // mainæ˜¯ç¨‹åºæ‰§è¡Œçš„å…¥å£ã€‚ func main() { // å‘æ ‡å‡†è¾“å‡ºæ‰“å°ä¸€è¡Œã€‚ fmt.Println(\"Hello, ä¸–ç•Œ\") // è°ƒç”¨å½“å‰åŒ…çš„å¦ä¸€ä¸ªå‡½æ•°ã€‚ beyondHello() } // å£°æ˜å‡½æ•°`func functionName(parameter type, [parameter type]) ([resultValue] type, [[resultValue] type]) { body }`ã€‚ func beyondHello() { var x int // å˜é‡å£°æ˜ï¼Œå˜é‡å¿…é¡»åœ¨ä½¿ç”¨ä¹‹å‰å£°æ˜ã€‚ x = 3 // å˜é‡èµ‹å€¼ã€‚ y := 4 // å¯ä»¥ç”¨:=æ¥å·æ‡’ï¼Œå®ƒæŠŠå˜é‡ç±»å‹ã€å£°æ˜å’Œèµ‹å€¼éƒ½æå®šäº†ã€‚ fmt.Println(\"x:\", x, \"y:\", y) // ç®€å•è¾“å‡ºã€‚ learnTypes() } // å†…ç½®å˜é‡ç±»å‹å’Œå…³é”®è¯ã€‚ func learnTypes() { str := \"å°‘è¯´è¯å¤šè¯»ä¹¦!\" // Stringç±»å‹ã€‚ s2 := `è¿™æ˜¯ä¸€ä¸ª å¯ä»¥æ¢è¡Œçš„å­—ç¬¦ä¸²` // åŒæ ·æ˜¯Stringç±»å‹ã€‚ // éasciiå­—ç¬¦ã€‚Goä½¿ç”¨UTF-8ç¼–ç ã€‚ g := 'Î£' // runeç±»å‹ f := 3.14195 // float64ç±»å‹ï¼ŒIEEE-754ï¼Œ64ä½æµ®ç‚¹æ•°ã€‚ c := 3 + 4i // complex128ç±»å‹ï¼Œå†…éƒ¨ä½¿ç”¨ä¸¤ä¸ªfloat64è¡¨ç¤ºã€‚ // Varå˜é‡å¯ä»¥ç›´æ¥åˆå§‹åŒ– var u uint = 7 // unsigned æ— ç¬¦å·å˜é‡ï¼Œä½†æ˜¯å®ç°ä¾èµ–intå‹å˜é‡çš„é•¿åº¦ã€‚ var pi float32 = 22. / 7 // å­—ç¬¦è½¬æ¢ n := byte('\\n') // byteæ˜¯uint8çš„åˆ«åã€‚ // æ•°ç»„ï¼ˆarrayï¼‰ç±»å‹çš„å¤§å°åœ¨ç¼–è¯‘æ—¶å³ç¡®å®šã€‚ a3 := [...]int{3, 1, 5} // æœ‰3ä¸ªintå˜é‡çš„æ•°ç»„ï¼ŒåŒæ—¶è¿›è¡Œäº†åˆå§‹åŒ–ã€‚ var a4 [4]int // æœ‰4ä¸ªintå˜é‡çš„æ•°ç»„ï¼Œåˆå§‹ä¸º0ã€‚ // åˆ‡ç‰‡ï¼ˆsliceï¼‰çš„å¤§å°æ˜¯åŠ¨æ€çš„ï¼Œå®ƒçš„é•¿åº¦å¯ä»¥æŒ‰éœ€å¢é•¿ã€‚ // ç”¨å†…ç½®å‡½æ•° append() å‘åˆ‡ç‰‡æœ«å°¾æ·»åŠ å…ƒç´ ã€‚ s := []int{1, 2, 3} // è¿™æ˜¯ä¸ªé•¿åº¦3çš„sliceã€‚ s = append(s, 4, 5, 6) // å†åŠ ä»¨å…ƒç´ ï¼Œé•¿åº¦å˜ä¸º6äº†ã€‚ fmt.Println(s) // æ›´æ–°åçš„æ•°ç»„æ˜¯ [1 2 3 4 5 6]ã€‚ // arrayå’Œsliceå„æœ‰æ‰€é•¿ï¼Œä½†æ˜¯sliceå¯ä»¥åŠ¨æ€çš„å¢åˆ ï¼Œæ‰€ä»¥æ›´å¤šæ—¶å€™è¿˜æ˜¯ä½¿ç”¨sliceã€‚ s3 := []int{4, 5, 9} // è¿™é‡Œæ²¡æœ‰çœç•¥å·ã€‚ s4 := make([]int, 4) // åˆ†é…4ä¸ªintå¤§å°çš„å†…å­˜å¹¶åˆå§‹åŒ–ä¸º0ã€‚ var d2 [][]float64 // è¿™é‡Œåªæ˜¯å£°æ˜ï¼Œå¹¶æœªåˆ†é…å†…å­˜ç©ºé—´ã€‚ bs := []byte(\"a slice\") // è¿›è¡Œç±»å‹è½¬æ¢ã€‚ // é™¤äº†å‘append()æä¾›ä¸€ç»„åŸå­å…ƒç´ ï¼ˆå†™æ­»åœ¨ä»£ç é‡Œçš„ï¼‰ä»¥å¤–ï¼Œæˆ‘ä»¬ // è¿˜å¯ä»¥ç”¨å¦‚ä¸‹æ–¹æ³•ä¼ é€’ä¸€ä¸ªsliceå¸¸é‡æˆ–å˜é‡ï¼Œå¹¶åœ¨åé¢åŠ ä¸Šçœç•¥å·ï¼Œ // ç”¨ä»¥è¡¨ç¤ºæˆ‘ä»¬å°†å¼•ç”¨ä¸€ä¸ªsliceã€è§£åŒ…å…¶ä¸­çš„å…ƒç´ å¹¶å°†å…¶æ·»åŠ åˆ°sæ•°ç»„æœ«å°¾ã€‚ s = append(s, []int{7, 8, 9}...) // ç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ªsliceå¸¸é‡ã€‚ fmt.Println(s) // æ›´æ–°åçš„æ•°ç»„æ˜¯ [1 2 3 4 5 6 7 8 9] p, q := learnMemory() // å£°æ˜p, qä¸ºintå‹å˜é‡çš„æŒ‡é’ˆã€‚ fmt.Println(*p, *q) // * å–å€¼ // Mapæ˜¯åŠ¨æ€å¯å¢é•¿å…³è”æ•°ç»„ï¼Œå’Œå…¶ä»–è¯­è¨€ä¸­çš„hashæˆ–è€…å­—å…¸ç›¸ä¼¼ã€‚ m := map[string]int{\"three\": 3, \"four\": 4} m[\"one\"] = 1 // åœ¨Goè¯­è¨€ä¸­æœªä½¿ç”¨çš„å˜é‡åœ¨ç¼–è¯‘çš„æ—¶å€™ä¼šæŠ¥é”™ï¼Œè€Œä¸æ˜¯warningã€‚ // ç©ºæ ‡è¯†ç¬¦ _ å¯ä»¥ä½¿ä½ â€œä½¿ç”¨â€ä¸€ä¸ªå˜é‡ã€‚ _, _, _, _, _, _, _, _, _, _ = str, s2, g, f, u, pi, n, a3, s4, bs // é€šå¸¸çš„ç”¨æ³•æ˜¯ï¼Œåœ¨è°ƒç”¨æ‹¥æœ‰å¤šä¸ªè¿”å›å€¼çš„å‡½æ•°æ—¶ï¼Œç”¨ä¸‹åˆ’çº¿æŠ›å¼ƒå…¶ä¸­çš„ä¸€ä¸ªå‚æ•°ã€‚ // è°ƒç”¨os.Createå¹¶ç”¨ä¸‹åˆ’çº¿å˜é‡æ‰”æ‰å®ƒçš„é”™è¯¯ä»£ç ï¼Œå› ä¸ºæˆ‘ä»¬è§‰å¾—è¿™ä¸ªæ–‡ä»¶ä¸€å®šä¼šæˆåŠŸåˆ›å»ºã€‚ file, _ := os.Create(\"output.txt\") fmt.Fprint(file, \"è¿™å¥ä»£ç è¿˜ç¤ºèŒƒäº†å¦‚ä½•å†™å…¥æ–‡ä»¶å‘¢\") file.Close() // å†™å®Œåå…³é—­æ–‡ä»¶ã€‚ // è¾“å‡ºå˜é‡ fmt.Println(s, c, a4, s3, d2, m) learnFlowControl() } // Goå…¨é¢æ”¯æŒåƒåœ¾å›æ”¶ã€‚Goæœ‰æŒ‡é’ˆï¼Œä½†æ˜¯ä¸æ”¯æŒæŒ‡é’ˆè¿ç®—ã€‚ func learnMemory() (p, q *int) { p = new(int) // å®šä¹‰ä¸€ä¸ªæŒ‡å‘intç±»å‹çš„æŒ‡é’ˆï¼Œç©ºå€¼ä¸ºnilã€‚ s := make([]int, 20) // ç»™20ä¸ªintå˜é‡åˆ†é…ä¸€å—å†…å­˜ s[3] = 7 // ç»™sliceå†…çš„å…ƒç´ èµ‹å€¼ r := -2 // å£°æ˜ä¸€ä¸ªå±€éƒ¨å˜é‡ return \u0026s[3], \u0026r // \u0026 å–åœ°å€ï¼Œpä¸ºsçš„ç¬¬4ä¸ªå…ƒç´ çš„åœ°å€ï¼Œqä¸ºrçš„åœ°å€ã€‚ } func learnFlowControl() { // iféœ€è¦èŠ±æ‹¬å·ï¼Œæ‹¬å·å°±å…äº†ã€‚ if true { fmt.Println(\"è¿™å¥è¯è‚¯å®šè¢«æ‰§è¡Œ\") } // å¦‚æœå¤ªå¤šåµŒå¥—çš„ifè¯­å¥ï¼Œæ¨èä½¿ç”¨switchã€‚ x := 1 switch x = 1; x { // åœ¨å¤´éƒ¨æ·»åŠ èµ‹å€¼è¡¨è¾¾å¼ã€‚ case 0: // éšå¼è°ƒç”¨breakè¯­å¥ï¼ŒåŒ¹é…ä¸Šä¸€ä¸ªå³åœæ­¢ã€‚ case 1: fallthrough // å¦‚æœæƒ³ç»§ç»­å‘ä¸‹æ‰§è¡Œï¼Œéœ€è¦åŠ ä¸Š fallthrough è¯­å¥ã€‚ case 2: // ç»§ç»­æ‰§è¡Œã€‚ } // å’Œifä¸€æ ·ï¼Œforä¹Ÿä¸ç”¨æ‹¬å·ã€‚ for x := 0; x \u003c 3; x++ { // ++ è‡ªå¢ã€‚ fmt.Println(\"éå†\", x) // å†…éƒ¨çš„xè¦†ç›–äº†å¤–éƒ¨çš„xã€‚ } // for æ˜¯goé‡Œå”¯ä¸€çš„å¾ªç¯å…³é”®å­—ï¼Œä¸è¿‡å®ƒæœ‰å¾ˆå¤šå˜ç§ã€‚ for { break // è·³å‡ºã€‚ continue // ç»§ç»­ä¸‹ä¸€ä¸ªå¾ªç¯ï¼Œè¿™é‡Œä¸ä¼šè¿è¡Œã€‚ } // ç”¨rangeå¯ä»¥æšä¸¾ arrayã€sliceã€stringã€mapã€channelç­‰ä¸åŒç±»å‹ã€‚ for key, value := range map[string]int{\"one\": 1, \"two\": 2, \"three\": 3} { // æ‰“å°mapä¸­çš„æ¯ä¸€ä¸ªé”®å€¼å¯¹ã€‚ fmt.Printf(\"ç´¢å¼•ï¼š%s, å€¼ä¸ºï¼š%d\\n\", key, value) } // å¦‚æœä½ åªæƒ³è¦å€¼ï¼Œé‚£å°±ç”¨å‰é¢è®²çš„ä¸‹åˆ’çº¿æ‰”æ‰ä¸ç”¨çš„å€¼ã€‚ for _, name := range []string{\"Bob\", \"Bill\", \"Joe\"} { fmt.Printf(\"ä½ æ˜¯ã€‚ã€‚ %s\\n\", name) } // å’Œforä¸€æ ·ï¼Œåœ¨å¤´éƒ¨æ·»åŠ èµ‹å€¼è¡¨è¾¾å¼ç»™yèµ‹å€¼ï¼Œç„¶åå†å’Œxä½œæ¯”è¾ƒã€‚ if y := expensiveComputation(); y \u003e x { x = y } // é—­åŒ…å‡½æ•°ã€‚ xBig := func() bool { return x \u003e 100 // xæ˜¯ä¸Šé¢å£°æ˜çš„å˜é‡å¼•ç”¨ã€‚ } fmt.Println(\"xBig:\", xBig()) // trueï¼ˆä¸Šé¢æŠŠyèµ‹ç»™xäº†ï¼‰ã€‚ x /= 1e5 // xå˜æˆ10ã€‚ fmt.Println(\"xBig:\", xBig()) // ç°åœ¨æ˜¯falseã€‚ // é™¤æ­¤ä¹‹å¤–ï¼Œå‡½æ•°ä½“å¯ä»¥åœ¨å…¶ä»–å‡½æ•°ä¸­å®šä¹‰å¹¶è°ƒç”¨ï¼Œ // æ»¡è¶³ä¸‹åˆ—æ¡ä»¶æ—¶ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå‚æ•°ä¼ é€’ç»™å…¶ä»–å‡½æ•°ï¼š // a) å®šä¹‰çš„å‡½æ•°è¢«ç«‹å³è°ƒç”¨ã€‚ // b) å‡½æ•°è¿”å›å€¼ç¬¦åˆè°ƒç”¨è€…å¯¹ç±»å‹çš„è¦æ±‚ã€‚ fmt.Println(\"ä¸¤æ•°ç›¸åŠ ä¹˜äºŒ: \", func(a, b int) int { return (a + b) * 2 }(10, 2)) // ä¼ å…¥ 10, 2 ä¸¤ä¸ªå‚æ•°ã€‚ // å½“ä½ éœ€è¦gotoçš„æ—¶å€™ï¼Œä½ ä¼šçˆ±æ­»å®ƒçš„ã€‚ // breakå’ŒcontinueåŒæ ·å¯ä»¥å¸¦labelã€‚ goto love love: return } func expensiveComputation() int { return 1e6 } ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:3","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"åŒ…ï¼ˆpackageï¼‰ Go ä½¿ç”¨ä»£ç åŒ…ï¼ˆpackageï¼‰æ¥ç»„ç»‡ç®¡ç†ä»£ç ã€‚ æˆ‘ä»¬å¿…é¡»å…ˆå¼•å…¥ä¸€ä¸ªä»£ç åŒ…ï¼ˆé™¤äº† builtin æ ‡å‡†åº“åŒ…ï¼‰æ‰èƒ½ä½¿ç”¨å…¶ä¸­å¯¼å‡ºçš„èµ„æºï¼ˆæ¯”å¦‚å‡½æ•°ã€ç±»å‹ã€å˜é‡å’Œæœ‰åå¸¸é‡ç­‰ï¼‰ã€‚ ä»£ç åŒ…ç›®å½•çš„åç§°å¹¶ä¸è¦æ±‚ä¸€å®šè¦å’Œå…¶å¯¹åº”çš„ä»£ç åŒ…çš„åç§°ç›¸åŒã€‚ ä½†æ˜¯ï¼Œåº“ä»£ç åŒ…ç›®å½•çš„åç§°æœ€å¥½è®¾ä¸ºå’Œå…¶å¯¹åº”çš„ä»£ç åŒ…çš„åç§°ç›¸åŒã€‚ å› ä¸ºä¸€ä¸ªä»£ç åŒ…çš„å¼•å…¥è·¯å¾„ä¸­åŒ…å«çš„æ˜¯æ­¤åŒ…çš„ç›®å½•åï¼Œä½†æ˜¯æ­¤åŒ…çš„é»˜è®¤å¼•å…¥åä¸ºæ­¤åŒ…çš„åç§°ã€‚ å¦‚æœä¸¤è€…ä¸ä¸€è‡´ï¼Œä¼šä½¿äººæ„Ÿåˆ°å›°æƒ‘ã€‚ åœ¨æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°é¡¹ç›®æ—¶ï¼Œåªéœ€è¦é¡¹ç›®æ–‡ä»¶å¤¹æ‰§è¡Œ go mod init \u003cproject name\u003eï¼Œå³å¯ä½¿ç”¨ go mod ç®¡ç†ä¾èµ– ä¹‹åå°±å¯ä»¥ä½¿ç”¨ go get æ‹‰å–è¿œç¨‹åŒ…ï¼Œæˆ–ä½¿ç”¨ go mod tidy ä¸€é”®å¤„ç†ä¾èµ–ã€‚ æ ‡å‡†åº“æ¦‚è¿° unsafe: åŒ…å«äº†ä¸€äº›æ‰“ç ´ Go è¯­è¨€â€œç±»å‹å®‰å…¨â€çš„å‘½ä»¤ï¼Œä¸€èˆ¬çš„ç¨‹åºä¸­ä¸ä¼šè¢«ä½¿ç”¨ï¼Œå¯ç”¨åœ¨ C/C++ ç¨‹åºçš„è°ƒç”¨ä¸­ã€‚ syscall-os-os/exec: os: æä¾›ç»™æˆ‘ä»¬ä¸€ä¸ªå¹³å°æ— å…³æ€§çš„æ“ä½œç³»ç»ŸåŠŸèƒ½æ¥å£ï¼Œé‡‡ç”¨ç±» Unix è®¾è®¡ï¼Œéšè—äº†ä¸åŒæ“ä½œç³»ç»Ÿé—´çš„å·®å¼‚ï¼Œè®©ä¸åŒçš„æ–‡ä»¶ç³»ç»Ÿå’Œæ“ä½œç³»ç»Ÿå¯¹è±¡è¡¨ç°ä¸€è‡´ã€‚ os/exec: æä¾›æˆ‘ä»¬è¿è¡Œå¤–éƒ¨æ“ä½œç³»ç»Ÿå‘½ä»¤å’Œç¨‹åºçš„æ–¹å¼ã€‚ syscall: åº•å±‚çš„å¤–éƒ¨åŒ…ï¼Œæä¾›äº†æ“ä½œç³»ç»Ÿåº•å±‚è°ƒç”¨çš„åŸºæœ¬æ¥å£ã€‚ archive/tar å’Œ /zip-compressï¼šå‹ç¼©ï¼ˆè§£å‹ç¼©ï¼‰æ–‡ä»¶åŠŸèƒ½ã€‚ fmt-io-bufio-path/filepath-flag: fmt: æä¾›äº†æ ¼å¼åŒ–è¾“å…¥è¾“å‡ºåŠŸèƒ½ã€‚ io: æä¾›äº†åŸºæœ¬è¾“å…¥è¾“å‡ºåŠŸèƒ½ï¼Œå¤§å¤šæ•°æ˜¯å›´ç»•ç³»ç»ŸåŠŸèƒ½çš„å°è£…ã€‚ bufio: ç¼“å†²è¾“å…¥è¾“å‡ºåŠŸèƒ½çš„å°è£…ã€‚ path/filepath: ç”¨æ¥æ“ä½œåœ¨å½“å‰ç³»ç»Ÿä¸­çš„ç›®æ ‡æ–‡ä»¶åè·¯å¾„ã€‚ flag: å¯¹å‘½ä»¤è¡Œå‚æ•°çš„æ“ä½œã€‚ strings-strconv-unicode-regexp-bytes: strings: æä¾›å¯¹å­—ç¬¦ä¸²çš„æ“ä½œã€‚ strconv: æä¾›å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºåŸºç¡€ç±»å‹çš„åŠŸèƒ½ã€‚ unicode: ä¸º unicode å‹çš„å­—ç¬¦ä¸²æä¾›ç‰¹æ®Šçš„åŠŸèƒ½ã€‚ regexp: æ­£åˆ™è¡¨è¾¾å¼åŠŸèƒ½ã€‚ bytes: æä¾›å¯¹å­—ç¬¦å‹åˆ†ç‰‡çš„æ“ä½œã€‚ index/suffixarray: å­å­—ç¬¦ä¸²å¿«é€ŸæŸ¥è¯¢ã€‚ math-math/cmath-math/big-math/rand-sort: math: åŸºæœ¬çš„æ•°å­¦å‡½æ•°ã€‚ math/cmath: å¯¹å¤æ•°çš„æ“ä½œã€‚ math/rand: ä¼ªéšæœºæ•°ç”Ÿæˆã€‚ sort: ä¸ºæ•°ç»„æ’åºå’Œè‡ªå®šä¹‰é›†åˆã€‚ math/big: å¤§æ•°çš„å®ç°å’Œè®¡ç®—ã€‚ container-/list-ring-heap: å®ç°å¯¹é›†åˆçš„æ“ä½œã€‚ list: åŒé“¾è¡¨ã€‚ ring: ç¯å½¢é“¾è¡¨ã€‚ time-log: time: æ—¥æœŸå’Œæ—¶é—´çš„åŸºæœ¬æ“ä½œã€‚ log: è®°å½•ç¨‹åºè¿è¡Œæ—¶äº§ç”Ÿçš„æ—¥å¿—ï¼Œæˆ‘ä»¬å°†åœ¨åé¢çš„ç« èŠ‚ä½¿ç”¨å®ƒã€‚ encoding/json-encoding/xml-text/template: encoding/json: è¯»å–å¹¶è§£ç å’Œå†™å…¥å¹¶ç¼–ç  JSON æ•°æ®ã€‚ encoding/xml: ç®€å•çš„ XML1.0 è§£æå™¨ï¼Œæœ‰å…³ JSON å’Œ XML çš„å®ä¾‹è¯·æŸ¥é˜…ç¬¬ 12.9/10 ç« èŠ‚ã€‚ text/template:ç”Ÿæˆåƒ HTML ä¸€æ ·çš„æ•°æ®ä¸æ–‡æœ¬æ··åˆçš„æ•°æ®é©±åŠ¨æ¨¡æ¿ï¼ˆå‚è§ç¬¬ 15.7 èŠ‚ï¼‰ã€‚ net-net/http-html:ï¼ˆå‚è§ç¬¬ 15 ç« ï¼‰ net: ç½‘ç»œæ•°æ®çš„åŸºæœ¬æ“ä½œã€‚ http: æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ HTTP æœåŠ¡å™¨å’ŒåŸºç¡€å®¢æˆ·ç«¯ï¼Œè§£æ HTTP è¯·æ±‚å’Œå›å¤ã€‚ html: HTML5 è§£æå™¨ã€‚ runtime: Go ç¨‹åºè¿è¡Œæ—¶çš„äº¤äº’æ“ä½œï¼Œä¾‹å¦‚åƒåœ¾å›æ”¶å’Œåç¨‹åˆ›å»ºã€‚ reflect: å®ç°é€šè¿‡ç¨‹åºè¿è¡Œæ—¶åå°„ï¼Œè®©ç¨‹åºæ“ä½œä»»æ„ç±»å‹çš„å˜é‡ã€‚ ç¨‹åºèµ„æºåˆå§‹åŒ– åœ¨ä¸€ä¸ªä»£ç åŒ…ä¸­ï¼Œç”šè‡³ä¸€ä¸ªæºæ–‡ä»¶ä¸­ï¼Œå¯ä»¥å£°æ˜è‹¥å¹²åä¸º init çš„å‡½æ•°ã€‚ è¿™äº› init å‡½æ•°å¿…é¡»ä¸å¸¦ä»»ä½•è¾“å…¥å‚æ•°å’Œè¿”å›ç»“æœã€‚åœ¨ç¨‹åºè¿è¡Œæ—¶åˆ»ï¼Œåœ¨è¿›å…¥ main å…¥å£å‡½æ•°ä¹‹å‰ï¼Œæ¯ä¸ª init å‡½æ•°åœ¨æ­¤åŒ…åŠ è½½çš„æ—¶å€™å°†è¢«ï¼ˆä¸²è¡Œï¼‰æ‰§è¡Œå¹¶ä¸”åªæ‰§è¡Œä¸€éã€‚ Go è¯­è¨€ç¨‹åºçš„åˆå§‹åŒ–å’Œæ‰§è¡Œæ€»æ˜¯ä» main.main å‡½æ•°å¼€å§‹çš„ã€‚ä½†æ˜¯å¦‚æœ main åŒ…å¯¼å…¥äº†å…¶å®ƒçš„åŒ…ï¼Œåˆ™ä¼šæŒ‰ç…§é¡ºåºå°†å®ƒä»¬åŒ…å«è¿› main åŒ…é‡Œï¼ˆå¯èƒ½æ˜¯ä»¥æ–‡ä»¶åæˆ–åŒ…è·¯å¾„åçš„å­—ç¬¦ä¸²é¡ºåºå¯¼å…¥ï¼‰ã€‚å¦‚æœæŸä¸ªåŒ…è¢«å¤šæ¬¡å¯¼å…¥çš„è¯ï¼Œåœ¨æ‰§è¡Œçš„æ—¶å€™åªä¼šå¯¼å…¥ä¸€æ¬¡ã€‚å½“ä¸€ä¸ªåŒ…è¢«å¯¼å…¥æ—¶ï¼Œå¦‚æœå®ƒè¿˜å¯¼å…¥äº†å…¶å®ƒçš„åŒ…ï¼Œåˆ™å…ˆå°†å…¶å®ƒçš„åŒ…åŒ…å«è¿›æ¥ï¼Œç„¶ååˆ›å»ºå’Œåˆå§‹åŒ–è¿™ä¸ªåŒ…çš„å¸¸é‡å’Œå˜é‡,å†è°ƒç”¨åŒ…é‡Œçš„ init å‡½æ•°ï¼Œå¦‚æœä¸€ä¸ªåŒ…æœ‰å¤šä¸ª init å‡½æ•°çš„è¯ï¼Œè°ƒç”¨é¡ºåºæœªå®šä¹‰ï¼ˆå¯èƒ½æ˜¯ä»¥æ–‡ä»¶åçš„é¡ºåºè°ƒç”¨ï¼‰ï¼ŒåŒä¸€ä¸ªæ–‡ä»¶å†…çš„å¤šä¸ª init åˆ™æ˜¯ä»¥å‡ºç°çš„é¡ºåºä¾æ¬¡è°ƒç”¨ã€‚æœ€åï¼Œå½“ main åŒ…çš„æ‰€æœ‰åŒ…çº§å¸¸é‡ã€å˜é‡è¢«åˆ›å»ºå’Œåˆå§‹åŒ–å®Œæˆï¼Œå¹¶ä¸” init å‡½æ•°è¢«æ‰§è¡Œåï¼Œæ‰ä¼šè¿›å…¥ main.main å‡½æ•°ï¼Œç¨‹åºå¼€å§‹æ­£å¸¸æ‰§è¡Œã€‚ä¸‹å›¾æ˜¯ Go ç¨‹åºå‡½æ•°å¯åŠ¨é¡ºåºçš„ç¤ºæ„å›¾ï¼š ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:4","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"å‡½æ•°ã€ç»“æ„ä½“ã€æ–¹æ³•å’Œæ¥å£ å‡½æ•° åœ¨ Go è¯­è¨€ä¸­ï¼Œå‡½æ•°æ˜¯ç¬¬ä¸€ç±»å¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥å°†å‡½æ•°å‚¨å­˜åœ¨å˜é‡ä¸­ã€‚å‡½æ•°ä¸»è¦æœ‰æœ‰åå’ŒåŒ¿åä¹‹åˆ†ï¼ŒåŒ…çº§å‡½æ•°ä¸€èˆ¬éƒ½æ˜¯æœ‰åå‡½æ•°ã€‚ // æœ‰åå‡½æ•° func Add(a, b int) int { return a + b } // åŒ¿åå‡½æ•° var Add = func(a, b int) int { return a + b } // å¤šä¸ªå‚æ•°å’Œå¤šä¸ªè¿”å›å€¼ func Swap(a, b int) (int, int) { return b, a } // å‘½åè¿”å›å€¼ func ReturnErr1() (err error) { return } // ä¸Šé¢çš„å‡½æ•°ä¸è¯¥å‡½æ•°ç­‰åŒ func ReturnErr2() error { var err error return err } // å¯å˜æ•°é‡çš„å‚æ•° // more ä¸º []int åˆ‡ç‰‡ç±»å‹ func Sum(a int, more ...int) int { for _, v := range more { a += v } return a } defer å°†å‡½æ•°è°ƒç”¨æ¨è¿Ÿåˆ°å¤–å±‚å‡½æ•°è¿”å›ä¹‹å‰ï¼ˆæ‰§è¡Œ return è¯­å¥æˆ–è€…å‘ç”Ÿå¼‚å¸¸åï¼‰ ä¸‹é¢è¿™æ®µä»£ç ä½¿ç”¨ defer å®ç°ä»£ç æ‰§è¡Œè¿½è¸ªã€‚ package main import \"fmt\" func trace(s string) { fmt.Println(\"entering:\", s) } func untrace(s string) { fmt.Println(\"leaving:\", s) } func a() { trace(\"a\") defer untrace(\"a\") fmt.Println(\"in a\") } func b() { trace(\"b\") defer untrace(\"b\") fmt.Println(\"in b\") a() } func main() { b() } è¾“å‡ºï¼š entering: b in b entering: a in a leaving: a leaving: b ç»“æ„ä½“ ä½¿ç”¨ type \u003cstruct_name\u003e struct {} å£°æ˜ç»“æ„ä½“ input: type student struct { name string grade int } func main() { var s student = student{name: \"SaltFish\", grade: 4} fmt.Printf(\"%v\\n\", s) } output: {SaltFish 4} %T å€¼ç±»å‹çš„ Go è¯­æ³•è¡¨ç¤º main.student %v å€¼ {SaltFish 4} %+v å€¼ï¼Œæ·»åŠ å­—æ®µå {name:SaltFish grade:4} %#v å€¼ï¼ŒGo è¯­æ³•è¡¨ç¤º main.student{name:\"SaltFish\", grade:4} æ–¹æ³• Go è¯­è¨€ä¸­æ¯ä¸ªç±»å‹ï¼ˆé™¤äº†æŒ‡é’ˆç±»å‹å’Œæ¥å£ç±»å‹ï¼‰è¿˜å¯ä»¥æœ‰è‡ªå·±çš„æ–¹æ³•ï¼Œæ–¹æ³•ä¹Ÿæ˜¯å‡½æ•°çš„ä¸€ç§ã€‚ // æ–‡ä»¶å¯¹è±¡ type File struct { fd int } // è¯»æ–‡ä»¶æ•°æ® func (f *File) Read(offset int64, data []byte) int { // ... } // å…³é—­æ–‡ä»¶ func (f *File) Close() error { // ... } func main() { f := new(File) f.Read() f.Close() } æ¥å£ æ¥å£ç±»å‹æ˜¯ Go ä¸­çš„ä¸€ç§å¾ˆç‰¹åˆ«çš„ç±»å‹ã€‚æ¥å£ç±»å‹åœ¨ Go ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚ é¦–å…ˆï¼Œåœ¨ Go ä¸­ï¼Œæ¥å£å€¼å¯ä»¥ç”¨æ¥åŒ…è£¹éæ¥å£å€¼ï¼›ç„¶åï¼Œé€šè¿‡å€¼åŒ…è£¹ï¼Œåå°„å’Œå¤šæ€å¾—ä»¥å®ç°ã€‚ ä¸€ä¸ªæ¥å£ç±»å‹å®šä¹‰äº†ä¸€ä¸ªæ–¹æ³•é›†ã€‚æ¥å£ç±»å‹ä¸­æŒ‡å®šçš„ä»»ä½•æ–¹æ³•åŸå‹ä¸­çš„æ–¹æ³•åç§°éƒ½ä¸èƒ½ä¸ºç©ºæ ‡è¯†ç¬¦_ã€‚ // å®šä¹‰ Aboutable æ¥å£ï¼Œæ–¹æ³•é›†ä¸­åªæœ‰ About() string ä¸€ä¸ªæ–¹æ³•ã€‚ type Aboutable interface { About() string } // å®šä¹‰ Book ç»“æ„ä½“ã€‚ type Book struct { name string } // Book æ‹¥æœ‰åŸå‹ä¸º About() string çš„æ–¹æ³•ï¼Œå› æ­¤å®ƒå®ç°äº† Aboutable æ¥å£ã€‚ func (b Book) About() string { return b.name } func main() { // *Book ç±»å‹çš„å­—é¢é‡å€¼è¢«åŒ…è£¹åœ¨ Aboutable ç±»å‹çš„å˜é‡ a ä¸­ã€‚ var a Aboutable = \u0026Book{\"Go\"} fmt.Println(a.About()) // è°ƒç”¨æ¥å£çš„æ–¹æ³• // iæ˜¯ä¸€ä¸ªç©ºæ¥å£å€¼ã€‚ä»»ä½•ç±»å‹éƒ½å®ç°äº†ç©ºæ¥å£ç±»å‹ã€‚ var i interface{} = \u0026Book{\"Go\"} fmt.Println(i) // Aboutable å®ç°äº†ç©ºæ¥å£ç±»å‹ interface{}ã€‚ i = a fmt.Println(i) } ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:5","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"é”™è¯¯å¤„ç† ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:6","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"åç¨‹ä¸é€šé“ ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:7","tags":null,"title":"Go è¯­è¨€å…¥é—¨","uri":"/posts/2021-12-02-go-intro/"},{"categories":["linux"],"content":"Linux å‘½ä»¤è§£é‡Šå™¨ï¼ˆShellï¼‰æ˜¯æœ‰å¾ˆå¤šå¿«æ·é”®çš„ï¼Œç†Ÿç»ƒæŒæ¡å¯ä»¥æå¤§çš„æé«˜æ“ä½œæ•ˆç‡ã€‚ ä¸‹é¢åˆ—å‡ºæœ€å¸¸ç”¨çš„å¿«æ·é”®ï¼Œè¿™è¿˜ä¸æ˜¯å®Œå…¨ç‰ˆã€‚ å‘½ä»¤è¡Œå¿«æ·é”®ï¼š å¸¸ç”¨ï¼š Ctrl L ï¼šæ¸…å± Ctrl M ï¼šç­‰æ•ˆäºå›è½¦ Ctrl C : ä¸­æ–­æ­£åœ¨å½“å‰æ­£åœ¨æ‰§è¡Œçš„ç¨‹åº å†å²å‘½ä»¤ï¼š Ctrl P : ä¸Šä¸€æ¡å‘½ä»¤ï¼Œå¯ä»¥ä¸€ç›´æŒ‰è¡¨ç¤ºä¸€ç›´å¾€å‰ç¿» Ctrl N : ä¸‹ä¸€æ¡å‘½ä»¤ Ctrl Rï¼Œå†æŒ‰å†å²å‘½ä»¤ä¸­å‡ºç°è¿‡çš„å­—ç¬¦ä¸²ï¼šæŒ‰å­—ç¬¦ä¸²å¯»æ‰¾å†å²å‘½ä»¤ï¼ˆé‡åº¦æ¨èï¼‰ å‘½ä»¤è¡Œç¼–è¾‘ï¼š Tab : è‡ªåŠ¨è¡¥é½ï¼ˆé‡åº¦æ¨èï¼‰ Ctrl A ï¼š ç§»åŠ¨å…‰æ ‡åˆ°å‘½ä»¤è¡Œé¦– Ctrl E : ç§»åŠ¨å…‰æ ‡åˆ°å‘½ä»¤è¡Œå°¾ Ctrl B : å…‰æ ‡åé€€ Ctrl F : å…‰æ ‡å‰è¿› Alt F : å…‰æ ‡å‰è¿›ä¸€ä¸ªå•è¯ Alt B : å…‰æ ‡åé€€ä¸€æ ¼å•è¯ Ctrl ] : ä»å½“å‰å…‰æ ‡å¾€åæœç´¢å­—ç¬¦ä¸²ï¼Œç”¨äºå¿«é€Ÿç§»åŠ¨åˆ°è¯¥å­—ç¬¦ä¸² Ctrl Alt ] : ä»å½“å‰å…‰æ ‡å¾€å‰æœç´¢å­—ç¬¦ä¸²ï¼Œç”¨äºå¿«é€Ÿç§»åŠ¨åˆ°è¯¥å­—ç¬¦ä¸² Ctrl H : åˆ é™¤å…‰æ ‡çš„å‰ä¸€ä¸ªå­—ç¬¦ Ctrl D : åˆ é™¤å½“å‰å…‰æ ‡æ‰€åœ¨å­—ç¬¦ Ctrl K ï¼šåˆ é™¤å…‰æ ‡ä¹‹åæ‰€æœ‰å­—ç¬¦ Ctrl U : æ¸…ç©ºå½“å‰é”®å…¥çš„å‘½ä»¤ Ctrl W : åˆ é™¤å…‰æ ‡å‰çš„å•è¯(Word, ä¸åŒ…å«ç©ºæ ¼çš„å­—ç¬¦ä¸²) Ctrl \\ : åˆ é™¤å…‰æ ‡å‰çš„æ‰€æœ‰ç©ºç™½å­—ç¬¦ Ctrl Y : ç²˜è´´Ctrl Wæˆ–Ctrl Kåˆ é™¤çš„å†…å®¹ Alt . : ç²˜è´´ä¸Šä¸€æ¡å‘½ä»¤çš„æœ€åä¸€ä¸ªå‚æ•°ï¼ˆå¾ˆæœ‰ç”¨ï¼‰ Alt [0-9] Alt . ç²˜è´´ä¸Šä¸€æ¡å‘½ä»¤çš„ç¬¬[0-9]ä¸ªå‚æ•° Alt [0-9] Alt . Alt. ç²˜è´´ä¸Šä¸Šä¸€æ¡å‘½ä»¤çš„ç¬¬[0-9]ä¸ªå‚æ•° Ctrl X Ctrl E : è°ƒå‡ºç³»ç»Ÿé»˜è®¤ç¼–è¾‘å™¨ç¼–è¾‘å½“å‰è¾“å…¥çš„å‘½ä»¤ï¼Œé€€å‡ºç¼–è¾‘å™¨æ—¶ï¼Œå‘½ä»¤æ‰§è¡Œ å…¶ä»–ï¼š Ctrl Z : æŠŠå½“å‰è¿›ç¨‹æ”¾åˆ°åå°ï¼ˆä¹‹åå¯ç”¨â€™â€˜fgâ€™â€˜å‘½ä»¤å›åˆ°å‰å°ï¼‰ Shift Insert : ç²˜è´´ï¼ˆç›¸å½“äº Windows çš„Ctrl Vï¼‰ åœ¨å‘½ä»¤è¡Œçª—å£é€‰ä¸­å³å¤åˆ¶ åœ¨å‘½ä»¤è¡Œçª—å£ä¸­é”®å³ç²˜è´´ï¼Œå¯ç”¨Shift Insertä»£æ›¿ Ctrl PageUp : å±å¹•è¾“å‡ºå‘ä¸Šç¿»é¡µ Ctrl PageDown : å±å¹•è¾“å‡ºå‘ä¸‹ç¿»é¡µ ","date":"2021-12-01","objectID":"/posts/2021-12-01-linux-shortcut-keymap/:0:0","tags":null,"title":"Linux å¸¸ç”¨å¿«æ·é”®","uri":"/posts/2021-12-01-linux-shortcut-keymap/"},{"categories":["technique"],"content":"åˆæ¬¡è¿è¡Œ Git å‰çš„é…ç½® è®¾ç½®ç”¨æˆ·åå’Œé‚®ç®± git config --global user.name \"Salt Fish\" git config --global user.email saltfishpr@gmail.com é…ç½®é»˜è®¤æ–‡æœ¬ç¼–è¾‘å™¨ git config --global core.editor code æ£€æŸ¥é…ç½®ä¿¡æ¯ git config --list ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:1:0","tags":["git"],"title":"Git å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"è®¾ç½®ä¿å­˜å¯†ç  # è®°ä½å¯†ç ï¼ˆé»˜è®¤15åˆ†é’Ÿï¼‰ git config --global credential.helper cache # è‡ªå·±è®¾ç½®æ—¶é—´ git config --global credential.helper cache --timeout=3600 # é•¿æœŸå­˜å‚¨å¯†ç  git config --global credential.helper store å¢åŠ è¿œç¨‹åœ°å€çš„æ—¶å€™å¸¦ä¸Šå¯†ç  git remote -v # https://yourname:password@github.com/saltfishpr/go-learning.git ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:2:0","tags":["git"],"title":"Git å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"Git clone/push å¤ªæ…¢ åœ¨å›½å†…ï¼Œgithub åŸŸåè¢«é™åˆ¶ï¼Œå¯¼è‡´ git clone å¾ˆæ…¢ï¼Œåªæœ‰ 40KB/s çš„é€Ÿåº¦ ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:3:0","tags":["git"],"title":"Git å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"Windows ä½¿ç”¨ nslookup æŸ¥è¯¢ github å¯¹åº”çš„ IP åœ°å€ nslookup github.global.ssl.fastly.net nslookup github.com æŠŠæŸ¥è¯¢åˆ°çš„ç»“æœæ·»åŠ åˆ° C:\\Windows\\System32\\drivers\\etc\\hosts ä¸­ github.global.ssl.fastly.net 69.63.184.14 github.com 140.82.112.3 åˆ·æ–° DNS ç¼“å­˜ ipconfig /flushdns ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:3:1","tags":["git"],"title":"Git å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"linux ä½¿ç”¨ nslookup æŸ¥è¯¢ github å¯¹åº”çš„ IP åœ°å€ nslookup github.global.ssl.fastly.net nslookup github.com æŠŠæŸ¥è¯¢åˆ°çš„ç»“æœæ·»åŠ åˆ° /etc/hosts ä¸­ github.global.ssl.fastly.net 69.63.184.14 github.com 140.82.112.3 åˆ·æ–° DNS ç¼“å­˜ sudo nscd -i hosts ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:3:2","tags":["git"],"title":"Git å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"æ¸…é™¤è¿œç¨‹åˆ†æ”¯çš„æœ¬åœ°ç¼“å­˜ å½“ä¿®æ”¹/åˆ é™¤è¿œç¨‹åˆ†æ”¯åï¼Œæœ¬åœ°çš„è¿œç¨‹åˆ†æ”¯ç¼“å­˜æœªè¢«åˆ é™¤ï¼Œå† checkout ä¼šå‡ºç° remote ref does not exist é”™è¯¯ï¼Œè¿™æ—¶è¦å…ˆæ¸…é™¤æœ¬åœ°ç¼“å­˜ git fetch -p origin ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:4:0","tags":["git"],"title":"Git å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021-11-25-git/"},{"categories":["python"],"content":"Kaggle ä¸Šçš„ Pandas å…¥é—¨æ•™ç¨‹","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/","tags":["data science"],"title":"Pandas å…¥é—¨","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"è¯»å–æ•°æ®é›† import pandas as pd reviews = pd.read_csv(\"~/Programming/datasets/winemag-data-130k-v2.csv\") ","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:1:0","tags":["data science"],"title":"Pandas å…¥é—¨","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"åˆ†ç»„å’Œæ’åº å¯¹åˆ†æ•°åˆ†ç»„æ±‚å’Œ reviews.groupby(\"points\").points.count() Output: points 80 397 81 692 82 1836 ... 98 77 99 33 100 19 Name: points, dtype: int64 è·å¾—ç›¸åŒè¯„åˆ†è‘¡è„é…’çš„æœ€ä½ä»·æ ¼ reviews.groupby(\"points\").price.min() Output: points 80 5.0 81 5.0 82 4.0 ... 98 50.0 99 44.0 100 80.0 Name: price, dtype: float64 ä»æ¯ä¸ªé…’åº„ä¸­é€‰æ‹©ç¬¬ä¸€ç“¶è‘¡è„é…’çš„åç§° reviews.groupby('winery').apply(lambda df: df.title.iloc[0]) Output: winery 1+1=3 1+1=3 NV RosÃ© Sparkling (Cava) 10 Knots 10 Knots 2010 Viognier (Paso Robles) 100 Percent Wine 100 Percent Wine 2015 Moscato (California) 1000 Stories 1000 Stories 2013 Bourbon Barrel Aged Zinfande... 1070 Green 1070 Green 2011 Sauvignon Blanc (Rutherford) ... Ã“rale Ã“rale 2011 Cabronita Red (Santa Ynez Valley) Ã–ko Ã–ko 2013 Made With Organically Grown Grapes Ma... Ã–konomierat Rebholz Ã–konomierat Rebholz 2007 Von Rotliegenden SpÃ¤t... Ã Maurice Ã Maurice 2013 Fred Estate Syrah (Walla Walla V... Å toka Å toka 2009 Izbrani Teran (Kras) Length: 16757, dtype: object æ ¹æ®å›½å®¶å’Œçœä»½æŒ‘é€‰æœ€å¥½çš„è‘¡è„é…’ reviews.groupby([\"country\", \"province\"]).apply(lambda df:df.loc[df.points.idxmax()]) Output: Unnamed: 0 ... winery country province ... Argentina Mendoza Province 82754 ... Bodega Catena Zapata Other 78303 ... ColomÃ© Armenia Armenia 66146 ... Van Ardi Australia Australia Other 37882 ... Marquis Philips New South Wales 85337 ... De Bortoli ... ... ... Uruguay Juanico 9133 ... Familia Deicas Montevideo 15750 ... Bouza Progreso 93103 ... Pisano San Jose 39898 ... Castillo Viejo Uruguay 39361 ... Narbona [425 rows x 14 columns] agg() æ–¹æ³•å¯ä»¥åœ¨ä¸€ä¸ª dataframe ä¸Šè¿è¡Œä¸€äº›ä¸€ç»´çš„å‡½æ•° reviews.groupby(['country']).price.agg([len, min, max]) Output: len min max country Argentina 3800.0 4.0 230.0 Armenia 2.0 14.0 15.0 Australia 2329.0 5.0 850.0 ... US 54504.0 4.0 2013.0 Ukraine 14.0 6.0 13.0 Uruguay 109.0 10.0 130.0 å¤šç´¢å¼•ï¼š ä½¿ç”¨ groupby() å¯èƒ½ä¼šäº§ç”Ÿå¤šç´¢å¼• countries_reviewed = reviews.groupby(['country', 'province']).description.agg([len]) countries_reviewed Output: len country province Argentina Mendoza Province 3264 Other 536 Armenia Armenia 2 Australia Australia Other 245 New South Wales 85 ... Uruguay Juanico 12 Montevideo 11 Progreso 11 San Jose 3 Uruguay 24 [425 rows x 1 columns] å¤šç´¢å¼•è½¬å›å¸¸è§„ç´¢å¼• countries_reviewed.reset_index() Output: country province len 0 Argentina Mendoza Province 3264 1 Argentina Other 536 2 Armenia Armenia 2 3 Australia Australia Other 245 4 Australia New South Wales 85 .. ... ... ... 420 Uruguay Juanico 12 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 423 Uruguay San Jose 3 424 Uruguay Uruguay 24 [425 rows x 3 columns] ä½¿ç”¨ sort_values() å¯¹æ•°æ®æ’åºï¼ˆé»˜è®¤ä¸ºå‡åºæ’åºï¼‰ countries_reviewed = countries_reviewed.reset_index() countries_reviewed.sort_values(by='len') Output: country province len 179 Greece Muscat of Kefallonian 1 192 Greece Sterea Ellada 1 194 Greece Thraki 1 354 South Africa Paardeberg 1 40 Brazil Serra do Sudeste 1 .. ... ... ... 409 US Oregon 5373 227 Italy Tuscany 5897 118 France Bordeaux 5941 415 US Washington 8639 392 US California 36247 [425 rows x 3 columns] é™åºæ’åº countries_reviewed.sort_values(by='len', ascending=False) Output: country province len 392 US California 36247 415 US Washington 8639 118 France Bordeaux 5941 227 Italy Tuscany 5897 409 US Oregon 5373 .. ... ... ... 101 Croatia Krk 1 247 New Zealand Gladstone 1 357 South Africa Piekenierskloof 1 63 Chile Coelemu 1 149 Greece Beotia 1 [425 rows x 3 columns] é‡æ–°æŒ‰ç…§ç´¢å¼•æ’åº countries_reviewed.sort_index() Output: country province len 0 Argentina Mendoza Province 3264 1 Argentina Other 536 2 Armenia Armenia 2 3 Australia Australia Other 245 4 Australia New South Wales 85 .. ... ... ... 420 Uruguay Juanico 12 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 423 Uruguay San Jose 3 424 Uruguay Uruguay 24 [425 rows x 3 columns] åŒæ—¶ä½¿ç”¨å¤šä¸ªåˆ—æ’åº countries_reviewed.sort_values(by=['country', 'len']) Output: country province len 1 Argentina Other 536 0 Argentina Mendoza Province 3264 2 Armenia Armenia 2 6 Australia Tasmania 42 4 Australia New South Wales 85 .. ... ... ... 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 420 Uruguay Juanico 12 424 Uruguay Uru","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:2:0","tags":["data science"],"title":"Pandas å…¥é—¨","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼ è·å– price åˆ—çš„æ•°æ®ç±»å‹ reviews.price.dtype Output: dtype('float64') æŸ¥çœ‹ dataframe ä¸­æ¯ä¸€åˆ—çš„æ•°æ®ç±»å‹ reviews.dtypes Output: Unnamed: 0 int64 country object description object designation object points int64 price float64 province object region_1 object region_2 object taster_name object taster_twitter_handle object title object variety object winery object dtype: object ä½¿ç”¨ astype() è½¬æ¢æ•°æ®ç±»å‹ reviews.points.astype('float64') Output: 0 87.0 1 87.0 2 87.0 3 87.0 4 87.0 ... 129966 90.0 129967 90.0 129968 90.0 129969 90.0 129970 90.0 Name: points, Length: 129971, dtype: float64 ç¼ºå°‘å€¼çš„æ¡ç›®è¢«èµ‹äºˆå€¼ NaNï¼Œç¼©å†™ä¸º â€œNot a Numberâ€ æŸ¥çœ‹ country ä¸º NaN çš„è¡Œ reviews[pd.isnull(reviews.country)] Output: Unnamed: 0 country ... variety winery 913 913 NaN ... Chinuri Gotsa Family Wines 3131 3131 NaN ... Red Blend Barton \u0026 Guestier 4243 4243 NaN ... Ojaleshi Kakhetia Traditional Winemaking 9509 9509 NaN ... White Blend Tsililis 9750 9750 NaN ... Chardonnay Ross-idi ... ... ... ... ... 124176 124176 NaN ... Red Blend Les FrÃ¨res Dutruy 129407 129407 NaN ... Cabernet Sauvignon El Capricho 129408 129408 NaN ... Tempranillo El Capricho 129590 129590 NaN ... Red Blend BÃ¼yÃ¼lÃ¼baÄŸ 129900 129900 NaN ... Merlot Psagot [63 rows x 14 columns] ä½¿ç”¨ fillna() å¡«å……ç¼ºå¤±çš„è¡Œ reviews.region_2.fillna(\"Unknown) Output: 0 Unknown 1 Unknown 2 Willamette Valley 3 Unknown 4 Willamette Valley ... 129966 Unknown 129967 Oregon Other 129968 Unknown 129969 Unknown 129970 Unknown Name: region_2, Length: 129971, dtype: object ä½¿ç”¨ replace(old_val, new_val) æ–¹æ³•æ›¿æ¢éç©ºå€¼ reviews.taster_twitter_handle.replace(\"@kerinokeefe\", \"@kerino\") Output: 0 @kerino 1 @vossroger 2 @paulgwine 3 NaN 4 @paulgwine ... 129966 NaN 129967 @paulgwine 129968 @vossroger 129969 @vossroger 129970 @vossroger Name: taster_twitter_handle, Length: 129971, dtype: object ","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:3:0","tags":["data science"],"title":"Pandas å…¥é—¨","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"é‡å‘½åå’Œåˆå¹¶ é‡å‘½åä¸€åˆ— reviews.rename(columns={'points': 'score'}) Output: Unnamed: 0 ... winery 0 0 ... Nicosia 1 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben MÃ¼ller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] é‡å‘½åå¯ä»¥é€‰æ‹© index æˆ–è€… colum å‚æ•° reviews.rename(index={0: 'firstEntry', 1: 'secondEntry'}) Output: Unnamed: 0 ... winery firstEntry 0 ... Nicosia secondEntry 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben MÃ¼ller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] å› ä¸ºå¾ˆå°‘é‡å‘½åç´¢å¼•å€¼ï¼Œé€šå¸¸æƒ…å†µä¸‹ set_index() æ›´å¥½ç”¨ ç»™è¡Œç´¢å¼•å’Œåˆ—ç´¢å¼•æ·»åŠ åç§°å±æ€§ reviews.rename_axis(\"wines\", axis='rows').rename_axis(\"fields\", axis='columns') Output: fields Unnamed: 0 ... winery wines ... 0 0 ... Nicosia 1 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben MÃ¼ller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] ç»„åˆæœ‰ä¸‰ä¸ªæ ¸å¿ƒæ–¹æ³•concat(), join() å’Œ merge() concat(): ç»™å®šä¸€ä¸ªå…ƒç´ åˆ—è¡¨ï¼Œæ­¤å‡½æ•°å°†æ²¿ç€ä¸€ä¸ª axis å°†è¿™äº›å…ƒç´ æ··åˆåœ¨ä¸€èµ· canadian_youtube british_youtube pd.concat([canadian_youtube, british_youtube]) Output: video_id ... description 0 n1WpP7iowLc ... Eminem's new track Walk on Water ft. BeyoncÃ© i... 1 0dBIkQ4Mz1M ... STill got a lot of packages. Probably will las... 2 5qpjK5DgCt4 ... WATCH MY PREVIOUS VIDEO â–¶ \\n\\nSUBSCRIBE â–º http... 3 d380meD0W0M ... I know it's been a while since we did this sho... 4 2Vv-BfVoq4g ... ğŸ§: https://ad.gt/yt-perfect\\nğŸ’°: https://atlant... ... ... ... 40876 sGolxsMSGfQ ... ğŸš¨ NEW MERCH! http://amzn.to/annoyingorange ğŸš¨â¤ ... 40877 8HNuRNi8t70 ... â–º Retrouvez vos programmes prÃ©fÃ©rÃ©s : https://... 40878 GWlKEM3m2EE ... Find out more about Kingdom Hearts 3: https://... 40879 lbMKLzQ4cNQ ... Peter Navarro isnâ€™t talking so tough now. Ana ... 40880 POTgw38-m58 ... è—äººï¼šæå¦ç‘¾ã€ç‰å…”ã€ç­å‚‘ã€LaLaã€å°å„ªã€å°‘å°‘å°ˆå®¶ï¼šé™³ç­±å±(å¾‹å¸«)ã€Wendy(å¿ƒç†å¸«)ã€ç¾…... [40881 rows x 16 columns] video_id ... description 0 Jw1Y-zhQURU ... Click here to continue the story and make your... 1 3s1rvMFUweQ ... Musical guest Taylor Swift performs â€¦Ready for... 2 n1WpP7iowLc ... Eminem's new track Walk on Water ft. BeyoncÃ© i... 3 PUTEiSjKwJU ... Salford drew 4-4 against the Class of 92 and F... 4 rHwDegptbI4 ... Dashcam captures truck's near miss with child ... ... ... ... 38911 l884wKofd54 ... NEW SONG - MOVE TO MIAMI feat. Pitbull (Click ... 38912 IP8k2xkhOdI ... THE OFFICIAL UP WITH IT MUSIC VIDEO!Get my new... 38913 Il-an3K9pjg ... Get 2002 by Anne-Marie HERE â–¶ http://ad.gt/200... 38914 -DRsfNObKIQ ... Eleni Foureira represented Cyprus at the first... 38915 4YFo4bdMO8Q ... Debut album 'Light of Mine' out now: http://ky... [38916 rows x 16 columns] video_id ... description 0 n1WpP7iowLc ... Eminem's new track Walk on Water ft. BeyoncÃ© i... 1 0dBIkQ4Mz1M ... STill got a lot of packages. Probably will las... 2 5qpjK5DgCt4 ... WATCH MY PREVIOUS VIDEO â–¶ \\n\\nSUBSCRIBE â–º http... 3 d380meD0W0M ... I know it's been a while since we did this sho... 4 2Vv-BfVoq4g ... ğŸ§: https://ad.gt/yt-perfect\\nğŸ’°: https://atlant... ... ... ... 38911 l884wKofd54 ... NEW SONG - MOVE TO MIAMI feat. Pitbull (Click ... 38912 IP8k2xkhOdI ... THE OFFICIAL UP WITH IT MUSIC VIDEO!Get my new... 38913 Il-an3K9pjg ... Get 2002 by Anne-Marie HERE â–¶ http://ad.gt/200... 38914 -DRsfNObKIQ ... Eleni Foureira represented Cyprus at the first... 38915 4YFo4bdMO8Q ... Debut album 'Light of Mine' out now: http://ky... [79797 rows x 16 columns] join(): å¯ä»¥ç»„åˆå…·æœ‰å…±åŒç´¢å¼•çš„ä¸åŒ dataframe å¯¹è±¡ left = canadian_youtube.set_index(['title', 'trending_date']) right = british_youtube.set_index(['title', 't","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:4:0","tags":["data science"],"title":"Pandas å…¥é—¨","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"å…¶ä»– æ˜¾ç¤ºæ‰€æœ‰åˆ— pd.set_option(\"display.max_columns\", None) ","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:5:0","tags":["data science"],"title":"Pandas å…¥é—¨","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"conda ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:1:0","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"æ¢æº åˆ›å»º .condarc é…ç½®æ–‡ä»¶ conda config --set show_channel_urls yes ç”¨æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€ ~/.condarc å¡«å…¥ä»¥ä¸‹å†…å®¹ channels: - defaults show_channel_urls: true channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:1:1","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"å‘½ä»¤ åˆ›å»ºæœ‰æœ€æ–°ç‰ˆæœ¬çš„ python çš„ç¯å¢ƒ conda create -n \u003cenv-name\u003e python=3 åˆ é™¤ç¯å¢ƒ conda remove -n \u003cenv-name\u003e --all æ¸…é™¤ç¼“å­˜ conda clean -a ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:1:2","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"pip ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:2:0","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"æ¢æº åœ¨ ~/.config/pip/pip.conf ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹ [global] index-url = https://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:2:1","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"åˆ é™¤ç¼“å­˜ æ‰¾åˆ° ~/.cache/pip æ–‡ä»¶å¤¹ï¼Œåˆ é™¤å³å¯ åœ¨å®‰è£…æ—¶ä½¿ç”¨ pip install \u003cpackage-name\u003e --no-cache-dir ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:2:2","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"å¯¼å…¥å¯¼å‡º python ç¯å¢ƒ ä»Šå¤©åœ¨å­¦ä¹ æ•°æ®æŒ–æ˜çš„æ—¶å€™ï¼Œnolearn å’Œ lasagne ä¸¤ä¸ªåº“çš„æ—¶å€™ç»™æˆ‘çš„ jypyterlab ç¯å¢ƒæå´©äº†ï¼Œåªå¥½ remove â€“all é‡æ–°é…èµ·ï¼ŒçœŸåæ‚”æ²¡æœ‰å…ˆæä¸ªç¯å¢ƒå¤‡ä»½( Â´â€¢ï¸µâ€¢` ) ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:3:0","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"conda conda æ˜¯ä¸ªå¥½ä¸œè¥¿ï¼Œå¯ä»¥è‡ªå·±å¤„ç†ç¯å¢ƒä¾èµ–ï¼Œç¼ºç‚¹å°±æ˜¯ã€‚ã€‚åŒ…æœ‰ç‚¹è€ï¼Œæœ‰äº›åŒ…è¿˜æ‰¾ä¸åˆ° å¯¼å…¥ç¯å¢ƒ conda create --name \u003cyour env name\u003e --file \u003cthis file\u003e --yes å¯¼å‡ºç¯å¢ƒ conda list -e \u003e requirements.txt ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:3:1","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"pip pip ä¹Ÿæœ‰å¾ˆå¤šä¼˜ç‚¹ï¼Œæˆ‘ä¸€èˆ¬æ˜¯åœ¨ conda æ‰¾ä¸åˆ°æ¨¡å—çš„æ—¶å€™ä½¿ç”¨ pip å¯¼å…¥ç¯å¢ƒ pip install -r requirements.txt å¯¼å‡ºç¯å¢ƒ pip freeze \u003e requirements.txt ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:3:2","tags":null,"title":"Python ç¯å¢ƒç®¡ç†","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["technique"],"content":"åº”æœ‹å‹çš„å¼ºçƒˆè¦æ±‚åœ¨è¿™é‡Œè®°å½•ä¸‹ä½¿ç”¨ github pages + jekyll æ­å»ºä¸ªäººç½‘ç«™çš„æ­¥éª¤ã€‚æˆ‘è‡ªå·±èŠ±äº†ä¸¤å¤©æ¢³ç†äº†ä¸€ä¸‹ jekyll æ¨¡æ¿çš„ç›®å½•ç»“æ„å’Œä½¿ç”¨æ–¹æ³•ï¼Œåªä¼šäº›ç®€å•çš„ä¿®æ”¹ä½†æ˜¯åŸºæœ¬å¤Ÿç”¨äº†ï¼ˆæ¯•ç«Ÿä¸æ˜¯å‰ç«¯äººï¼‰ã€‚ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:0:0","tags":["jekyll"],"title":"Github Pages ä¸ªäººåšå®¢æ­å»º","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"å‡†å¤‡ æ³¨å†Œ github è´¦å· åˆ›å»ºä¸€ä¸ªä»“åº“ï¼Œå‘½åå¿…é¡»ä¸º\u003cusername\u003e.github.ioï¼Œç™¾åº¦ä¸Šä»»æ„æœç´¢éƒ½æœ‰ä»‹ç»è¿‡ç¨‹ï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°äº† ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:1:0","tags":["jekyll"],"title":"Github Pages ä¸ªäººåšå®¢æ­å»º","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"windows: å®‰è£… gitï¼Œä¸‹è½½å®‰è£…æœ€æ–°ç‰ˆæœ¬å³å¯ å®‰è£… msys2 å®‰è£…å¥½ msys2 åï¼Œé¦–å…ˆè®°å¾—åˆ‡æ¢é•œåƒæºï¼Œä¸ç„¶ä¼šå› ä¸ºç½‘é€Ÿå¤ªæ…¢å¿ƒæ€å´©æºƒã€‚åœ¨è®¾ç½®é•œåƒæºçš„æ—¶å€™ï¼Œå¯ä»¥å°†é™¤äº†æ¸…åæºçš„å…¶ä»–æºå…¨éƒ¨æ³¨é‡Šæ‰ï¼Œæé«˜ä¸‹è½½é€Ÿåº¦ã€‚ å®‰è£… rubyï¼Œè¿™é‡Œä¸‹è½½ 2.6.5 ç‰ˆæœ¬ï¼Œå› ä¸ºæœ€æ–°ç‰ˆåé¢ä¼šæç¤ºç‰ˆæœ¬ä¸åŒ¹é…ã€‚ ä¸‹è½½è¿‡ç¨‹å¯èƒ½å¾ˆä¹…â€¦å¦‚æœè§‰å¾—æ…¢å¯ä»¥ä¸‹è½½ä¸åŒ…å« devkit çš„ç‰ˆæœ¬ã€‚ å®‰è£…æ—¶åœ¨ Select Components ç•Œé¢ä¸ç”¨å‹¾é€‰ msys2ï¼Œå› ä¸ºåœ¨ä¸Šä¸€æ­¥ä¸­å·²ç»å®‰è£…å¥½äº†ã€‚ åœ¨ Finish æ—¶å‹¾é€‰ Run â€˜ridk installâ€™ to setup msys2â€¦ å¼¹å‡ºé…ç½®ç•Œé¢ï¼Œåœ¨è¿™é‡Œæˆ‘é€‰æ‹© 3 å¹¶æŒ‰å›è½¦ã€‚ç”±äºé…ç½®è¿‡ msys2 çš„æºï¼Œè¿™é‡Œä¸‹è½½å®‰è£…é€Ÿåº¦å¾ˆå¿«ã€‚ å®‰è£…å®Œæˆåï¼Œæ‰“å¼€ msys2 å‘½ä»¤è¡Œçª—å£ï¼Œä¸º rubygemsé…ç½®æºã€‚ å®‰è£…rubygemsï¼Œä¸‹è½½åè§£å‹ç¼©ï¼Œåœ¨æ–‡ä»¶å¤¹ä¸­æ‰“å¼€ git bash è¾“å…¥ ruby setup.rb å®‰è£… bundlerï¼šåœ¨ msys2 æˆ–è€… git bash å‘½ä»¤è¡Œä¸­è¾“å…¥gem install bundlerï¼Œç­‰å¾…å®‰è£…å®Œæˆ å†æ¬¡ä½¿ç”¨æ¸…åæºä¸º bundle é…ç½®é•œåƒæºã€‚ å®‰è£… jekyllï¼šåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥gem install jekyll è‡³æ­¤å‡†å¤‡å·¥ä½œå…¨éƒ¨å®Œæˆï½ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:1:1","tags":["jekyll"],"title":"Github Pages ä¸ªäººåšå®¢æ­å»º","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"linux(Debian 10.3) debian ç³»ç»Ÿè‡ªå¸¦å¤§éƒ¨åˆ†åŸºç¡€åŒ… git: sudo apt-get install git ä¸º gem é…ç½®é•œåƒæºï¼Œæ­¥éª¤ä¸ Windows ä¸€æ · å®‰è£… bundler: gem install bundler ä¸º bundler é…ç½®é•œåƒæºï¼ŒåŒä¸Š å®‰è£… jekyll: gem install jekyll è‡³æ­¤å‡†å¤‡å·¥ä½œå…¨éƒ¨å®Œæˆï½ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:1:2","tags":["jekyll"],"title":"Github Pages ä¸ªäººåšå®¢æ­å»º","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"æ­å»º blog ä½¿ç”¨ git clone å°†è‡ªå·±çš„ä»“åº“ clone ä¸‹æ¥ã€‚ å¦‚æœæœ‰èƒ½åŠ›å¯ä»¥è‡ªå·±ç”Ÿæˆä¸€ä¸ªæ–°çš„ jekyll é¡¹ç›®ï¼šæ–°å»ºä¸€ä¸ªç©ºæ–‡ä»¶å¤¹ï¼Œåœ¨æ­¤æ–‡ä»¶å¤¹ä¸­å‘½ä»¤è¡Œè¾“å…¥jekyll new site-nameï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹çš„æ‰€æœ‰æ–‡ä»¶å¤åˆ¶åˆ° clone ä¸‹æ¥çš„ä»“åº“ä¸­ã€‚ ä¹Ÿå¯ä»¥åœ¨æ¨¡æ¿ç½‘ç«™ä¸‹è½½ä¸€ä¸ªä¿®æ”¹é…ç½®ã€‚æˆ‘çš„github pageä½¿ç”¨çš„æ˜¯ Hux å¤§ä½¬çš„æ¨¡æ¿ã€‚ è¿™é‡Œå¯ä»¥å…‹éš†æˆ‘çš„æ¨¡æ¿ git clone https://github.com/SaltFishPr/saltfishpr.github.io.git åœ¨é¡¹ç›®æ–‡ä»¶å¤¹ä¸­æ‰“å¼€ git bash è¾“å…¥bundle installï¼Œç­‰å¾…å®‰è£…å®Œæˆã€‚å¦‚æœå®‰è£…å‡ºç°ä¾èµ–é—®é¢˜ï¼Œå¯ä»¥åœ¨ç™¾åº¦ä¸­æœç´¢ç¼ºå°‘çš„åŒ…çœ‹å¦‚ä½•å®‰è£…ï¼Œéƒ½å¯ä»¥æ‰¾åˆ°è§£å†³ç­”æ¡ˆã€‚ é¡¹ç›®æ–‡ä»¶çš„ç¼–è¾‘æ¨èä½¿ç”¨ vscodeã€‚ç”¨ vscode æ‰“å¼€æ–‡ä»¶å¤¹åï¼Œå¯ä»¥çœ‹åˆ°æ–‡ä»¶å¤¹çš„ç›®å½•ç»“æ„: â”œâ”€â”€ 404.html â”œâ”€â”€ about.html â”œâ”€â”€ archive.html â”œâ”€â”€ CNAME â”œâ”€â”€ _config.yml â”œâ”€â”€ css â”œâ”€â”€ feed.xml â”œâ”€â”€ fonts â”œâ”€â”€ Gemfile â”œâ”€â”€ Gemfile.lock â”œâ”€â”€ Gruntfile.js â”œâ”€â”€ img â”œâ”€â”€ _includes â”œâ”€â”€ index.html â”œâ”€â”€ js â”œâ”€â”€ _layouts â”œâ”€â”€ less â”œâ”€â”€ LICENSE â”œâ”€â”€ offline.html â”œâ”€â”€ _posts â”œâ”€â”€ pwa â”œâ”€â”€ README.md â”œâ”€â”€ _site â””â”€â”€ sw.js è¿™é‡Œä¸»è¦ä¿®æ”¹é…ç½®æ–‡ä»¶_config.ymlã€‚ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:2:0","tags":["jekyll"],"title":"Github Pages ä¸ªäººåšå®¢æ­å»º","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"è¿è¡Œæœ¬åœ°æœåŠ¡ åœ¨é¡¹ç›®æ ¹ç›®å½•å‘½ä»¤è¡Œä¸­è¾“å…¥bundle exec jekyll server --watchï¼Œåœ¨æœ¬åœ°å¯åŠ¨ jekyll æœåŠ¡å™¨ï¼Œæµè§ˆå™¨è¾“å…¥ 127.0.0.1:4000 æœ¬åœ°æŸ¥çœ‹ blogã€‚ å¯¹ç…§_config.yml ä¿®æ”¹ï¼›å–„ç”¨ ctrl+shift+F å…¨å±€æœç´¢ã€‚ä¿®æ”¹å‡ºå±äºè‡ªå·±çš„ blog å§ï¼ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:3:0","tags":["jekyll"],"title":"Github Pages ä¸ªäººåšå®¢æ­å»º","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"å†™åœ¨æœ€å ç”±äºå»ºç«‹ blog æ˜¯åœ¨ä¸€ä¸ªæ˜ŸæœŸå‰äº†ï¼Œå¯èƒ½æœ‰äº›å°æ­¥éª¤è®°ä¸æ¸…æ¥šï¼Œæœ‰ä»€ä¹ˆé—®é¢˜æ¬¢è¿ä¸‹æ–¹ issue æŒ‡æ­£ O(âˆ©_âˆ©)Oã€‚ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:4:0","tags":["jekyll"],"title":"Github Pages ä¸ªäººåšå®¢æ­å»º","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"issues ç¼ºå°‘ ruby.in : sudo apt-get install ruby-dev zlib is missing; necessary for building libxml2 : sudo apt-get install zlib1g zlib1g.dev å‚è€ƒèµ„æ–™ï¼š jekyll ç›®å½•ç»“æ„ github pages æ–‡æ¡£ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:5:0","tags":["jekyll"],"title":"Github Pages ä¸ªäººåšå®¢æ­å»º","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["python"],"content":"ã€ŠPython æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µã€‹å­¦ä¹ ç¬”è®°","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ä¹¦ä¸Šçš„æºç åœ¨å®˜ç½‘ä¸Šå¯ä»¥æ³¨å†Œè´¦å·ä¸‹è½½ï¼Œè¿™é‡Œåªä¸ºè®°å½•è‡ªå·±çš„å­¦ä¹ è¿‡ç¨‹ã€‚ å¦‚æœæœ‰ä¾µæƒæƒ…å†µï¼Œè¯·ç»™æˆ‘å‘é‚®ä»¶é€šçŸ¥æˆ‘åˆ é™¤ 526191197@qq.com æ­¤ç¬”è®°çš„ä»£ç å‡åœ¨ pycharm - python3.8 ä¸­è¿è¡Œé€šè¿‡ å­¦ä¹ æ•°æ®æŒ–æ˜ï¼Œè®©æ•°æ®æœåŠ¡äºäººç±» ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:0:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬ä¸€ç«  ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:1:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"äº²å’Œæ€§åˆ†æ äº²å’Œæ€§åˆ†ææ ¹æ®æ ·æœ¬ä¸ªä½“ï¼ˆç‰©ä½“ï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œç¡®å®šä»–ä»¬çš„å…³ç³»äº²ç–ã€‚åº”ç”¨åœºæ™¯æœ‰ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š å‘ç”¨æˆ·æŠ•æ”¾å®šå‘å¹¿å‘Š ä¸ºç”¨æˆ·æä¾›æ¨èï¼ˆå¦‚æ­Œæ›²æ¨èï¼Œç”µå½±æ¨èç­‰ï¼‰ åè¯ï¼š è§„åˆ™ï¼šä¸€æ¡è§„åˆ™ç”±å‰ææ¡ä»¶å’Œç»“è®ºä¸¤éƒ¨åˆ†ç»„æˆ æ”¯æŒåº¦ï¼šæ•°æ®é›†ä¸­è§„åˆ™åº”éªŒçš„æ¬¡æ•° ç½®ä¿¡åº¦ï¼šè§„åˆ™ï¼ˆç»“æœï¼‰å‡ºç°çš„æ¬¡æ•° / æ¡ä»¶å‡ºç°çš„æ¬¡æ•°ï¼ˆæ¡ä»¶ç›¸åŒçš„è§„åˆ™æ•°é‡ï¼‰ï¼Œè¡¡é‡è§„åˆ™çš„å‡†ç¡®ç‡ # -*- coding: utf-8 -*- import numpy as np from collections import defaultdict from operator import itemgetter if __name__ == '__main__': dataset_filename = \"affinity_dataset.txt\" X = np.loadtxt(dataset_filename) n_samples, n_features = X.shape # æ ·æœ¬æ•°ï¼Œç‰¹å¾æ•° features = [\"bread\", \"milk\", \"cheese\", \"apples\", \"bananas\"] # å•†å“ååˆ—è¡¨ # å¦‚æœxxxï¼Œé‚£ä¹ˆxxx å°±æ˜¯ä¸€æ¡è§„åˆ™ã€‚è§„åˆ™ç”±å‰ææ¡ä»¶å’Œç»“è®ºä¸¤éƒ¨åˆ†ç»„æˆ # è¿™é‡Œæ³¨æ„'å¦‚æœä¹°Aåˆ™ä»–ä»¬ä¼šä¹°B'å’Œ'å¦‚æœä¹°Båˆ™ä»–ä»¬ä¼šä¹°A'ä¸æ˜¯ä¸€ä¸ªè§„åˆ™ï¼Œåœ¨ä¸‹é¢çš„å¾ªç¯ä¸­ä½“ç°å‡ºæ¥ valid_rules = defaultdict(int) # è§„åˆ™åº”éªŒ invalid_rules = defaultdict(int) # è§„åˆ™æ— æ•ˆ num_occurences = defaultdict(int) # å•†å“è´­ä¹°æ•°é‡å­—å…¸ for sample in X: # å¯¹æ•°æ®é›†é‡Œçš„æ¯ä¸ªæ¶ˆè´¹è€… for premise in range(n_features): if sample[premise] == 0: # å¦‚æœè¿™ä¸ªå•†å“æ²¡æœ‰ä¹°ï¼Œç»§ç»­çœ‹ä¸‹ä¸€ä¸ªå•†å“ continue num_occurences[premise] += 1 # è®°å½•è¿™ä¸ªå•†å“è´­ä¹°æ•°é‡ for conclusion in range(n_features): if premise == conclusion: # è·³è¿‡æ­¤å•†å“ continue if sample[conclusion] == 1: valid_rules[(premise, conclusion)] += 1 # è§„åˆ™åº”éªŒ else: invalid_rules[(premise, conclusion)] += 1 # è§„åˆ™æ— æ•ˆ support = valid_rules # æ”¯æŒåº¦å­—å…¸ï¼Œå³è§„åˆ™åº”éªŒæ¬¡æ•° confidence = defaultdict(float) # ç½®ä¿¡åº¦å­—å…¸ for premise, conclusion in valid_rules.keys(): # æ¡ä»¶/ç»“è®º rule = (premise, conclusion) # ç½®ä¿¡åº¦ = è§„åˆ™å‘ç”Ÿçš„æ¬¡æ•°/æ¡ä»¶å‘ç”Ÿçš„æ¬¡æ•° confidence[rule] = valid_rules[rule] / num_occurences[premise] def print_rule(premise, conclusion, support, confidence, features): premise_name = features[premise] conclusion_name = features[conclusion] print( \"Rule: If a person buys {0}they will also buy {1}\".format( premise_name, conclusion_name)) print( \" - Confidence: {0:.3f}\".format(confidence[(premise, conclusion)])) print(\" - Support: {0}\".format(support[(premise, conclusion)])) print(\"\") # å¾—åˆ°æ”¯æŒåº¦æœ€é«˜çš„è§„åˆ™ï¼Œitems()è¿”å›å­—å…¸æ‰€æœ‰å…ƒç´ çš„åˆ—è¡¨ï¼Œitemgetter(1)è¡¨ç¤ºç”¨æ”¯æŒåº¦çš„å€¼ä½œä¸ºé”®ï¼Œè¿›è¡Œé™åºæ’åˆ— sorted_support = sorted(support.items(), key=itemgetter(1), reverse=True) for i in range(5): print(\"Rule #{0}\".format(i + 1)) premise, conclusion = sorted_support[i][0] print_rule(premise, conclusion, support, confidence, features) sorted_confidence = sorted(confidence.items(), key=itemgetter(1), reverse=True) for i in range(5): print(\"Rule #{0}\".format(i + 1)) premise, conclusion = sorted_confidence[i][0] print_rule(premise, conclusion, support, confidence, features) Outputï¼š Rule #1 Rule: If a person buys cheese they will also buy bananas - Confidence: 0.659 - Support: 27 Rule #2 Rule: If a person buys bananas they will also buy cheese - Confidence: 0.458 - Support: 27 Rule #3 Rule: If a person buys cheese they will also buy apples - Confidence: 0.610 - Support: 25 Rule #1 Rule: If a person buys apples they will also buy cheese - Confidence: 0.694 - Support: 25 Rule #2 Rule: If a person buys cheese they will also buy bananas - Confidence: 0.659 - Support: 27 Rule #3 Rule: If a person buys bread they will also buy bananas - Confidence: 0.630 - Support: 17 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:1:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"One Rule ç®—æ³• OneR(One Rule)ç®—æ³•æ ¹æ®å·²æœ‰çš„æ•°æ®ä¸­ï¼Œå…·æœ‰ç›¸åŒç‰¹å¾å€¼çš„ä¸ªä½“æœ€å¯èƒ½å±äºå“ªä¸ªç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚One Rule å°±æ˜¯ä»å››ä¸ªç‰¹å¾ä¸­é€‰æ‹©åˆ†ç±»æ•ˆæœæœ€å¥½çš„å“ªä¸ªä½œä¸ºåˆ†ç±»ä¾æ®ã€‚ å‡å¦‚æ•°æ®é›†çš„æŸä¸€ä¸ªç‰¹å¾å¯ä»¥å– 0 æˆ– 1 ä¸¤ä¸ªå€¼ã€‚æ•°æ®é›†å…±æœ‰ä¸‰ä¸ªç±»åˆ«ã€‚ç‰¹å¾å€¼ä¸º 0 çš„æƒ…å†µä¸‹ï¼ŒA ç±»æœ‰ 20 ä¸ªè¿™æ ·çš„ä¸ªä½“ï¼ŒB ç±»æœ‰ 60 ä¸ªï¼ŒC ç±»ä¹Ÿæœ‰ 20 ä¸ªã€‚é‚£ä¹ˆç‰¹å¾å€¼ä¸º 0 çš„ä¸ªä½“æœ€å¯èƒ½å±äº B ç±»,å½“ç„¶è¿˜æœ‰ 40 ä¸ªä¸ªä½“ç¡®å®æ˜¯ç‰¹å¾å€¼ä¸º 0ï¼Œä½†æ˜¯å®ƒä»¬ä¸å±äº B ç±»ã€‚å°†ç‰¹å¾å€¼ä¸º 0 çš„ä¸ªä½“åˆ†åˆ° B ç±»çš„é”™è¯¯ç‡å°±æ˜¯ 40%ï¼Œå› ä¸ºæœ‰ 40 ä¸ªè¿™æ ·çš„ä¸ªä½“åˆ†åˆ«å±äº A ç±»å’Œ C ç±»ã€‚ç‰¹å¾å€¼ä¸º 1 æ—¶ï¼Œè®¡ç®—æ–¹æ³•ç±»ä¼¼ï¼Œä¸å†èµ˜è¿°ï¼›å…¶ä»–å„ç‰¹å¾å€¼æœ€å¯èƒ½å±äºçš„ç±»åˆ«åŠé”™è¯¯ç‡çš„è®¡ç®—æ–¹æ³•ä¹Ÿä¸€æ ·ã€‚ # -*- coding: utf-8 -*- import numpy as np from sklearn.datasets import load_iris # Irisæ¤ç‰©åˆ†ç±»æ•°æ®é›† from collections import defaultdict # åˆå§‹åŒ–æ•°æ®å­—å…¸ from operator import itemgetter # å¾—åˆ°ä¸€ä¸ªåˆ—è¡¨çš„åˆ¶å®šå…ƒç´  from sklearn.model_selection import train_test_split # å°†ä¸€ä¸ªæ•°æ®é›†ä¸”åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† from sklearn.metrics import classification_report # åˆ†æé¢„æµ‹ç»“æœ # è¿™é‡Œä¿ç•™å‡½æ•°çš„æ–‡æ¡£æ–¹ä¾¿æŸ¥é˜… def train(X, y_true, feature): \"\"\" Computes the predictors and error for a given feature using the OneR algorithm Parameters ---------- X: array [n_samples, n_features] The two dimensional array that holds the dataset. Each row is a sample, each column is a feature. y_true: array [n_samples,] The one dimensional array that holds the class values. Corresponds to X, such that y_true[i] is the class value for sample X[i]. feature: int An integer corresponding to the index of the variable we wish to test. 0 \u003c= variable \u003c n_features Returns ------- predictors: dictionary of tuples: (value, prediction) For each item in the array, if the variable has a given value, make the given prediction. error: float The ratio of training data that this rule incorrectly predicts. \"\"\" # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­— n_samples, n_features = X.shape assert 0 \u003c= feature \u003c n_features # X[:, feature]ä¸ºnumpyçŸ©é˜µçš„ç´¢å¼•ç”¨æ³•ï¼Œç¬¬ä¸€ç»´ï¼šæ‰€æœ‰æ•°ç»„ï¼Œç¬¬äºŒç»´ï¼šfeatureï¼Œsetå»é‡å¾—åˆ°valueæœ‰å‡ ä¸ªå–å€¼ # è¿™ä¸ªfeatureç‰¹å¾å€¼åœ¨æ¯ä¸ªæ•°æ®ä¸­æœ‰å¤šå°‘ä¸ªå–å€¼ values = set(X[:, feature]) # Stores the predictors array that is returned predictors = dict() errors = [] # å¯¹æ¯ä¸ªç‰¹å¾å€¼çš„æ¯ä¸ªå–å€¼è°ƒç”¨train_feature_valueå‡½æ•°è·å¾—è¯¥å–å€¼å‡ºç°æœ€å¤šçš„ç±»å’Œé”™è¯¯ç‡ for current_value in values: most_frequent_class, error = train_feature_value( X, y_true, feature, current_value) predictors[current_value] = most_frequent_class # è¯¥å–å€¼å‡ºç°æœ€å¤šçš„ç±» errors.append(error) # å­˜å‚¨é”™è¯¯ç‡ total_error = sum(errors) # è¿”å›é¢„æµ‹æ–¹æ¡ˆï¼ˆå³featureçš„å–å€¼åˆ†åˆ«å¯¹åº”å“ªä¸ªç±»åˆ«ï¼‰å’Œæ€»é”™è¯¯ç‡ return predictors, total_error def train_feature_value(X, y_true, feature, value): class_counts = defaultdict(int) # Iterate through each sample and count the frequency of each class/value pair # ç¬¬featureä¸ªç‰¹å¾çš„å€¼ä¸ºvalueçš„æ—¶å€™ï¼Œåœ¨æ¯ä¸ªç§ç±»ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œè¿™é‡Œçš„æ¤ç‰©æœ‰ä¸‰ä¸ªç§ç±» # å› æ­¤æœ€ç»ˆclass_countsæœ‰ä¸‰ä¸ªé”®å€¼å¯¹ for sample, y in zip(X, y_true): if sample[feature] == value: class_counts[y] += 1 # å¯¹class_countä»¥valueç”±å¤§åˆ°å°æ’åˆ— sorted_class_counts = sorted( class_counts.items(), key=itemgetter(1), reverse=True) most_frequent_class = sorted_class_counts[0][0] # å‡ºç°æœ€å¤šæ¬¡çš„ç±» n_samples = X.shape[1] error = sum([class_count for class_value, class_count in class_counts.items( ) if class_value != most_frequent_class]) # errorå°±æ˜¯é™¤å»ä¸Šé¢é‚£ä¸ªç±»çš„å…¶å®ƒvalueçš„å’Œ return most_frequent_class, error # è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»å’Œé”™è¯¯ç‡ def predict(X_test, model): variable = model['variable'] # ä½¿ç”¨å“ªä¸ªfeatureä½œä¸ºOneRuleè¿›è¡Œé¢„æµ‹ predictor = model['predictor'] # ä¸€ä¸ªå­—å…¸ï¼Œä¿å­˜ç€featureå–å€¼å¯¹åº”å“ªä¸€ç±» y_predicted = np.array([predictor[int(sample[variable])] for sample in X_test]) return y_predicted # è¿”å›é¢„æµ‹ç»“æœ if __name__ == '__main__': dataset = load_iris() X = dataset.data y = dataset.target n_samples, n_features = X.shape # è®¡ç®—æ¯ä¸ªå±æ€§çš„å‡å€¼ attribute_means = X.mean(axis=0) assert attribute_means.shape == (n_features,) # å¯¹æ•°æ®é›†ç¦»æ•£åŒ– X_d = np.array(X \u003e= attribute_means, dtype='int') random_state = 14 X_train, X_test, y_train, y_test = train_test_split( X_d, y, random_state=random_state) # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† print(\"There are {}training samples\".format(y_train.shape)) # è®­ç»ƒé›†æ•°é‡ print(\"There are {}testing samples\".format(y_test.shape)) # æµ‹è¯•é›†æ•°é‡ # å¯¹æ¯ä¸ªç‰¹å¾è¿”å›é¢„æµ‹å™¨å’Œé”™è¯¯ç‡[0ï¼š{0: x, 1: x}, sum_errorï¼Œ ...] all_predictors = { variable: train( X_train, y_train, variable) for variable in range( X_train.shape[1])} errors = {variable: error for variable, (mapping, error) in all_predictors.items()} # æŠŠæ¯ä¸ªé¢„æµ‹å™¨çš„å€¼æå–å‡ºæ¥ # æ‰¾å‡ºæœ€å¥½ï¼ˆé”™è¯¯æœ€å°‘ï¼‰çš„é‚£ä¸ªfeatureæ„æˆçš„é¢„æµ‹å™¨ best_variable, best_error = sorted(errors.items(), key=itemgetter(1))[0] print( \"The best model is based on variable {0}and has error {1:.2f}%\".format( best_variable","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:1:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬äºŒç«  ä¸»è¦å­¦ä¹ æ•°æ®æŒ–æ˜é€šç”¨æ¡†æ¶çš„æ­å»ºæ–¹æ³• ä¼°è®¡å™¨(Estimator)ï¼šç”¨äºåˆ†ç±»ã€èšç±»å’Œå›å½’åˆ†æ è½¬æ¢å™¨(Transformer)ï¼šç”¨äºæ•°æ®é¢„å¤„ç†å’Œæ•°æ®è½¬æ¢ æµæ°´çº¿(Pipline)ï¼šç»„åˆæ•°æ®æŒ–æ˜æµç¨‹ï¼Œä¾¿äºå†æ¬¡ä½¿ç”¨ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:2:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"scikit-learn ä¼°è®¡å™¨ ä¼°è®¡å™¨ç”¨äºåˆ†ç±»ï¼Œä¸»è¦åŒ…å«ä¸‹é¢ä¸¤ä¸ªå‡½æ•°ï¼š fit(): è®­ç»ƒç®—æ³•ï¼Œè®¾ç½®å†…éƒ¨å‚æ•°ã€‚è¯¥å‡½æ•°æ¥å—è®­ç»ƒé›†å’Œç±»åˆ«ä¸¤ä¸ªå‚æ•° predict(): å‚æ•°ä¸ºæµ‹è¯•é›†ã€‚é¢„æµ‹æµ‹è¯•é›†ç±»åˆ«ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«æµ‹è¯•é›†å„æ¡æ•°æ®ç±»åˆ«çš„æ•°ç»„ è¿‘é‚»ç®—æ³• ç”¨é€”å¹¿æ³› è®¡ç®—é‡å¾ˆå¤§ è·ç¦»åº¦é‡ æ¬§æ°è·ç¦»ï¼šå³çœŸå®è·ç¦» æ›¼å“ˆé¡¿è·ç¦»ï¼šä¸¤ä¸ªç‰¹å¾åœ¨æ ‡å‡†åæ ‡ç³»ä¸­ç»å¯¹è½´è·ä¹‹å’Œ(x1,y1),(x2,y2)å³ abs(x1-x2)+abs(y1-y2) ä½™å¼¦è·ç¦»ï¼šæŒ‡çš„æ˜¯ç‰¹å¾å‘é‡å¤¹è§’çš„ä½™å¼¦å€¼ï¼Œæ›´é€‚åˆè§£å†³å¼‚å¸¸å€¼å’Œæ•°æ®ç¨€ç–é—®é¢˜ã€‚ ç”µç¦»å±‚(Ionosphere)æ•°æ®é›†åˆ†æ Input: # -*- coding: utf-8 -*- import numpy as np import csv from matplotlib import pyplot as plt from sklearn.neighbors import KNeighborsClassifier # å¯¼å…¥Kè¿‘é‚»åˆ†ç±»å™¨ from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score # å¯¼å…¥äº¤å‰æ£€éªŒçš„ # æŠŠæ¯ä¸ªç‰¹å¾å€¼çš„å€¼åŸŸè§„èŒƒåŒ–åˆ°0ï¼Œ1ä¹‹é—´ï¼Œæœ€å°å€¼ç”¨0ä»£æ›¿ï¼Œæœ€å¤§å€¼ç”¨1ä»£æ›¿ from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline # æµæ°´çº¿ if __name__ == '__main__': # æ•°æ®é›†å¤§å°å·²çŸ¥æœ‰351è¡Œï¼Œæ¯è¡Œ35ä¸ªå€¼å‰34ä¸ªä¸ºå¤©çº¿é‡‡é›†çš„æ•°æ®ï¼Œæœ€åä¸€ä¸ª g/b è¡¨ç¤ºæ•°æ®çš„å¥½å X = np.zeros((351, 34), dtype='float') y = np.zeros((351,), dtype='bool') # æ‰“å¼€æ ¹ç›®å½•çš„æ•°æ®é›†æ–‡ä»¶ with open(\"ionosphere.data\", 'r', encoding='utf-8') as input_file: # åˆ›å»ºcsvé˜…è¯»å™¨å¯¹è±¡ reader = csv.reader(input_file) # ä½¿ç”¨æšä¸¾å‡½æ•°ä¸ºæ¯è¡Œæ•°æ®åˆ›å»ºç´¢å¼• for i, row in enumerate(reader): # è·å–è¡Œæ•°æ®çš„å‰34ä¸ªå€¼ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæµ®ç‚¹å‹ï¼Œä¿å­˜åœ¨Xä¸­ data = [float(datum) for datum in row[:-1]] # Set the appropriate row in our dataset X[i] = data # æ•°æ®é›† # 1 if the class is 'g', 0 otherwise y[i] = row[-1] == 'g' # ç±»åˆ« # åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=14) print( \"There are {}samples in the training dataset\".format( X_train.shape[0])) print( \"There are {}samples in the testing dataset\".format( X_test.shape[0])) print(\"Each sample has {}features\".format(X_train.shape[1])) Output: There are 263 samples in the training dataset There are 88 samples in the testing dataset Each sample has 34 features Input: # åˆå§‹åŒ–ä¸€ä¸ªKè¿‘é‚»åˆ†ç±»å™¨å®ä¾‹ï¼Œè¯¥ç®—æ³•é»˜è®¤é€‰æ‹©5ä¸ªè¿‘é‚»ä½œä¸ºåˆ†ç±»ä¾æ® estimator = KNeighborsClassifier() # ç”¨è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒ estimator.fit(X_train, y_train) # ä½¿ç”¨æµ‹è¯•é›†æµ‹è¯•ç®—æ³•ï¼Œè¯„ä»·å…¶è¡¨ç° y_predicted = estimator.predict(X_test) # å‡†ç¡®æ€§ accuracy = np.mean(y_test == y_predicted) * 100 print(\"The accuracy is {0:.1f}%\".format(accuracy)) # ä½¿ç”¨äº¤å‰æ£€éªŒçš„æ–¹å¼è·å¾—å¹³å‡å‡†ç¡®æ€§ scores = cross_val_score(estimator, X, y, scoring='accuracy') average_accuracy = np.mean(scores) * 100 print(\"The average accuracy is {0:.1f}%\".format(average_accuracy)) Output: The accuracy is 86.4% The average accuracy is 82.6% Input: # è®¾ç½®å‚æ•° # å‚æ•°çš„é€‰å–è·Ÿæ•°æ®é›†çš„ç‰¹å¾æ¯æ¯ç›¸å…³ avg_scores = [] all_scores = [] parameter_values = list(range(1, 21)) for n_neighbors in parameter_values: estimator = KNeighborsClassifier(n_neighbors=n_neighbors) scores = cross_val_score(estimator, X, y, scoring='accuracy') avg_scores.append(np.mean(scores)) all_scores.append(scores) # ä½œå‡ºn_neighborsä¸åŒå–å€¼å’Œåˆ†ç±»æ­£ç¡®ç‡ä¹‹é—´çš„å…³ç³»çš„æŠ˜çº¿å›¾ plt.figure(figsize=(32, 20)) plt.plot(parameter_values, avg_scores, '-o', linewidth=5, markersize=24) plt.show() Output: ç»è¿‡ä¸Šé¢çš„ä¾‹å­ï¼Œå¯ä»¥æ€»ç»“æ•°æ®æŒ–æ˜æœ€ç®€å•åŸºæœ¬çš„æµç¨‹å¦‚ä¸‹ï¼š è½½å…¥æ•°æ®é›†ï¼Œæ•°æ®åˆ†ç±»æå–åˆ°å†…å­˜ä¸­ åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† é€‰æ‹©åˆé€‚çš„ç®—æ³•è¿›è¡Œè®­ç»ƒ ä½¿ç”¨æµ‹è¯•é›†æµ‹è¯•ç®—æ³•ï¼Œè¯„ä¼°å…¶è¡¨ç° ä¸ºäº†ä¿è¯ç®—æ³•çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥å°†å¤§æ•°æ®é›†åˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†ï¼Œé€šè¿‡äº¤å‰æ£€éªŒçš„æ–¹æ³•æµ‹è¯•ç®—æ³•ã€‚ä½¿ç”¨ cross_val_score å‡½æ•°æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚ åœ¨å‚æ•°çš„è®¾ç½®ä¸Šï¼Œå¯ä»¥é’ˆå¯¹ä¸åŒçš„å‚æ•°è¿›è¡Œäº¤å‰æµ‹è¯•ï¼Œä½¿ç”¨å›¾è¡¨ç›´è§‚åœ°è¡¨ç¤ºå‡ºå‚æ•°çš„å½±å“ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:2:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æµæ°´çº¿åœ¨é¢„å¤„ç†ä¸­çš„ä½œç”¨ sckit-learn çš„é¢„å¤„ç†å·¥å…·å«åšè½¬æ¢å™¨Transformer Input: # æ¨¡æ‹Ÿè„æ•°æ® X_broken = np.array(X) X_broken[:, ::2] /= 10 # å¯¹æ¯”ä¸¤ç§æƒ…å†µä¸‹é¢„æµ‹å‡†ç¡®ç‡ estimator = KNeighborsClassifier() original_scores = cross_val_score(estimator, X, y, scoring='accuracy') print( \"The original average accuracy for is {0:.1f}%\".format( np.mean(original_scores) * 100)) broken_scores = cross_val_score(estimator, X_broken, y, scoring='accuracy') print( \"The broken average accuracy for is {0:.1f}%\".format( np.mean(broken_scores) * 100)) Output: The original average accuracy for is 82.6% The broken average accuracy for is 73.8% Input: # ç»„åˆæˆä¸ºä¸€ä¸ªå·¥ä½œæµ X_transformed = MinMaxScaler.fit_transform(X_broken) # å®Œæˆè®­ç»ƒå’Œè½¬æ¢ estimator = KNeighborsClassifier() transformed_scores = cross_val_score( estimator, X_transformed, y, scoring='accuracy') print(\"The average accuracy for is {0:.1f}%\".format( np.mean(transformed_scores) * 100)) Output: The average accuracy for is 82.9% å°†æ•°æ®ç»è¿‡è§„èŒƒåŒ–åï¼Œæ­£ç¡®ç‡å†æ¬¡æé«˜ å…¶å®ƒçš„è§„èŒƒåŒ–å‡½æ•°ä¸¾ä¾‹ï¼š ä¸ºä½¿æ¯æ¡æ•°æ®å„ç‰¹å¾å€¼çš„å’Œä¸º 1ï¼šsklearn.preprocessing.Normalizer ä¸ºä½¿å„ç‰¹å¾å€¼çš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼šsklearn.preprocessing.StandardScaler ä¸ºå°†æ•°å€¼å‹ç‰¹å¾äºŒå€¼åŒ–ï¼šsklearn.preprocessing.Binarizer ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:2:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æµæ°´çº¿ sklearn.pipeline.Pipelineç”¨äºåˆ›å»ºæµæ°´çº¿ã€‚æµæ°´çº¿çš„è¾“å…¥ä¸ºä¸€è¿ä¸²çš„æ•°æ®æŒ–æ˜æ­¥éª¤ï¼Œæœ€åä¸€æ­¥å¿…é¡»æ˜¯ä¼°è®¡å™¨ï¼Œå‰å‡ æ­¥æ˜¯è½¬æ¢å™¨ã€‚ Input: # åˆ›å»ºæµæ°´çº¿ # æµæ°´çº¿çš„æ¯ä¸€æ­¥éƒ½ç”¨('åç§°',æ­¥éª¤)çš„å…ƒç»„è¡¨ç¤º scaling_pipeline = Pipeline([('scale', MinMaxScaler()), # è§„èŒƒç‰¹å¾å–å€¼ ('predict', KNeighborsClassifier())]) # é¢„æµ‹ # è°ƒç”¨æµæ°´çº¿ scores = cross_val_score(scaling_pipeline, X_broken, y, scoring='accuracy') print( \"The pipelin scored an average accuracy for is {0:.1f}%\".format( np.mean(scores) * 100)) Output: The pipelin scored an average accuracy for is 82.9% ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:2:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬ä¸‰ç«  å†³ç­–æ ‘ä¹Ÿæ˜¯ä¸€ç§åˆ†ç±»ç®—æ³•ï¼Œå®ƒçš„ä¼˜ç‚¹å¦‚ä¸‹ï¼š æœºå™¨å’Œäººéƒ½èƒ½çœ‹æ‡‚ èƒ½å¤Ÿå¤„ç†å¤šç§ä¸åŒçš„ç‰¹å¾ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"åŠ è½½æ•°æ®é›† pandas(Python Data Analysis çš„ç®€å†™) é€—å·åˆ†éš”å€¼ï¼ˆComma-Separated Valuesï¼ŒCSVï¼Œæœ‰æ—¶ä¹Ÿç§°ä¸ºå­—ç¬¦åˆ†éš”å€¼ï¼Œå› ä¸ºåˆ†éš”å­—ç¬¦ä¹Ÿå¯ä»¥ä¸æ˜¯é€—å·ï¼‰ï¼Œå…¶æ–‡ä»¶ä»¥çº¯æ–‡æœ¬å½¢å¼å­˜å‚¨è¡¨æ ¼æ•°æ®ï¼ˆæ•°å­—å’Œæ–‡æœ¬ï¼‰ï¼Œæ¥æºç™¾åº¦ç™¾ç§‘ã€‚ è¿™é‡Œä½¿ç”¨ pandas å¯¼å…¥.csv æ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä¸ª dataframe ï¼ˆæ•°æ®æ¡†ï¼‰çš„ç±»ã€‚å¯¼å…¥ä½¿ç”¨ read_csv() å‡½æ•°ï¼Œå¸¸ç”¨å‚æ•°å¦‚ä¸‹ï¼š sep=',' ä»¥,ä¸ºæ•°æ®åˆ†éš”ç¬¦ parse_dates='col_name' å°†æŸä¸ªç‰¹å¾å€¼è¯»å–ä¸ºæ—¥æœŸæ ¼å¼ error_bad_lines=False å½“æŸè¡Œæ•°æ®æœ‰é—®é¢˜æ—¶ï¼Œè·³è¿‡è€Œä¸æŠ¥é”™ skiprows=[\u003cparam\u003e] è·³è¿‡åˆ—è¡¨ä¸­æ‰€åŒ…æ‹¬çš„è¡Œï¼Œå‚æ•°å¯ä»¥æ˜¯ 0,1,â€¦çš„æ•°å­—åºåˆ—ï¼Œä¹Ÿå¯ä»¥ç”¨åˆ‡ç‰‡è¡¨è¾¾å¼[0:] usecols=[\u003cparam\u003e] é€‰æ‹©ä½¿ç”¨å“ªå‡ ä¸ªç‰¹å¾å€¼ï¼Œå‚æ•°åŒä¸Š åœ¨ä½¿ç”¨ dataframe.ix[]è·å– dataframe ä¸­çš„æŸå‡ è¡Œæ•°æ®æ—¶ï¼Œæç¤ºé”™è¯¯ä¿¡æ¯ï¼ŒåŸå› æ˜¯ pandas åœ¨ 0.20.0 ç‰ˆæœ¬åå°±åºŸå¼ƒæ‰äº†è¿™ä¸ªå‡½æ•°ã€‚åœ¨è¿™é‡Œæˆ‘æ”¹ä¸ºä½¿ç”¨ iloc å‡½æ•°ã€‚ Input: # -*- coding: utf-8 -*- import numpy as np import pandas as pd from collections import defaultdict from sklearn.tree import DecisionTreeClassifier # åˆ›å»ºå†³ç­–æ ‘çš„ç±» from sklearn.model_selection import cross_val_score from sklearn.preprocessing import LabelEncoder # èƒ½å°†å­—ç¬¦ä¸²ç±»å‹çš„ç‰¹å¾è½¬åŒ–æˆæ•´å‹ from sklearn.preprocessing import OneHotEncoder # å°†ç‰¹å¾è½¬åŒ–ä¸ºäºŒè¿›åˆ¶æ•°å­— from sklearn.ensemble import RandomForestClassifier # éšæœºæ£®æ— from sklearn.model_selection import GridSearchCV # ç½‘æ ¼æœç´¢ï¼Œæ‰¾åˆ°æœ€ä½³å‚æ•° if __name__ == '__main__': # æ¸…æ´—æ•°æ®é›† results = pd.read_csv( \"NBA_data.csv\", parse_dates=[\"Date\"], skiprows=[ 0, ], usecols=[ 0, 2, 3, 4, 5, 6, 7, 9]) # åŠ è½½æ•°æ®é›† # ä¿®å¤æ•°æ®ç‰¹å¾å results.columns = [ \"Date\", \"Visitor Team\", \"VisitorPts\", \"Home Team\", \"HomePts\", \"Score Type\", \"OT?\", \"Notes\"] # results.ix[]å·²è¢«å¼ƒç”¨ print(results.loc[:5]) # æŸ¥çœ‹æ•°æ®é›†å‰äº”è¡Œ Output: Date Visitor Team VisitorPts ... Score Type OT? Notes 0 2013-10-29 Orlando Magic 87 ... Box Score NaN NaN 1 2013-10-29 Chicago Bulls 95 ... Box Score NaN NaN 2 2013-10-29 Los Angeles Clippers 103 ... Box Score NaN NaN 3 2013-10-30 Brooklyn Nets 94 ... Box Score NaN NaN 4 2013-10-30 Boston Celtics 87 ... Box Score NaN NaN 5 2013-10-30 Miami Heat 110 ... Box Score NaN NaN [6 rows x 8 columns] ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"å†³ç­–æ ‘ åˆ›å»ºæ–°çš„ç‰¹å¾åˆ—ï¼Œå¯ä»¥ä»æ•°æ®é›†ä¸­å¯¼å…¥ï¼š dataset[\"New Feature\"] = feature_creator() ä¹Ÿå¯ä»¥ä¸€å¼€å§‹ä¸ºæ–°ç‰¹å¾å€¼è®¾ç½®é»˜è®¤çš„å€¼ï¼š dataset[\"My New Feature\"] = 0 è¿™é‡Œçš„ X_previouswins = results[[\"HomeLastWin\", \"VisitorLastWin\"]].values ç”Ÿæˆä¸€ä¸ªæ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†æœ‰ä¸¤ä¸ªç‰¹å¾ DecisionTreeClassifier() ç”¨æ¥åˆ›å»ºå†³ç­–æ ‘ï¼Œå¸¸ç”¨å‚æ•°å¦‚ä¸‹ï¼š min_samples_split: æŒ‡å®šäº†åˆ›å»ºä¸€ä¸ªæ–°èŠ‚ç‚¹è‡³å°‘éœ€è¦å¤šå°‘ä¸ªä¸ªä½“ min_samples_leaf: æŒ‡å®šä¸ºäº†ä¿ç•™èŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹è‡³å°‘åº”è¯¥åŒ…å«çš„ä¸ªä½“æ•°é‡ åˆ›å»ºå†³ç­–çš„æ ‡å‡†: åŸºå°¼ä¸çº¯åº¦/ä¿¡æ¯å¢ç›Š Input: # æå–æ–°ç‰¹å¾ï¼Œå€¼ä¸ºè¿™åœºä¸­ä¸»åœºé˜Ÿä¼æ˜¯å¦èƒœåˆ© results[\"HomeWin\"] = results[\"VisitorPts\"] \u003c results[\"HomePts\"] y_true = results[\"HomeWin\"].values # èƒœè´Ÿæƒ…å†µ # åˆ›å»ºä¸¤ä¸ªæ–°featureï¼Œåˆå§‹å€¼éƒ½è®¾ä¸º0ï¼Œä¿å­˜è¿™åœºæ¯”èµ›çš„ä¸¤ä¸ªé˜Ÿä¼ä¸Šåœºæ¯”èµ›çš„æƒ…å†µ results[\"HomeLastWin\"] = False results[\"VisitorLastWin\"] = False won_last = defaultdict(int) for index, row in results.iterrows(): home_team = row[\"Home Team\"] visitor_team = row[\"Visitor Team\"] # è¿™åœºæ¯”èµ›ä¹‹å‰ä¸¤ä¸ªçƒé˜Ÿä¸Šæ¬¡æ˜¯å¦è·èƒœä¿å­˜åœ¨resultä¸­ row[\"HomeLastWin\"] = won_last[home_team] row[\"VisitorLastWin\"] = won_last[visitor_team] results.iloc[index] = row # è¿™åœºæ¯”èµ›çš„ç»“æœæ›´æ–°won_lastä¸­çš„æƒ…å†µ won_last[home_team] = row[\"HomeWin\"] won_last[visitor_team] = not row[\"HomeWin\"] X_previouswins = results[[\"HomeLastWin\", \"VisitorLastWin\"]].values # åˆ›å»ºå†³ç­–æ ‘ç”Ÿæˆå™¨å®ä¾‹ clf = DecisionTreeClassifier(random_state=14) # äº¤å‰è®­ç»ƒ scores = cross_val_score(clf, X_previouswins, y_true, scoring='accuracy') print(\"Using just the last result from the home and visitor teams\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: Using just the last result from the home and visitor teams Accuracy: 56.4% è¿™é‡Œä¸ºäº†åˆ›å»ºä¸€ä¸ªæ–°çš„ç‰¹å¾å¯¼å…¥äº†ä¸Šä¸€å¹´çš„ NBA æ’åã€‚ Input: ladder = pd.read_csv(\"NBA_standings.csv\", skiprows=[0, ]) # åˆ›å»ºä¸€ä¸ªæ–°ç‰¹å¾ï¼Œä¸¤ä¸ªé˜Ÿä¼åœ¨ä¸Šä¸ªèµ›å­£çš„æ’åå“ªä¸ªæ¯”è¾ƒé«˜ results[\"HomeTeamRanksHigher\"] = 0 for index, row in results.iterrows(): home_team = row[\"Home Team\"] visitor_team = row[\"Visitor Team\"] # è¿™ä¸ªçƒé˜Ÿæ”¹åäº† if home_team == \"New Orleans Pelicans\": home_team = \"New Orleans Hornets\" elif visitor_team == \"New Orleans Pelicans\": visitor_team = \"New Orleans Hornets\" # è¿™é‡Œæºä»£ç æ— æ³•è¿è¡Œï¼Œå°‘åŠ äº†ä¸€ä¸ªæ‹¬å· ladder[(ladder[\"Team\"] == home_team)] è¡¨ç¤ºæ ¹æ®æ¡ä»¶è·å–è¿™ä¸€è¡Œçš„æ•°æ® home_row = ladder[(ladder[\"Team\"] == home_team)] visitor_row = ladder[(ladder[\"Team\"] == visitor_team)] home_rank = home_row[\"Rk\"].values[0] visitor_rank = visitor_row[\"Rk\"].values[0] row[\"HomeTeamRanksHigher\"] = int(home_rank \u003e visitor_rank) results.iloc[index] = row X_homehigher = results[[\"HomeLastWin\", \"VisitorLastWin\", \"HomeTeamRanksHigher\"]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_homehigher, y_true, scoring='accuracy') print(\"Using whether the home team is ranked higher\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: Using whether the home team is ranked higher Accuracy: 60.0% Input: # åˆ›å»ºæ–°ç‰¹å¾ï¼Œä¸¤ä¸ªé˜Ÿä¼ä¸Šä¸€æ¬¡è¿›è¡Œæ¯”èµ›æ—¶çš„è·èƒœè€… last_match_winner = defaultdict(int) results[\"HomeTeamWonLast\"] = 0 for index, row in results.iterrows(): home_team = row[\"Home Team\"] visitor_team = row[\"Visitor Team\"] # æŒ‰ç…§è‹±æ–‡å­—æ¯è¡¨æ’åºï¼Œä¸å»è€ƒè™‘å“ªä¸ªæ˜¯ä¸»åœºçƒé˜Ÿ teams = tuple(sorted([home_team, visitor_team])) # æ‰¾åˆ°ä¸¤æ”¯çƒé˜Ÿä¸Šæ¬¡æ¯”èµ›çš„èµ¢å®¶ï¼Œæ›´æ–°æ¡†ä¸­çš„æ•°æ®ï¼Œåˆå§‹ä¸º0 # è¿™é‡Œçš„HomeTeamWonLastè·Ÿä¸»åœºå®¢åœºæ²¡æœ‰ä»€ä¹ˆå…³ç³»ï¼Œä¹Ÿå¯ä»¥å«WhichTeamWonLastï¼Œè¿™é‡Œä¸ºäº†å’Œæºç å°½é‡ä¿æŒä¸€è‡´ä½¿ç”¨äº†æºç  row[\"HomeTeamWonLast\"] = 1 if last_match_winner[teams] == row[\"Home Team\"] else 0 results.iloc[index] = row winner = row[\"Home Team\"] if row[\"HomeWin\"] else row[\"Visitor Team\"] # å°†ä¸¤ä¸ªçƒé˜Ÿä¸Šæ¬¡é‡è§æ¯”èµ›çš„æƒ…å†µå­˜åˆ°å­—å…¸ä¸­å» last_match_winner[teams] = winner X_home_higher = results[[\"HomeTeamRanksHigher\", \"HomeTeamWonLast\"]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_home_higher, y_true, scoring='accuracy') print(\"Using whether the home team is ranked higher\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: Using whether the home team is ranked higher Accuracy: 59.9% ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"éšæœºæ£®æ— LabelEncoder() ç”¨æ¥å°†ä¸€ä¸ªå­—ç¬¦ä¸²å‹çš„ç‰¹å¾è½¬åŒ–ä¸ºæ•´å‹ OneHotEncoder() å°†æ•´æ•°è½¬åŒ–æˆæ¶ˆé™¤å·®å¼‚çš„äºŒè¿›åˆ¶æ•°å­—ï¼Œå³å°† 1,2,3 è½¬æ¢æˆ 001,010,100 stacking ï¼ˆå‘é‡ç»„åˆï¼‰ï¼Œè¿™é‡Œ np.vstack() å°†ä¸¤ä¸ªé˜Ÿä¼åå‘é‡çºµå‘ç»„åˆæˆä¸€ä¸ªçŸ©é˜µ.Tè¡¨ç¤ºå°†çŸ©é˜µè½¬ç½® å†³ç­–æ ‘å­˜åœ¨çš„é—®é¢˜ï¼š åˆ›å»ºçš„å¤šé¢—å†³ç­–æ ‘åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç›¸åŒçš„ï¼Œè®­ç»ƒé›†ç›¸åŒï¼Œåˆ™ç”Ÿæˆçš„å†³ç­–æ ‘ä¹Ÿç›¸åŒã€‚ä¸€ä¸ªè§£å†³åŠæ³•æ˜¯è£…è¢‹(bagging) ç”¨äºå‰å‡ ä¸ªå†³ç­–èŠ‚ç‚¹çš„ç‰¹å¾éå¸¸çªå‡ºï¼Œå³ä½¿é‡‡ç”¨ä¸åŒçš„è®­ç»ƒé›†ï¼Œåˆ›å»ºçš„å†³ç­–æ ‘ç›¸ä¼¼æ€§ä¾æ—§å¾ˆå¤§ã€‚è§£å†³åŠæ³•æ˜¯éšæœºé€‰å–éƒ¨åˆ†ç‰¹å¾ä½œä¸ºå†³ç­–æ•°æ® RandomForestClassifier() ç”¨æ¥è°ƒç”¨éšæœºæ£®æ—ç®—æ³•ï¼Œå› ä¸ºå®ƒè°ƒç”¨äº† DecisionTreeClassifier çš„å¤§é‡å®ä¾‹ï¼Œæ‰€ä»¥ä»–ä»¬çš„å‚æ•°æœ‰å¾ˆå¤šæ˜¯ä¸€è‡´çš„ã€‚å…¶å¼•å…¥çš„ä¸€éƒ¨åˆ†æ–°å‚æ•°å¦‚ä¸‹ï¼š n_estimators ç”¨æ¥æŒ‡å®šåˆ›å»ºå†³ç­–æ ‘çš„æ•°é‡ï¼Œå€¼è¶Šé«˜ï¼Œè€—æ—¶è¶Šé•¿ï¼Œå‡†ç¡®ç‡(å¯èƒ½)è¶Šé«˜ oob_score å¦‚æœè®¾ç½®ä¸ºçœŸï¼Œæµ‹è¯•æ—¶å°†ä¸é€‚ç”¨è®­ç»ƒæ¨¡å‹æ—¶ç”¨è¿‡çš„æ•°æ® n_jobs é‡‡ç”¨å¹¶è¡Œç®—æ³•è®­ç»ƒæ—¶æ‰€ç”¨åˆ°çš„å†…æ ¸æ•°é‡ï¼Œè®¾ç½®ä¸º -1 åˆ™å¯ç”¨å…¨éƒ¨å†…æ ¸ Input: # åˆ›å»ºä¸€ä¸ªè½¬åŒ–å™¨å®ä¾‹ encoding = LabelEncoder() # å°†çƒé˜Ÿåè½¬åŒ–ä¸ºæ•´å‹ encoding.fit(results[\"Home Team\"].values) # æŠ½å–æ‰€æœ‰æ¯”èµ›ä¸­ä¸»å®¢åœºçƒé˜Ÿçš„çƒé˜Ÿåï¼Œç»„åˆèµ·æ¥å½¢æˆä¸€ä¸ªçŸ©é˜µ home_teams = encoding.transform(results[\"Home Team\"].values) visitor_teams = encoding.transform(results[\"Visitor Team\"].values) # å»ºç«‹è®­ç»ƒé›†ï¼Œ[[\"Home Team Feature\"ï¼Œ\"Visitor Team Feature\"],[\"Home Team Feature\"ï¼Œ\"Visitor Team Feature\"]...] X_teams = np.vstack([home_teams, visitor_teams]).T # åˆ›å»ºè½¬åŒ–å™¨å®ä¾‹ onehot = OneHotEncoder() # ç”Ÿæˆè½¬åŒ–åçš„ç‰¹å¾ X_teams = onehot.fit_transform(X_teams).todense() clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_teams, y_true, scoring='accuracy') print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_teams, y_true, scoring='accuracy') print(\"Using full team labels is ranked higher\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: Accuracy: 60.5% Using full team labels is ranked higher Accuracy: 61.4% å°†ä¸Šé¢ç”Ÿæˆçš„ç‰¹å¾æ•´åˆèµ·æ¥ï¼Œåˆ›å»ºæ–°çš„å†³ç­–æ–¹æ¡ˆ è¿™é‡Œä½¿ç”¨ np.hstack()æ¨ªå‘æ‹¼æ¥ä¸¤ä¸ªå†³ç­–æ–¹æ¡ˆçŸ©é˜µ Input: X_all = np.hstack([X_home_higher, X_teams]) # å°†ä¸Šé¢è®¡ç®—çš„ç‰¹å¾è¿›è¡Œç»„åˆ print(X_all.shape) scores = cross_val_score(clf, X_all, y_true, scoring='accuracy') print(\"Using whether the home team is ranked higher\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: (1319, 62) Using whether the home team is ranked higher Accuracy: 61.6% ä½¿ç”¨ GridSearchCV ï¼ˆç½‘æ ¼æœç´¢ï¼‰æœç´¢æœ€ä½³å‚æ•° Input: # è®¾ç½®å‚æ•°æœç´¢èŒƒå›´ parameter_space = { \"max_features\": [2, 10, 'auto'], \"n_estimators\": [100, ], \"criterion\": [\"gini\", \"entropy\"], \"min_samples_leaf\": [2, 4, 6], } grid = GridSearchCV(clf, parameter_space) grid.fit(X_all, y_true) print(\"Accuracy: {0:.1f}%\".format(grid.best_score_ * 100)) # è¾“å‡ºæœ€ä½³æ–¹æ¡ˆ print(grid.best_estimator_) Output: Accuracy: 65.6% RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=14, verbose=0, warm_start=False) ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"è¯¾åç»ƒä¹  æ‹¿åˆ°äº†æ•°æ®ï¼Œå¦‚ä½•åˆ›å»ºæ–°çš„ç‰¹å¾ï¼Œå¦‚ä½•åœ¨æ•°æ®ä¸­å‘ç°å…¶å…³é”®ç‚¹ï¼Œå¦‚ä½•æ‰¾å‡ºæ•°æ®å†…éƒ¨çš„è”ç³»ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªéœ€è¦æ–Ÿé…Œçš„æ–¹é¢ åˆ›å»ºä¸‹è¿°ç‰¹å¾å¹¶çœ‹ä¸€ä¸‹æ•ˆæœ: çƒé˜Ÿä¸Šæ¬¡æ‰“æ¯”èµ›è·ä»Šæœ‰å¤šé•¿æ—¶é—´ï¼Ÿ ä¸¤æ”¯çƒé˜Ÿè¿‡å»äº”åœºæ¯”èµ›ç»“æœå¦‚ä½•ï¼Ÿ çƒé˜Ÿæ˜¯ä¸æ˜¯è·ŸæŸæ”¯ç‰¹å®šçƒé˜Ÿæ‰“æ¯”èµ›æ—¶å‘æŒ¥æ›´å¥½ï¼Ÿ åœ¨è¿™é‡Œä½¿ç”¨äº†ä¸Šé¢ä¹¦ä¸­çš„æ–¹æ³•ï¼Œå®Œæˆäº†å‰ä¸¤ä¸ªç‚¹ï¼Œç¬¬ä¸‰ä¸ªç‚¹å®ç°èµ·æ¥æœ‰ç‚¹éº»çƒ¦ï¼Œç°åœ¨åªæœ‰ä¸€ä¸ªæ€è·¯ï¼šå»ºç«‹ä¸€ä¸ªå­—å…¸ï¼Œæ•°æ®å½¢å¼ä¸º (ä¸¤æ”¯çƒé˜Ÿå»ºç«‹ä¸€ä¸ªå…ƒç»„:(å‰ä¸€ä¸ªé˜Ÿä¼è·èƒœçš„æ¬¡æ•°ï¼Œåä¸€ä¸ªé˜Ÿä¼è·èƒœçš„æ¬¡æ•°)) åœ¨å¤„ç† dataset ä¸­çš„æ•°æ®é¡¹æ—¶ï¼Œå¯¹äº pandas ä¸­çš„ Timestamp ç±»å‹æ²¡æœ‰äº†è§£ï¼Œè€—è´¹äº†å¤ªé•¿æ—¶é—´ï¼ŒæŸ¥é˜…æ–‡æ¡£åå‘ç°å¯ä»¥ç”¨ date() å°†å…¶è½¬åŒ–ä¸º datetime.date æ—¥æœŸã€‚ ä½¿ç”¨å‰ä¸¤ä¸ªç‰¹å¾ä½œä¸ºå†³ç­–æ ‡å‡†æ—¶ï¼Œæ•ˆæœè¿˜ç®—å¯ä»¥ï¼ŒåŠ ä¸Šä¹¦ä¸Šçš„æ‰€æœ‰ç‰¹å¾åï¼Œå‡†ç¡®ç‡åè€Œè¾ƒä¸Šé¢çš„ç»“æœé™ä½äº†ã€‚ï¼ˆä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼‰ è¿™ä¸ªâ€œè¯¾åç»ƒä¹ â€ä½¿æˆ‘å¯¹äºæ ‡å‡†åº“äº†è§£åŒ®ä¹çš„çŸ­æ¿æ˜¾ç°å‡ºæ¥ï¼Œè¦æŠ½å‡ºæ—¶é—´å­¦ä¹ ä¸€ä¸‹ python, numpy å’Œ pandas æ ‡å‡†åº“ä¸­å¸¸ç”¨å‡½æ•°åŠå…¶å‚æ•°ã€‚ Input: #!/usr/bin/env python3 # -*- coding: utf-8 -*- import datetime import numpy as np import pandas as pd from collections import defaultdict from sklearn.model_selection import cross_val_score from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from ch3.nba_test import X_all from sklearn.model_selection import GridSearchCV # ç½‘æ ¼æœç´¢ï¼Œæ‰¾åˆ°æœ€ä½³å‚æ•° if __name__ == '__main__': \"\"\" - çƒé˜Ÿä¸Šæ¬¡æ‰“æ¯”èµ›è·ä»Šæœ‰å¤šé•¿æ—¶é—´ï¼Ÿ - ä¸¤æ”¯çƒé˜Ÿè¿‡å»äº”åœºæ¯”èµ›ç»“æœå¦‚ä½•ï¼Ÿ - çƒé˜Ÿæ˜¯ä¸æ˜¯è·ŸæŸæ”¯ç‰¹å®šçƒé˜Ÿæ‰“æ¯”èµ›æ—¶å‘æŒ¥æ›´å¥½ï¼Ÿ \"\"\" dataset = pd.read_csv( \"NBA_data.csv\", parse_dates=[\"Date\"], skiprows=[ 0, ], usecols=[ 0, 2, 3, 4, 5, 6, 7, 9]) # åŠ è½½æ•°æ®é›† dataset.columns = [ \"Date\", \"Visitor Team\", \"VisitorPts\", \"Home Team\", \"HomePts\", \"Score Type\", \"OT?\", \"Notes\"] dataset[\"HomeWin\"] = dataset[\"VisitorPts\"] \u003c dataset[\"HomePts\"] y_true = dataset[\"HomeWin\"].values # èƒœè´Ÿæƒ…å†µ # ä¿å­˜ä¸Šæ¬¡æ‰“æ¯”èµ›çš„æ—¶é—´ last_played_date = defaultdict(datetime.date) # æ‰‹åŠ¨ä¸ºæ¯ä¸ªçƒé˜Ÿåˆå§‹åŒ– for team in set(dataset[\"Home Team\"]): last_played_date[team] = datetime.date(year=2013, month=10, day=25) # ä¸¤æ”¯çƒé˜Ÿè¿‡å»çš„æ¯”èµ›ç»“æœï¼Œæ¯ä¸ªçƒé˜Ÿçš„æ•°æ®æ˜¯[True,False,,,]çš„åºåˆ— last_five_games = defaultdict(list) # å­˜æ”¾Homeå’ŒVisitorå‰äº”æ¬¡æ¯”èµ›çš„è·èƒœæ¬¡æ•° dataset[\"HWinTimes\"] = 0 dataset[\"VWinTimes\"] = 0 # å­˜æ”¾è·ç¦»ä¸Šæ¬¡æ¯”èµ›çš„æ—¶é—´é—´éš”ï¼Œç”¨å¤©è®¡æ•° dataset[\"HLastPlayedSpan\"] = 0 dataset[\"VLastPlayedSpan\"] = 0 for index, row in dataset.iterrows(): home_team = row[\"Home Team\"] visitor_team = row[\"Visitor Team\"] row[\"HWinTimes\"] = sum(last_five_games[home_team][-5:]) row[\"VWinTimes\"] = sum(last_five_games[visitor_team][-5:]) row[\"HLastPlayedSpan\"] = ( row[\"Date\"].date() - last_played_date[home_team]).days row[\"VLastPlayedSpan\"] = ( row[\"Date\"].date() - last_played_date[visitor_team]).days dataset.iloc[index] = row last_played_date[home_team] = row[\"Date\"].date() last_played_date[visitor_team] = row[\"Date\"].date() last_five_games[home_team].append(row[\"HomeWin\"]) last_five_games[visitor_team].append(not row[\"HomeWin\"]) X_1 = dataset[[\"HLastPlayedSpan\", \"VLastPlayedSpan\", \"HWinTimes\", \"VWinTimes\"]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_1, y_true, scoring='accuracy') print(\"DecisionTree: Using time span and win times\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_1, y_true, scoring='accuracy') print(\"RandomForest: Using time span and win times\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) print(\"---------------------------------\") X_all = np.hstack([X_1, X_all]) clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_all, y_true, scoring='accuracy') print(\"DecisionTree: Using time span and win times\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_all, y_true, scoring='accuracy') print(\"RandomForest: Using time span and win times\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) print(\"---------------------------------\") parameter_space = { \"max_features\": [2, 10, 'auto'], \"n_estimators\": [100, ], \"criterion\": [\"gini\", \"entropy\"], \"min_samples_leaf\": [2, 4, 6], } grid = GridSearchCV(clf, parameter_space) grid.fit(X_all, y_true) print(\"Accuracy: {0:.1f}%\".format(grid.best_score_ * 100)) print(grid.best_estimator_) Output: DecisionTree: Using time span and win times Accuracy: 56.4% RandomForest: Using time span and win times Accuracy: 58.3% --------------------------------- DecisionTree: Using time span and win times Accuracy: 57.2% RandomForest: Using time span and win times Accuracy: 61.0% -","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:4","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬å››ç«  æœ¬ç« é‡ç‚¹ï¼š äº²å’Œæ€§åˆ†æ ç”¨ Apriori ç®—æ³•æŒ–æ˜å…³è”ç‰¹å¾ æ•°æ®ç¨€ç–é—®é¢˜ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"äº²å’Œæ€§åˆ†æ Apriori ç®—æ³•æ˜¯ç»å…¸çš„äº²å’Œæ€§åˆ†æç®—æ³•ï¼Œå®ƒåªä»æ•°æ®é›†ä¸­é¢‘ç¹å‡ºç°çš„å•†å“ä¸­é€‰å–å‡ºå…±åŒå‡ºç°çš„å•†å“ç»„æˆé¢‘ç¹é¡¹é›†ï¼Œé¿å…äº†å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿çš„é—®é¢˜ã€‚ä¸€æ—¦æ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œç”Ÿæˆå…³è”è§„åˆ™å°±å˜å¾—å®¹æ˜“äº†ã€‚ åŸç†ï¼šç¡®ä¿äº†è§„åˆ™åœ¨æ•°æ®é›†ä¸­æœ‰è¶³å¤Ÿçš„æ”¯æŒåº¦ã€‚Apriori ç®—æ³•ä¸€ä¸ªé‡è¦å‚æ•°å°±æ˜¯æœ€å°æ”¯æŒåº¦ï¼Œå¦‚æœæƒ³è¦ç”Ÿæˆ(A,B,C)çš„é¢‘ç¹é¡¹é›†ï¼Œåˆ™å…¶å­é›†å¿…é¡»éƒ½è¦æ»¡è¶³æœ€å°æ”¯æŒåº¦æ ‡å‡†ã€‚ å…¶å®ƒäº²å’Œæ€§ç®—æ³•è¿˜æœ‰ Eclat å’Œé¢‘ç¹é¡¹é›†æŒ–æ˜ç®—æ³•(FP-growth)ã€‚è¿™äº›ç®—æ³•æ¯”èµ·åŸºç¡€çš„ Apriori ç®—æ³•æœ‰å¾ˆå¤šæ”¹è¿›ï¼Œæ€§èƒ½ä¹Ÿæœ‰è¿›ä¸€æ­¥æå‡ã€‚ ç¬¬ä¸€é˜¶æ®µï¼Œä¸º Apriori ç®—æ³•æŒ‡å®šä¸€ä¸ªé¡¹é›†è¦æˆä¸ºé¢‘ç¹é¡¹é›†æ‰€éœ€çš„æœ€å°æ”¯æŒåº¦ã€‚ç¬¬äºŒé˜¶æ®µï¼Œæ ¹æ®ç½®ä¿¡åº¦å–å…³è”è§„åˆ™ï¼Œè®¾å®šæœ€å°ç½®ä¿¡åº¦ï¼Œè¿”å›å¤§äºæ­¤å€¼çš„è§„åˆ™ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç”µå½±æ¨èé—®é¢˜ ä¸‹è½½å¹¶åŠ è½½æ•°æ®é›† Input: import sys import pandas as pd from collections import defaultdict from operator import itemgetter if __name__ == '__main__': # header=None ä¸æŠŠç¬¬ä¸€è¡Œå½“åšè¡¨å¤´ all_ratings = pd.read_csv( \"ml-100k/u.data\", delimiter=\"\\t\", header=None, names=[ \"UserID\", \"MovieID\", \"Rating\", \"Datetime\"]) # è½¬åŒ–æ—¶é—´æˆ³ä¸ºdatetime all_ratings[\"Datetime\"] = pd.to_datetime(all_ratings[\"Datetime\"], unit='s') # è¾“å‡ºç”¨æˆ·-ç”µå½±-è¯„åˆ†ç¨€ç–çŸ©é˜µ print(all_ratings[:5]) print() # åˆ›å»ºFavoriteç‰¹å¾ï¼Œå°†è¯„åˆ†å±æ€§äºŒå€¼åŒ–ä¸ºæ˜¯å¦å–œæ¬¢ all_ratings[\"Favorable\"] = all_ratings[\"Rating\"] \u003e 3 # å–ç”¨æˆ·IDä¸ºå‰200çš„ç”¨æˆ·çš„æ‰“åˆ†æ•°æ® ratings = all_ratings[all_ratings[\"UserID\"].isin(range(200))] favorable_ratings = ratings[ratings[\"Favorable\"]] # åˆ›å»ºç”¨æˆ·å–œæ¬¢å“ªäº›ç”µå½±çš„å­—å…¸ favorable_reviews_by_users = dict( (k, frozenset( v.values)) for k, v in favorable_ratings.groupby(\"UserID\")[\"MovieID\"]) # åˆ›å»ºä¸€ä¸ªæ•°æ®æ¡†ï¼Œäº†è§£æ¯éƒ¨ç”µå½±çš„å½±è¿·æ•°é‡ num_favorable_by_movie = ratings[[ \"MovieID\", \"Favorable\"]].groupby(\"MovieID\").sum() # æŸ¥çœ‹æœ€å—æ¬¢è¿çš„äº”éƒ¨ç”µå½± print(num_favorable_by_movie.sort_values(\"Favorable\", ascending=False)[:5]) Output: UserID MovieID Rating Datetime 0 196 242 3 1997-12-04 15:55:49 1 186 302 3 1998-04-04 19:22:22 2 22 377 1 1997-11-07 07:18:36 3 244 51 2 1997-11-27 05:02:03 4 166 346 1 1998-02-02 05:33:16 Favorable MovieID 50 100.0 100 89.0 258 83.0 181 79.0 174 74.0 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"Apriori ç®—æ³•çš„å®ç° æŠŠå„é¡¹ç›®æ”¾åˆ°åªåŒ…å«è‡ªå·±çš„é¡¹é›†ä¸­ï¼Œç”Ÿæˆæœ€åˆçš„é¢‘ç¹é¡¹é›†ã€‚åªä½¿ç”¨è¾¾åˆ°æœ€å°æ”¯æŒåº¦çš„é¡¹ç›®ã€‚ æŸ¥æ‰¾ç°æœ‰é¢‘ç¹é¡¹é›†çš„è¶…é›†ï¼Œå‘ç°æ–°çš„é¢‘ç¹é¡¹é›†ï¼Œå¹¶ç”¨å…¶ç”Ÿæˆæ–°çš„å¤‡é€‰é¡¹é›†ã€‚ æµ‹è¯•æ–°ç”Ÿæˆçš„å¤‡é€‰é¡¹é›†çš„é¢‘ç¹ç¨‹åº¦ï¼ˆä¸æœ€å°æ”¯æŒåº¦æ¯”è¾ƒï¼‰ï¼Œå¦‚æœä¸å¤Ÿé¢‘ç¹åˆ™èˆå¼ƒã€‚å¦‚æœæ²¡æœ‰æ–°çš„é¢‘ç¹é¡¹é›†ï¼Œå°±è·³åˆ°æœ€åä¸€æ­¥ã€‚ å­˜å‚¨æ–°å‘ç°çš„é¢‘ç¹é¡¹é›†ï¼Œè·³åˆ°æ­¥éª¤ 2 è¿”å›æ‰€æœ‰çš„é¢‘ç¹é¡¹é›† Input: # å­—å…¸ä¿å­˜æœ€æ–°å‘ç°çš„é¢‘ç¹é¡¹é›† frequent_itemsets = {} min_support = 50 # ç¬¬ä¸€æ­¥ï¼Œæ¯ä¸€æ­¥ç”µå½±ç”ŸæˆåªåŒ…å«å®ƒè‡ªå·±çš„é¡¹é›† # frozenset() è¿”å›ä¸€ä¸ªå†»ç»“çš„é›†åˆï¼Œå†»ç»“åé›†åˆä¸èƒ½å†æ·»åŠ æˆ–åˆ é™¤ä»»ä½•å…ƒç´  # æ™®é€šé›†åˆå¯å˜ï¼Œé›†åˆä¸­ä¸èƒ½æœ‰å¯å˜çš„å…ƒç´ ï¼Œå› æ­¤æ™®é€šé›†åˆä¸èƒ½è¢«æ”¾åœ¨é›†åˆä¸­ï¼›å†»ç»“é›†åˆä¸å¯å˜ï¼Œå› æ­¤å¯ä»¥è¢«æ”¾å…¥é›†åˆ frequent_itemsets[1] = dict((frozenset((movie_id,)), row[\"Favorable\"]) for movie_id, row in num_favorable_by_movie.iterrows() if row[\"Favorable\"] \u003e min_support) # ä¼šæœ‰é‡å¤ï¼Œå¯¼è‡´å–œæ¬¢ç”µå½±1,50çš„äººåˆ†åˆ«ä¸º50,100ä½†æ˜¯ {1,50} çš„é›†åˆæœ‰100ä¸ª # ä¸¤ä¸ªåŸå› ï¼Œç¬¬ä¸€åœ¨current_supersetæ—¶é¡¹é›†æœ‰æ—¶å€™ä¼šçªç„¶è°ƒæ¢ä½ç½® def find_frequent_itemsets( favorable_reviews_by_users, k_1_itemsets, min_support): counts = defaultdict(int) # éå†æ¯ä¸€ä¸ªç”¨æˆ·ï¼Œè·å–å…¶å–œæ¬¢çš„ç”µå½± for user, reviews in favorable_reviews_by_users.items(): # éå†æ¯ä¸ªé¡¹é›† for itemset in k_1_itemsets: if itemset.issubset(reviews): # åˆ¤æ–­itemsetæ˜¯å¦æ˜¯ç”¨æˆ·å–œæ¬¢çš„ç”µå½±çš„å­é›† # å¯¹ç”¨æˆ·å–œæ¬¢çš„ç”µå½±ä¸­é™¤äº†è¿™ä¸ªå­é›†çš„ç”µå½±è¿›è¡Œéå† for other_reviewed_movie in reviews - itemset: # å°†è¯¥ç”µå½±å¹¶å…¥é¡¹é›†ä¸­ current_superset = itemset | frozenset( {other_reviewed_movie}) counts[current_superset] += 1 # è¿™ä¸ªé¡¹é›†çš„æ”¯æŒåº¦+1 # è¿”å›å…ƒç´ æ•°ç›®+1çš„é¡¹é›†å’Œæ•°é‡ res = dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency \u003e= min_support]) return res for k in range(2, 20): cur_frequent_itemsets = find_frequent_itemsets( favorable_reviews_by_users, frequent_itemsets[k - 1], min_support) frequent_itemsets[k] = cur_frequent_itemsets if len(cur_frequent_itemsets) == 0: print(\"Did not find any frequent itemsets of length {}\".format(k)) sys.stdout.flush() # å°†ç¼“å†²åŒºå†…å®¹è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œä¸å®œå¤šç”¨ï¼Œè¾“å‡ºæ“ä½œå¸¦æ¥çš„è®¡ç®—å¼€é”€ä¼šæ‹–æ…¢ç¨‹åºè¿è¡Œé€Ÿåº¦ break else: print( \"I found {}frequent itemsets of length {}\".format( len(cur_frequent_itemsets), k)) sys.stdout.flush() # é™¤å»åªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„åˆå§‹é›†åˆ del frequent_itemsets[1] Output: I found 93 frequent itemsets of length 2 I found 295 frequent itemsets of length 3 I found 593 frequent itemsets of length 4 I found 785 frequent itemsets of length 5 I found 677 frequent itemsets of length 6 I found 373 frequent itemsets of length 7 I found 126 frequent itemsets of length 8 I found 24 frequent itemsets of length 9 I found 2 frequent itemsets of length 10 Did not find any frequent itemsets of length 11 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æŠ½å–å…³è”è§„åˆ™ å¯¹æ¯ä¸ªé¢‘ç¹é¡¹é›†ï¼Œé€‰å‡ºå…¶ä¸­çš„ä¸€ä¸ªå…ƒç´ å½“ç»“è®ºï¼Œå‰©ä¸‹çš„å…ƒç´ éƒ½ä½œä¸ºæ¡ä»¶ï¼Œç”Ÿæˆè§„åˆ™ã€‚ Input: # è§„åˆ™å½¢å¼ï¼šå¦‚æœç”¨æˆ·å–œæ¬¢å‰æä¸­çš„æ‰€æœ‰ç”µå½±ï¼Œé‚£ä¹ˆä»–ä»¬ä¹Ÿä¼šå–œæ¬¢ç»“è®ºä¸­çš„ç”µå½± candidate_rules = [] for itemset_length, itemset_counts in frequent_itemsets.items(): for itemset in itemset_counts.keys(): for conclusion in itemset: premise = itemset - {conclusion} candidate_rules.append((premise, conclusion)) print(candidate_rules[:5]) Output: [(frozenset({7}), 1), (frozenset({1}), 7), (frozenset({50}), 1), (frozenset({1}), 50), (frozenset({1}), 56)] ç½®ä¿¡åº¦è®¡ç®—ï¼Œæ–¹æ³•ä¸ç¬¬ä¸€ç« ç±»ä¼¼ã€‚ # è®¡ç®—ç½®ä¿¡åº¦ correct_counts = defaultdict(int) incorrect_counts = defaultdict(int) # éå†æ¯ä¸€ä¸ªç”¨æˆ·ï¼Œè·å–å…¶å–œæ¬¢çš„ç”µå½± for user, reviews in favorable_reviews_by_users.items(): # éå†æ¯ä¸ªè§„åˆ™ for candidate_rule in candidate_rules: # è·å–è§„åˆ™çš„æ¡ä»¶å’Œç»“è®º premise, conclusion = candidate_rule # å¦‚æœæ¡ä»¶æ˜¯å–œæ¬¢ç”µå½±çš„å­é›†ï¼ˆæ¡ä»¶æˆç«‹ï¼‰ if premise.issubset(reviews): # å¦‚æœç”¨æˆ·ä¹Ÿå–œæ¬¢ç»“è®ºçš„ç”µå½± if conclusion in reviews: correct_counts[candidate_rule] += 1 else: incorrect_counts[candidate_rule] += 1 # è®¡ç®—ç½®ä¿¡åº¦ï¼Œç»“è®ºå‘ç”Ÿçš„æ¬¡æ•°é™¤ä»¥æ¡ä»¶å‘ç”Ÿçš„æ¬¡æ•° rule_confidence = { candidate_rule: correct_counts[candidate_rule] / float( correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in candidate_rules} # ç»™ç½®ä¿¡åº¦æ’åº sorted_confidence = sorted( rule_confidence.items(), key=itemgetter(1), reverse=True) for index in range(5): print(\"Rule #{}\".format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] print( \"Rule: If a person recommends {}they will also recommand {}\".format( premise, conclusion)) print( \"- Confidence: {0:.3f}\".format(rule_confidence[(premise, conclusion)])) print(\"--------------------\") Output: Rule #1 Rule: If a person recommends frozenset({98, 181}) they will also recommand 50 - Confidence: 1.000 -------------------- Rule #2 Rule: If a person recommends frozenset({172, 79}) they will also recommand 174 - Confidence: 1.000 -------------------- Rule #3 Rule: If a person recommends frozenset({258, 172}) they will also recommand 174 - Confidence: 1.000 -------------------- Rule #4 Rule: If a person recommends frozenset({1, 181, 7}) they will also recommand 50 - Confidence: 1.000 -------------------- Rule #5 Rule: If a person recommends frozenset({1, 172, 7}) they will also recommand 174 - Confidence: 1.000 -------------------- è°ƒæ•´è¾“å‡ºï¼ŒåŠ ä¸Šç”µå½±å Input: movie_name_data = pd.read_csv( \"ml-100k/u.item\", delimiter='|', header=None, encoding=\"mac-roman\") movie_name_data.columns = [ 'MovieID', 'Title', 'Release Date', 'Video Release', 'IMDB', '\u003cUNK\u003e', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'] for index in range(5): print('Rule #{0}'.format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] premise_names = ', '.join(get_movie_name(idx) for idx in premise) conclusion_name = get_movie_name(conclusion) print( 'Rule: if a person recommends {0}they will also recommend {1}'.format( premise_names, conclusion_name)) print( ' - Confidence: {0:.3f}'.format(rule_confidence[(premise, conclusion)])) print(\"--------------------\") Output: Rule #1 Rule: if a person recommends Silence of the Lambs, The (1991), Return of the Jedi (1983) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #2 Rule: if a person recommends Empire Strikes Back, The (1980), Fugitive, The (1993) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #3 Rule: if a person recommends Contact (1997), Empire Strikes Back, The (1980) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #4 Rule: if a person recommends Toy Story (1995), Return of the Jedi (1983), Twelve Monkeys (1995) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #5 Rule: if a person recommends Toy Story (1995), Empire Strikes Back, The (1980), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:4","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"è¯„ä¼°æµ‹è¯• ä½¿ç”¨å‰©ä¸‹çš„æ•°æ®é›†è®¡ç®—è§„åˆ™çš„ç½®ä¿¡åº¦ï¼Œä¹Ÿæ˜¯æŸ¥çœ‹æ¯æ¡è§„åˆ™è¡¨ç°çš„ä¸€ä¸ªæ–¹æ³•ã€‚ Input: # è¯„ä¼°æµ‹è¯• test_dataset = all_ratings[~all_ratings['UserID'].isin(range(200))] test_favorable = test_dataset[test_dataset[\"Favorable\"]] test_favorable_by_users = dict((k, frozenset(v.values)) for k, v in test_favorable.groupby(\"UserID\")[\"MovieID\"]) correct_counts = defaultdict(int) incorrect_counts = defaultdict(int) for user, reviews in test_favorable_by_users.items(): for candidate_rule in candidate_rules: premise, conclusion = candidate_rule if premise.issubset(reviews): if conclusion in reviews: correct_counts[candidate_rule] += 1 else: incorrect_counts[candidate_rule] += 1 test_confidence = { candidate_rule: correct_counts[candidate_rule] / float( correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in rule_confidence} for index in range(5): print(\"Rule #{0}\".format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] premise_names = \", \".join(get_movie_name(idx) for idx in premise) conclusion_name = get_movie_name(conclusion) print( 'Rule: if a person recommends {0}they will also recommend {1}'.format( premise_names, conclusion_name)) print( ' - Confidence: {0:.3f}'.format(rule_confidence[(premise, conclusion)])) print(\"--------------------\") Output: Rule #1 Rule: if a person recommends Silence of the Lambs, The (1991), Return of the Jedi (1983) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #2 Rule: if a person recommends Empire Strikes Back, The (1980), Fugitive, The (1993) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #3 Rule: if a person recommends Contact (1997), Empire Strikes Back, The (1980) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #4 Rule: if a person recommends Toy Story (1995), Return of the Jedi (1983), Twelve Monkeys (1995) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #5 Rule: if a person recommends Toy Story (1995), Empire Strikes Back, The (1980), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- è¿™ä¸€ç« ç”¨ç”µå½±è¿›è¡Œäº²å’Œåº¦åˆ†æï¼Œç”±äºå…ƒç´ çš„æ•°é‡å˜å¤šäº†ï¼Œæ—¶é—´å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œéå†çš„ç¬¨æ–¹æ³•å·²ç»ä¸é€‚ç”¨ã€‚éœ€è¦å¯»æ‰¾æ›´åŠ å·§å¦™åœ°è§£å†³æ–¹æ¡ˆã€‚ åœ¨ç”¨é›†åˆè®¡ç®—ç”µå½±çš„é¡¹é›†æ—¶ï¼Œ{1, 2} ä¸ {2, 1} æ˜¯åŒä¸€ä¸ªäº‹ä»¶ï¼Œä½†åœ¨éå†çš„æ—¶å€™ä¼šè¢«å¤šæ¬¡è®¡ç®—ï¼Œå¯èƒ½è¿™æ˜¯ä¸€ä¸ªé”™è¯¯çš„ç‚¹ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:5","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬äº”ç«  æœ¬ç« è®¨è®ºå¦‚ä½•ä»æ•°æ®é›†ä¸­æŠ½å–æ•°å€¼å’Œç±»åˆ«å‹ç‰¹å¾ï¼Œå¹¶é€‰å‡ºæœ€ä½³ç‰¹å¾ã€‚è¿˜ä¼šä»‹ç»ç‰¹å¾æŠ½å–çš„å¸¸ç”¨æ¨¡å¼å’ŒæŠ€å·§ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç‰¹å¾æŠ½å– æŠŠå®ä½“ç”¨ç‰¹å¾è¡¨ç¤ºå‡ºæ¥ï¼Œé€šè¿‡ç‰¹å¾å»ºæ¨¡ï¼Œå†é€šè¿‡æœºå™¨æŒ–æ˜ç®—æ³•èƒ½å¤Ÿç†è§£çš„è¿‘ä¼¼æ–¹å¼æ¥è¡¨ç¤ºç°å®ã€‚ ç‰¹å¾å¯ä»¥æ˜¯æ•°å€¼å‹æˆ–ç±»åˆ«å‹ã€‚æ•°å€¼ç‰¹å¾å¯ä»¥ç¦»æ•£åŒ–ç”Ÿæˆç±»åˆ«ç‰¹å¾ã€‚ Input: import numpy as np import pandas as pd if __name__ == '__main__': adult = pd.read_csv(\"adult.data\", header=None, names=[\"Age\", \"Work-Class\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-gain\", \"Capital-loss\", \"Hours-per-week\", \"Native-Country\", \"Earnings-Raw\"]) # å»é™¤ç©ºå€¼ adult.dropna(how='all', inplace=True) # è¾“å‡ºè¯¦ç»†æè¿° print(adult[\"Hours-per-week\"].describe()) # è¾“å‡ºä¸­ä½æ•° print(adult[\"Education-Num\"].median()) # è¾“å‡ºå·¥ä½œçš„ç§ç±» print(adult[\"Work-Class\"].unique()) # å°†å·¥ä½œæ—¶é•¿äºŒå€¼åŒ–ä¸ºæ˜¯å¦è¶…è¿‡40h adult[\"LongHours\"] = adult[\"Hours-per-week\"] \u003e 40 Output: count 32561.000000 mean 40.437456 std 12.347429 min 1.000000 25% 40.000000 50% 40.000000 75% 45.000000 max 99.000000 Name: Hours-per-week, dtype: float64 10.0 [' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov' ' ?' ' Self-emp-inc' ' Without-pay' ' Never-worked'] ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç‰¹å¾é€‰æ‹© å®ç‰©çš„ç‰¹å¾æœ‰å¾ˆå¤šï¼Œæˆ‘ä»¬åªé€‰æ‹©å…¶ä¸­ä¸€å°éƒ¨åˆ†ã€‚ é™ä½å¤æ‚åº¦ï¼Œæé«˜ç®—æ³•è¿è¡Œé€Ÿåº¦ å‡ä½å™ªéŸ³ï¼Œå¢åŠ æ— å…³çš„ç‰¹å¾ä¼šå¹²æ‰°ç®—æ³•çš„å·¥ä½œ å¢åŠ æ¨¡å‹å¯è¯»æ€§ï¼Œç‰¹å¾è¾ƒå°‘ï¼Œäººä»¬æ˜“äºç†è§£ æ‹¿åˆ°æ•°æ®åï¼Œå…ˆè¿›è¡Œç®€å•ç›´æ¥çš„åˆ†æï¼Œäº†è§£æ•°æ®çš„ç‰¹ç‚¹ã€‚ sklearn.feature_selection.VarianceThreshold è½¬æ¢å™¨å¯ä»¥ç”¨æ¥åˆ é™¤ç‰¹å¾å€¼çš„æ–¹å·®è¾¾ä¸åˆ°æœ€ä½æ ‡å‡†çš„ç‰¹å¾ã€‚ Input: # æ„é€ æµ‹è¯•æ•°æ®é›† X = np.arange(30).reshape((10, 3)) X[:, 1] = 1 print(X) print(\"----------------\") vt = VarianceThreshold() Xt = vt.fit_transform(X) # ç¬¬äºŒåˆ—æ¶ˆå¤±äº†ï¼Œå› ä¸ºç¬¬äºŒåˆ—éƒ½æ˜¯1ï¼Œæ–¹å·®ä¸º0ï¼Œä¸åŒ…æ‹¬å…·æœ‰åŒºåˆ«æ„ä¹‰çš„ä¿¡æ¯ print(Xt) print(\"----------------\") print(vt.variances_) Output: [[ 0 1 2] [ 3 1 5] [ 6 1 8] [ 9 1 11] [12 1 14] [15 1 17] [18 1 20] [21 1 23] [24 1 26] [27 1 29]] ---------------- [[ 0 2] [ 3 5] [ 6 8] [ 9 11] [12 14] [15 17] [18 20] [21 23] [24 26] [27 29]] ---------------- [27. 0. 27.] é€‰æ‹©æœ€ä½³ç‰¹å¾ éšç€ç‰¹å¾æ•°é‡çš„å¢åŠ ï¼Œå¯»æ‰¾æœ€ä½³ç‰¹å¾ç»„åˆçš„ä»»åŠ¡å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚åˆ†ç±»ä»»åŠ¡é€šå¸¸çš„åšæ³•æ˜¯å¯»æ‰¾è¡¨ç°å¥½çš„å•ä¸ªç‰¹å¾ï¼Œä¾æ®æ˜¯ä»–ä»¬èƒ½è¾¾åˆ°çš„ç²¾ç¡®åº¦ã€‚ scikit-learn æä¾›äº†å‡ ä¸ªç”¨äºé€‰æ‹©å•å˜é‡ç‰¹å¾çš„è½¬æ¢å™¨ã€‚ SelectKBest è¿”å› k ä¸ªæœ€ä½³ç‰¹å¾ SelectPercentile è¿”å›è¡¨ç°æœ€ä½³çš„ r%ä¸ªç‰¹å¾ è¿™ä¸¤ä¸ªè½¬æ¢å™¨éƒ½æä¾›è®¡ç®—ç‰¹å¾è¡¨ç°çš„ä¸€ç³»åˆ—æ–¹æ³•ã€‚ å•ä¸ªç‰¹å¾å’ŒæŸä¸€ç±»åˆ«ä¹‹é—´çš„ç›¸å…³æ€§è®¡ç®—æ–¹æ³•æœ‰å¡æ–¹æ£€éªŒ(xÂ²)ã€äº’ä¿¡æ¯å’Œä¿¡æ¯ç†µç­‰ã€‚ Input: # æ„é€ æ•°æ®é›† X = adult[[\"Age\", \"Education-Num\", \"Capital-gain\", \"Capital-loss\", \"Hours-per-week\"]] y = (adult[\"Earnings-Raw\"] == ' \u003e50K').values # ä½¿ç”¨SelectKBestè½¬æ¢å™¨ï¼Œç”¨å¡æ–¹æ‰“åˆ† transformer = SelectKBest(score_func=chi2, k=3) # è°ƒç”¨fit_transformæ–¹æ³•å¯¹ç›¸åŒçš„æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†å’Œè½¬æ¢ Xt_chi2 = transformer.fit_transform(X, y) # è¾“å‡ºæ¯ä¸ªç‰¹å¾çš„å¾—åˆ† print(transformer.scores_) print(\"----------------\") # ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°è®¡ç®—ç›¸å…³æ€§,åˆ›å»ºåŒ…è£…å‡½æ•° def mutivariate_pearsonr(X, y): scores, pvalues = [], [] for column in range(X.shape[1]): cur_score, cur_p = pearsonr(X[:, column], y) scores.append(abs(cur_score)) pvalues.append(cur_p) return np.array(scores), np.array(pvalues) transformer = SelectKBest(score_func=mutivariate_pearsonr, k=3) Xt_pearson = transformer.fit_transform(X, y) print(transformer.scores_) print(\"----------------\") clf = DecisionTreeClassifier(random_state=14) scores_chi2 = cross_val_score(clf, Xt_chi2, y, scoring='accuracy') scores_pearson = cross_val_score(clf, Xt_pearson, y, scoring='accuracy') print('å¡æ–¹: {}'.format(np.mean(scores_chi2))) print(\"----------------\") print(\"pearson: {}\".format(np.mean(scores_pearson))) Output: [8.60061182e+03 2.40142178e+03 8.21924671e+07 1.37214589e+06 6.47640900e+03] ---------------- [0.2340371 0.33515395 0.22332882 0.15052631 0.22968907] ---------------- å¡æ–¹: 0.8291514400795839 ---------------- pearson: 0.7721507467016449 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"åˆ›å»ºç‰¹å¾ ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§å¾ˆå¼ºï¼Œæˆ–è€…ç‰¹å¾å†—ä½™ï¼Œä¼šå¢åŠ ç®—æ³•å¤„ç†éš¾åº¦ã€‚ è¿™é‡Œåœ¨åŠ è½½ ad æ•°æ®é›†ä¹‹å‰å…ˆåˆ›å»ºäº†ä¸€ä¸ªè½¬æ¢å™¨ï¼Œç”¨äºåœ¨åŠ è½½æ—¶è½¬æ¢æ•°æ®é›†ä¸­çš„å€¼ã€‚ æºç è¿è¡Œä¼šäº§ç”ŸæŠ¥é”™ï¼Œç¬¬ä¸€ä¸ªåŸå› æ˜¯ï¼Œç”¨å‡½æ•°åˆå§‹åŒ–è½¬æ¢å™¨å¹¶æ²¡æœ‰æŠŠå‡½æ•°åä¼ å…¥ï¼Œå› æ­¤å°† defaultdict ä¸­æ¯ä¸€ä¸ªç´¢å¼•éƒ½è¿›è¡Œäº†åˆå§‹åŒ–ã€‚ç¬¬äºŒä¸ªåŸå› æ˜¯ï¼ŒPCA è½¬æ¢å™¨æ— æ³•å¯¹ NaN æ•°æ®è¿›è¡Œå¤„ç†ï¼Œäºæ˜¯æˆ‘åœ¨å¤„ç”Ÿæˆæ•°æ®é›†ä¹‹å‰å°†æ‰€æœ‰å«æœ‰ NaN çš„è¡Œåˆ æ‰ã€‚ Input: # -*- coding: utf-8 -*- import numpy as np import pandas as pd from collections import defaultdict from sklearn.decomposition import PCA from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score from matplotlib import pyplot as plt # åˆ›å»ºè½¬æ¢å‡½æ•° def convert_number(x): try: res = float(x) return res except ValueError: return np.nan if __name__ == '__main__': # åˆ›å»ºæ•°æ®åŠ è½½çš„è½¬æ¢å™¨ converters = defaultdict(convert_number, {i: convert_number for i in range(1588)}) converters[1558] = lambda x: 1 if x.strip() == \"ad.\" else 0 # ä½¿ç”¨è½¬æ¢å™¨è¯»å–æ•°æ®é›† temp = pd.read_csv(\"ad.data\", header=None, converters=converters) # åˆ é™¤æ‰€æœ‰å«æœ‰nançš„è¡Œ,axis=0æ˜¯æ•°æ®ç´¢å¼•(index)ï¼Œaxis=1æ˜¯åˆ—æ ‡ç­¾(column) ads = temp.dropna(axis=0, how='any') print(ads[10:15]) Output: 0 1 2 3 4 5 ... 1553 1554 1555 1556 1557 1558 11 90.0 52.0 0.5777 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 12 90.0 60.0 0.6666 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 13 90.0 60.0 0.6666 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 14 33.0 230.0 6.9696 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 15 60.0 468.0 7.8000 1.0 0.0 0.0 ... 0.0 1.0 1.0 0.0 0.0 1 [5 rows x 1559 columns] ä¸»æˆåˆ†åˆ†æ(PCA) ç›®çš„æ˜¯æ‰¾åˆ°èƒ½ç”¨è¾ƒå°‘ä¿¡æ¯æè¿°æ•°æ®é›†çš„ç‰¹å¾ç»„åˆã€‚ä¸»æˆåˆ†çš„æ–¹å·®è·Ÿæ•´ä½“æ–¹å·®æ²¡æœ‰å¤šå¤§å·®è·ã€‚ç»è¿‡åˆ†æä¸»æˆåˆ†ï¼Œç¬¬ä¸€ä¸ªç‰¹å¾çš„æ–¹å·®å¯¹æ•°æ®é›†æ–¹å·®çš„è´¡çŒ®ç‡ä¸º 85.4%ï¼Œç¬¬äºŒä¸ªä¸º 14.5%ï¼Œåé¢è¶Šæ¥è¶Šå°‘ã€‚ Input: X = ads.drop(1558, axis=1).values y = ads[1558] # å‚æ•°ä¸ºä¸»æˆåˆ†æ•°é‡ pca = PCA(n_components=5) Xd = pca.fit_transform(X) # è®¾ç½®è¾“å‡ºé€‰é¡¹ # ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºè¾“å‡ºç²¾åº¦ä½æ•°ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä½¿ç”¨å®šç‚¹è¡¨ç¤ºæ³•æ‰“å°æµ®ç‚¹æ•° np.set_printoptions(precision=3, suppress=True) print(pca.explained_variance_ratio_) Output: [0.854 0.145 0.001 0. 0. ] ä½¿ç”¨éšæœºæ£®æ—éªŒè¯æ¨¡å‹æ­£ç¡®ç‡ï¼Œå¹¶å°† pca è½¬æ¢ç»“æœç»˜åˆ¶å‡ºæ¥ã€‚ Input: clf = DecisionTreeClassifier(random_state=14) scores_reduced = cross_val_score(clf, Xd, y, scoring='accuracy') print(np.mean(scores_reduced)) # è·å–æ•°æ®é›†ç±»åˆ«çš„æ‰€æœ‰å–å€¼ classes = set(y) # æŒ‡å®šåœ¨å›¾å½¢ä¸­ç”¨ä»€ä¹ˆé¢œè‰²è¡¨ç¤ºè¿™ä¸¤ä¸ªç±»åˆ« colors = ['red', 'green'] # åŒæ—¶éå†è¿™ä¸¤ä¸ªå®¹å™¨ for cur_class, color in zip(classes, colors): # ä¸ºå±äºå½“å‰ç±»åˆ«çš„æ‰€æœ‰ä¸ªä½“åˆ›å»ºé®ç½©å±‚ mask = (y == cur_class).values plt.scatter(Xd[mask, 0], Xd[mask, 1], marker='o', color=color, label=int(cur_class)) plt.legend() plt.show() Output: 0.936405592140775 è¾“å‡ºç»“æœpca \" è¾“å‡ºç»“æœ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"åˆ›å»ºè‡ªå·±çš„è½¬æ¢å™¨ è½¬æ¢å™¨æœ‰ä¸¤ä¸ªå…³é”®å‡½æ•° fit() æ¥æ”¶è®­ç»ƒæ•°æ®ï¼Œè®¾ç½®å†…éƒ¨å‚æ•° transform() è½¬æ¢è¿‡ç¨‹ã€‚æ¥æ”¶è®­ç»ƒæ•°æ®é›†æˆ–ç›¸åŒæ ¼å¼çš„æ–°æ•°æ®é›† æ¥å£è¦ä¸ scikit-learn æ¥å£ä¸€è‡´ï¼Œä¾¿äºåœ¨æµæ°´çº¿ä¸­ä½¿ç”¨ã€‚ Input: # -*- coding: utf-8 -*- import numpy as np from sklearn.base import TransformerMixin from sklearn.utils import as_float_array from numpy.testing import assert_array_equal class MeanDiscrete(TransformerMixin): def fit(self, X): # å°è¯•å¯¹Xè¿›è¡Œè½¬æ¢ï¼Œæ•°æ®è½¬æ¢æˆfloatç±»å‹ X = as_float_array(X) # è®¡ç®—æ•°æ®é›†çš„å‡å€¼ self.mean = X.mean(axis=0) # è¿”å›å®ƒæœ¬èº«ï¼Œè¿›è¡Œé“¾å¼è°ƒç”¨transformer.fit(X).transform(X) return self def transform(self, X): X = as_float_array(X) # æ£€æŸ¥è¾“å…¥æ˜¯å¦åˆæ³• assert X.shape[1] == self.mean.shape[0] # è¿”å›Xä¸­å¤§äºå‡å€¼çš„æ•°æ® return X \u003e self.mean def test_meandiscrete(): X_test = np.array([[0, 2], [3, 5], [6, 8], [9, 11], [12, 14], [15, 17], [18, 20], [21, 23], [24, 26], [27, 29]]) mean_discrete = MeanDiscrete() mean_discrete.fit(X_test) # ä¸æ­£ç¡®çš„è®¡ç®—ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œæ£€æŸ¥å†…éƒ¨å‚æ•°æ˜¯å¦æ­£ç¡®è®¾ç½® assert_array_equal(mean_discrete.mean, np.array([13.5, 15.5])) # è½¬æ¢åçš„X X_transfromed = mean_discrete.transform(X_test) # éªŒè¯æ•°æ® X_expected = np.array([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]) assert_array_equal(X_transfromed, X_expected) if __name__ == '__main__': test_meandiscrete() Output: # æ²¡æœ‰è¾“å‡ºï¼Œè¯´æ˜æµ‹è¯•é€šè¿‡ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:4","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬å…­ç«  æœ¬ç« ä»‹ç»å¦‚ä½•ä»æ–‡æœ¬æ•°æ®ä¸­æå–ç‰¹å¾ã€‚é€šè¿‡å¼ºå¤§å´ç®€å•çš„æœ´ç´ è´å¶æ–¯ç®—æ³•æ¶ˆé™¤ç¤¾ä¼šåª’ä½“ç”¨è¯­çš„æ­§ä¹‰ã€‚ æœ´ç´ è´å¶æ–¯ç®—æ³•åœ¨è®¡ç®—ç”¨äºåˆ†ç±»çš„æ¦‚ç‡æ—¶ï¼Œä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œå‡å®šå„ç‰¹å¾é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå› æ­¤åå­—ä¸­å«æœ‰æœ´ç´ äºŒå­—ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æ¶ˆæ­§ ç”±äºæ— æ³•ç”³è¯·åˆ° Twitter app æš‚æ—¶æç½®ã€‚ã€‚ã€‚%\u003e_\u003c% ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æ–‡æœ¬è½¬æ¢å™¨ è¯è¢‹ï¼šä¸€ç§æœ€ç®€å•å´éå¸¸æœ‰æ•ˆçš„æ¨¡å‹å°±æ˜¯åªç»Ÿè®¡æ•°æ®é›†ä¸­æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°ã€‚æ¨¡å‹ä¸»è¦åˆ†ä¸ºä»¥ä¸‹ä¸‰ç§ ä½¿ç”¨è¯è¯­å®é™…å‡ºç°çš„æ¬¡æ•°ä½œä¸ºè¯é¢‘ã€‚ç¼ºç‚¹æ˜¯å½“æ–‡ç« é•¿åº¦æ˜æ˜¾å·®å¼‚æ—¶ï¼Œè¯é¢‘å·®è·ä¼šéå¸¸å¤§ã€‚ ä½¿ç”¨å½’ä¸€åŒ–åçš„è¯é¢‘ï¼Œæ¯ç¯‡æ–‡ç« ä¸­æ‰€æœ‰è¯è¯­çš„è¯é¢‘ä¹‹å’Œä¸º 1 ç›´æ¥ä½¿ç”¨äºŒå€¼ç‰¹å¾æ¥è¡¨ç¤ºï¼Œå•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°å€¼ä¸º 1ï¼Œä¸å‡ºç°å€¼ä¸º 0 è¿˜æœ‰ä¸€ç§æ›´é€šç”¨çš„è§„èŒƒåŒ–æ–¹æ³•å«åšè¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡æ³•ï¼Œè¯¥åŠ æƒæ–¹æ³•ç”¨è¯é¢‘æ¥ä»£æ›¿è¯çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åå†ç”¨è¯é¢‘é™¤ä»¥åŒ…å«è¯¥è¯çš„æ–‡æ¡£çš„æ•°é‡ã€‚ Input: # -*- coding: utf-8 -*- from collections import Counter if __name__ == '__main__': s = \"\"\"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in halls of stone, Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne In the Land of Mordor where the Shadows lie. One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them. In the Land of Mordor where the Shadows lie\"\"\".lower() words = s.split() c = Counter(words) # è¾“å‡ºå‡ºç°æ¬¡æ•°æœ€å¤šçš„å‰5ä¸ªè¯ print(c.most_common(5)) Output: [('the', 9), ('for', 4), ('in', 4), ('to', 4), ('one', 4)] N å…ƒè¯­æ³•æ˜¯æŒ‡ç”±å‡ ä¸ªè¿ç»­çš„è¯ç»„æˆçš„å­åºåˆ—ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æœ´ç´ è´å¶æ–¯ æˆ‘ä»¬ç”¨ C è¡¨ç¤ºæŸç§ç±»åˆ«ï¼Œç”¨ D è¡¨ç¤ºæ•°æ®é›†ä¸­ä¸€ç¯‡æ–‡æ¡£ï¼Œæ¥è®¡ç®—è´å¶æ–¯å…¬å¼æ‰€è¦ç”¨åˆ°çš„å„ç§ç»Ÿè®¡é‡ï¼Œå¯¹äºä¸å¥½è®¡ç®—ï¼Œå‡ºæœ´ç´ å‡è®¾ï¼Œç®€åŒ–è®¡ç®—ã€‚æœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•ä½¿ç”¨è´å¶æ–¯å®šç†è®¡ç®—ä¸ªä½“ä»å±äºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ã€‚ P(C) ä¸ºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ï¼Œå¯ä»¥ä»è®­ç»ƒé›†ä¸­è®¡ç®—å¾—åˆ°ï¼ˆæ–¹æ³•è·Ÿä¸Šæ–‡æ£€æµ‹åƒåœ¾é‚®ä»¶ä¾‹å­æ‰€ç”¨åˆ°çš„ä¸€è‡´ï¼‰ã€‚ç»Ÿè®¡è®­ç»ƒé›†æ‰€æœ‰æ–‡æ¡£ä»å±äºç»™å®šç±»åˆ«çš„ç™¾åˆ†æ¯”ã€‚ P(D) ä¸ºæŸä¸€æ–‡æ¡£çš„æ¦‚ç‡ï¼Œå®ƒç‰µæ‰¯åˆ°å„ç§ç‰¹å¾ï¼Œè®¡ç®—èµ·æ¥å¾ˆå›°éš¾ï¼Œä½†æ˜¯åœ¨è®¡ç®—æ–‡æ¡£å±äºå“ªä¸ªç±»åˆ«æ—¶ï¼Œå¯¹äºæ‰€æœ‰ç±»åˆ«æ¥è¯´ï¼ŒP(D)ç›¸åŒï¼Œå› æ­¤æ ¹æœ¬å°±ä¸ç”¨è®¡ç®—å®ƒã€‚ç¨åæˆ‘ä»¬æ¥çœ‹ä¸‹æ€ä¹ˆå¤„ç†ã€‚ P(D|C) ä¸ºæ–‡æ¡£ D å±äº C ç±»çš„æ¦‚ç‡ã€‚ç”±äº D åŒ…å«å¤šä¸ªç‰¹å¾ï¼Œè®¡ç®—èµ·æ¥å¯èƒ½å¾ˆå›°éš¾ï¼Œè¿™æ—¶æœ´ç´ è´å¶æ–¯ç®—æ³•å°±æ´¾ä¸Šç”¨åœºäº†ã€‚æˆ‘ä»¬æœ´ç´ åœ°å‡å®šå„ä¸ªç‰¹å¾ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œåˆ†åˆ«è®¡ç®—æ¯ä¸ªç‰¹å¾ï¼ˆD1ã€D2ã€D3 ç­‰ï¼‰åœ¨ç»™å®šç±»åˆ«å‡ºç°çš„æ¦‚ç‡ï¼Œå†æ±‚å®ƒä»¬çš„ç§¯ã€‚ P(D|C) = P(D1|C) x P(D2|C) ... x P(Dn|C) ä¸¾ä¾‹è¯´æ˜ä¸‹è®¡ç®—è¿‡ç¨‹ï¼Œå‡å¦‚æ•°æ®é›†ä¸­æœ‰ä»¥ä¸‹ä¸€æ¡ç”¨äºŒå€¼ç‰¹å¾è¡¨ç¤ºçš„æ•°æ®ï¼š[1, 0, 0, 1] è®­ç»ƒé›†ä¸­æœ‰ 75% çš„æ•°æ®å±äºç±»åˆ« 0ï¼Œ 25% å±äºç±»åˆ« 1ï¼Œä¸”æ¯ä¸ªç‰¹å¾å±äºæ¯ä¸ªç±»åˆ«çš„ä¼¼ç„¶åº¦å¦‚ä¸‹ã€‚ ç±»åˆ« 0ï¼š[0.3, 0.4, 0.4, 0.7] ç±»åˆ« 1ï¼š[0.7, 0.3, 0.4, 0.9] æ‹¿ç±»åˆ« 0 ä¸­ç‰¹å¾ 1 çš„ä¼¼ç„¶åº¦ä¸¾ä¾‹å­ï¼Œä¸Šé¢è¿™ä¸¤è¡Œæ•°æ®å¯ä»¥è¿™æ ·ç†è§£ï¼šç±»åˆ« 0 ä¸­æœ‰ 30%çš„æ•°æ®ï¼Œç‰¹å¾ 1 çš„å€¼ä¸º 1ã€‚ æˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹è¿™æ¡æ•°æ®å±äºç±»åˆ« 0 çš„æ¦‚ç‡ã€‚ç±»åˆ«ä¸º 0 æ—¶ï¼ŒP(C=0) = 0.75ã€‚ æœ´ç´ è´å¶æ–¯ç®—æ³•ç”¨ä¸åˆ° P(D)ï¼Œå› æ­¤æˆ‘ä»¬ä¸ç”¨è®¡ç®—å®ƒã€‚ P(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0) = 0.3 x 0.6 x 0.6 x 0.7 = 0.0756 æˆ‘ä»¬å°±å¯ä»¥è®¡ç®—è¯¥æ¡æ•°æ®ä»å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚æˆ‘ä»¬æ²¡æœ‰è®¡ç®— P(D)ï¼Œå› æ­¤ï¼Œè®¡ç®—ç»“æœä¸æ˜¯å®é™…çš„æ¦‚ç‡ã€‚ç”±äºä¸¤æ¬¡éƒ½ä¸è®¡ç®— P(D)ï¼Œç»“æœå…·æœ‰å¯æ¯”è¾ƒæ€§ï¼Œèƒ½å¤ŸåŒºåˆ†å‡ºå¤§å°å°±è¶³å¤Ÿäº†ã€‚æ¥çœ‹ä¸‹è®¡ç®—ç»“æœã€‚ P(C=0|D) = P(C=0) P(D|C=0) = 0.75 * 0.0756 = 0.0567 P(D|C=1) = P(D1|C=1) x P(D2|C=1) x P(D3|C=1) x P(D4|C=1) = 0.7 x 0.7 x 0.6 x 0.9 = 0.2646 P(C=1|D) = P(C=1)P(D|C=1) = 0.25 * 0.2646 = 0.06615 å› æ­¤è¿™æ¡æ•°æ®å±äºç±»åˆ« 1 çš„æ¦‚ç‡å¤§äºå±äºç±»åˆ« 2 çš„æ¦‚ç‡ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"åº”ç”¨ åˆ›å»ºæµæ°´çº¿ï¼Œæ¥æ”¶ä¸€æ¡æ¶ˆæ¯ï¼Œä»…æ ¹æ®æ¶ˆæ¯å†…å®¹ï¼Œç¡®å®šå®ƒä¸ç¼–ç¨‹è¯­è¨€ Python æ˜¯å¦ç›¸å…³ã€‚ ç”¨ NLTK çš„ word_tokenize å‡½æ•°ï¼Œå°†åŸå§‹æ–‡æ¡£è½¬æ¢ä¸ºç”±å•è¯åŠå…¶æ˜¯å¦å‡ºç°ç»„æˆçš„å­—å…¸ã€‚ ç”¨ scikit-learn ä¸­çš„ DictVectorizer è½¬æ¢å™¨å°†å­—å…¸è½¬æ¢ä¸ºå‘é‡çŸ©é˜µï¼Œè¿™æ ·æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨å°±èƒ½ä½¿ç”¨ç¬¬ä¸€æ­¥ä¸­æŠ½å–çš„ç‰¹å¾ã€‚ æ­£å¦‚å‰å‡ ç« åšè¿‡çš„é‚£æ ·ï¼Œè®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ã€‚ è¿˜éœ€è¦æ–°å»ºä¸€ä¸ªç¬”è®°æœ¬æ–‡ä»¶ ch6_classify_twitterï¼ˆæœ¬ç« æœ€åä¸€ä¸ªï¼‰ï¼Œç”¨äºåˆ†ç±»ã€‚ F1 å€¼æ¥è¯„ä¼°ç®—æ³• F1 å€¼æ˜¯ä»¥æ¯ä¸ªç±»åˆ«ä¸ºåŸºç¡€è¿›è¡Œå®šä¹‰çš„ï¼ŒåŒ…æ‹¬ä¸¤å¤§æ¦‚å¿µï¼šå‡†ç¡®ç‡ï¼ˆprecisionï¼‰å’Œå¬å›ç‡ï¼ˆrecallï¼‰ã€‚å‡†ç¡®ç‡æ˜¯æŒ‡é¢„æµ‹ç»“æœå±äºæŸä¸€ç±»çš„ä¸ªä½“ï¼Œå®é™…å±äºè¯¥ç±»çš„æ¯”ä¾‹ã€‚å¬å›ç‡æ˜¯æŒ‡è¢«æ­£ç¡®é¢„æµ‹ä¸ºæŸä¸ªç±»åˆ«çš„ä¸ªä½“æ•°é‡ä¸æ•°æ®é›†ä¸­è¯¥ç±»åˆ«ä¸ªä½“æ€»é‡çš„æ¯”ä¾‹ Input: # -*- coding: utf-8 -*- import json import numpy as np from sklearn.base import TransformerMixin from nltk import word_tokenize from sklearn.feature_extraction import DictVectorizer # æ¥å—å…ƒç´ ä¸ºå­—å…¸çš„åˆ—è¡¨ï¼Œå°†å…¶è½¬æ¢ä¸ºçŸ©é˜µ from sklearn.model_selection import cross_val_score from sklearn.naive_bayes import BernoulliNB # ç”¨äºäºŒå€¼ç‰¹å¾åˆ†ç±»çš„ BernoulliNB åˆ†ç±»å™¨ï¼Œ from sklearn.pipeline import Pipeline # åˆ›å»ºè½¬æ¢å™¨ç±» class NLTKBOW(TransformerMixin): def fit(self, X, y=None): return self def transform(self, X): return [{word: True for word in word_tokenize(document)} for document in X] if __name__ == '__main__': tweets = [] input_filename = \"\" classes_filename = \"\" with open(input_filename) as inf: for line in inf: if len(line.strip()) == 0: continue tweets.append(json.loads(line)['text']) with open(classes_filename, 'r') as inf: labels = json.load(inf) # ç»„è£…æµæ°´çº¿ pipline = Pipeline([('bag-of-words', NLTKBOW()), ('vectorizer', DictVectorizer()), ('naive-bayes', BernoulliNB())]) # ç”¨F1å€¼æ¥è¯„ä¼° scores = cross_val_score(pipline, tweets, labels, scoring='f1') print(\"Score: {:.3f}\".format(np.mean(scores))) model = pipline.fit(tweets, labels) nb = model.named_steps['naive-bayes'] feature_probabilities = nb.feature_log_prob_ top_features = np.argsort(-feature_probabilities[1])[:50] dv = model.named_steps['vectorizer'] for i, feature_index in enumerate(top_features): print(i, dv.feature_names_[feature_index], np.exp(feature_probabilities[1][feature_index])) Output: æš‚æ—¶æ²¡æœ‰æ•°æ®é›† ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:4","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬ä¸ƒç«  æœ¬ç« ä»‹ç»çš„ç®—æ³•å¼•å…¥èšç±»åˆ†ææ¦‚å¿µâ€“æ ¹æ®ç›¸ä¼¼åº¦ï¼ŒæŠŠå¤§æ•°æ®é›†åˆ’åˆ†ä¸ºå‡ ä¸ªå­é›†ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:7:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"åŠ è½½æ•°æ®é›† ç”±äºç”³è¯·ä¸åˆ° Twitter å¼€å‘è€…è´¦å·ï¼Œæˆ‘æƒ³åŠæ³•çˆ¬äº†ä¸€äº› b ç«™ç”¨æˆ·å…³æ³¨æ•°æ®ï¼Œåšæˆäº†æœ¬æ¬¡è¯•éªŒç›¸ä»¿çš„å½¢å¼ Input: # -*- coding: utf-8 -*- import json import pandas as pd import networkx as nx from matplotlib import pyplot as plt import numpy as np from scipy.optimize import minimize from sklearn.metrics import silhouette_score if __name__ == '__main__': with open('bili.txt', mode='r') as fin: temp = json.load(fin) users = pd.DataFrame(temp) users.columns = ['Id', 'Friends'] print(users[:5]) Output: Id Friends 0 214582845 [4370617, 259345180, 186334806, 546195, 477132... 1 4370617 [74507, 883968, 122879, 585267] 2 259345180 [] 3 186334806 [] 4 546195 [] å°†æ¯ä¸ªè®°å½•çš„ç”¨æˆ·å·¦å³ main_usersï¼ŒæŠŠä»–ä»¬å…³æ³¨çš„äººä½œä¸ºè¾¹ï¼Œç”Ÿæˆæœ‰å‘å›¾ ç”±äºå¯¹ matplotlib åº“å’Œ networkx åº“äº†è§£å¤ªå°‘ï¼Œåœ¨ä½œå›¾æ—¶é‡åˆ°äº†è®¸å¤šå›°éš¾ï¼ˆæ ¹åŸºä¸ç‰¢ï¼Œåœ°åŠ¨å±±æ‘‡ã€‚(\u003e_\u003c)ï¼‰ Input: G = nx.DiGraph() main_users = list(users['Id'].values) for u in main_users: G.add_node(u, label=u) for user in users.values: friends = user[1] for friend in friends: if friend in main_users: G.add_edge(user[0], int(friend)) print('graph finished') plt.figure(3, figsize=(100, 100)) nx.draw(G, alpha=0.1, edge_color='b', with_labels=True, font_size=16, node_size=30, node_color='r') plt.savefig('fix1.png') Output: åˆ›å»ºç”¨æˆ·ç›¸ä¼¼åº¦å›¾ ç”±äºæ¯ä¸ªç”¨æˆ·å…³æ³¨çš„äººæ•°å¯èƒ½ç›¸å·®å¾ˆå¤§ï¼Œå› æ­¤ä½¿ç”¨æ°å¡å¾·ç›¸ä¼¼ç³»æ•°ï¼ˆä¸¤ä¸ªç”¨æˆ·å…³æ³¨çš„é›†åˆçš„äº¤é›†é™¤ä»¥å¹¶é›†ï¼‰ï¼Œè¯¥ç³»æ•°åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œä»£è¡¨ä¸¤è€…é‡åˆçš„æ¯”ä¾‹ã€‚ è§„èŒƒåŒ–æ˜¯æ•°æ®æŒ–æ˜çš„ä¸€ä¸ªé‡è¦æ–¹æ³•ï¼Œè¦åšæŒä½¿ç”¨ï¼ˆé™¤éæœ‰å……è¶³çš„ç†ç”±ä¸è¿™æ ·åšï¼‰ è®¿é—®http://networkx.lanl.gov/reference/drawing/htmläº†è§£ networkx çš„å¸ƒå±€æ–¹æ³• Input: friends = {user: set(friends) for user, friends in users.values} def compute_similarity(friends1, friends2): return len(friends1 \u0026 friends2) / len(friends1 | friends2) def create_graph(followers, threshold=0.0): G = nx.Graph() for user1 in friends.keys(): if len(friends[user1]) == 0: continue for user2 in friends.keys(): if len(friends[user2]) == 0: continue if user1 == user2: continue weight = compute_similarity(friends[user1], friends[user2]) if weight \u003e= threshold: G.add_node(user1, lable=user1) G.add_node(user2, lable=user2) G.add_edge(user1, user2, weight=weight) return G G = create_graph(friends) plt.figure(3, figsize=(100, 100)) pos = nx.spring_layout(G) nx.draw_networkx_nodes(G, pos, node_size=30) edgewidth = [d['weight'] for (u, v, d) in G.edges(data=True)] nx.draw_networkx_edges(G, pos, width=edgewidth) plt.savefig('fix2.png') Output: ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:7:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"å¯»æ‰¾å­å›¾ networkx çš„ connected_component_subgraphs() å‡½æ•°åœ¨ 2.1 ç‰ˆæœ¬ä¸­è¢«ç§»é™¤äº†ï¼ˆä»£ç è¿‡æ—¶çš„æ¯”è¾ƒå¤šï¼Œå¹¶ä¸”ä½¿ç”¨ Twitter ä½œä¸ºæ¼”ç¤ºæ•°æ®é›†è®©æˆ‘è¿™ä¸¤ç« åšçš„å¾ˆå¤´ç–¼ï¼‰ï¼Œæˆ‘æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£åå‘ç°å¯ä»¥ä½¿ç”¨ connected_components() æ›¿ä»£ï¼Œä½†æ˜¯æ­¤å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œä¸€æ¬¡ç”Ÿæˆä¸€ç»„è¿é€šé¡¶ç‚¹ï¼Œå¯ä»¥é…åˆ G.subgraph(nodes) ä½¿ç”¨è·å¾—è¿é€šåˆ†æ”¯ Input: # ç”Ÿæˆæ–°å›¾ï¼ŒæŒ‡å®šæœ€ä½é˜ˆå€¼ä¸º0.1 G = create_graph(friends, 0.1) sub_graphs = nx.connected_components(G) for i, sub_graphs in enumerate(sub_graphs): n_nodes = len(sub_graphs) print(\"Subgraph{}has {}nodes\".format(i, n_nodes)) print('---------------------') G = create_graph(friends, 0.15) sub_graphs = nx.connected_components(G) for i, sub_graphs in enumerate(sub_graphs): n_nodes = len(sub_graphs) print(\"Subgraph{}has {}nodes\".format(i, n_nodes)) sub_graphs = [c for c in sorted(nx.connected_components(G), key=len, reverse=True)] n_subgraphs = nx.number_connected_components(G) fig = plt.figure(figsize=(20, (n_subgraphs*3))) for i, sub_graph in enumerate(sub_graphs): # sub_graphæ˜¯ä¸€ä¸ªè¿é€šåˆ†æ”¯é¡¶ç‚¹çš„é›†åˆ ax = fig.add_subplot(int(n_subgraphs / 3) + 1, 3, i + 1) # å°†åæ ‡è½´æ ‡ç­¾å…³æ‰ ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) pos = nx.spring_layout(G) nx.draw(G=G.subgraph(sub_graph), alpha=0.1, edge_color='b', with_labels=True, font_size=16, node_size=30, node_color='r', ax=ax) plt.show() Output: è½®å»“ç³»æ•°å®šä¹‰ï¼š s = (b - a) / max(a, b) å…¶ä¸­ a ä¸ºç°‡å†…è·ç¦»ï¼Œè¡¨ç¤ºä¸ç°‡å†…å…¶å®ƒä¸ªä½“ä¹‹é—´çš„å¹³å‡è·ç¦»ã€‚b ä¸ºç°‡é—´è·ç¦»ï¼Œä¹Ÿå°±æ˜¯æœ€è¿‘ç°‡å†…å„ä¸ªä¸ªä½“ä¹‹é—´çš„å¹³å‡è·ç¦» Input: def compute_silhouette(threshold, friends): G = create_graph(friends, threshold=threshold)\\ # å›¾æ˜¯å¦è‡³å°‘æœ‰ä¸¤ä¸ªé¡¶ç‚¹ if len(G.nodes()) \u003c 2: # è¿”å›-99è¡¨ç¤ºé—®é¢˜æ— æ•ˆ return -99 # æŠ½å–è¿é€šåˆ†æ”¯ sub_graphs = nx.connected_components(G) # è‡³å°‘æœ‰ä¸¤ä¸ªè¿é€šåˆ†æ”¯ if not (2 \u003c= nx.number_connected_components(G) \u003c len(G.nodes()) - 1): return -99 label_dict = {} for i, sub_graph in enumerate(sub_graphs): for node in sub_graph: # ç»™ä¸åŒè¿é€šåˆ†æ”¯çš„é¡¶ç‚¹åˆ†é…ä¸åŒçš„æ ‡ç­¾ label_dict[node] = i labels = np.array([label_dict[node] for node in G.nodes()]) X = nx.to_scipy_sparse_matrix(G).todense() # è¿™é‡Œè¦å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºè·ç¦»ï¼Œæ‰€ä»¥ç”¨æœ€å¤§ç›¸ä¼¼åº¦å‡å»ç°æœ‰ç›¸ä¼¼åº¦ï¼ŒæŠŠç›¸ä¼¼åº¦è½¬åŒ–ä¸ºè·ç¦» X = 1 - X # è¿™é‡Œå°†è·ç¦»çŸ©é˜µçš„å¯¹è§’çº¿å¤„ç†ä¸º0ï¼Œå› ä¸ºè‡ªå·±åˆ°è‡ªå·±çš„è·ç¦»ä¸º0 np.fill_diagonal(X, 0) return silhouette_score(X, labels, metric='precomputed') def inverted_silhouette(threshold, friends): # å¯¹è½®å»“ç³»æ•°å–åï¼Œå°†æ‰“åˆ†å‡½æ•°è½¬åŒ–æˆæŸå¤±å‡½æ•° res = compute_silhouette(threshold, friends=friends) return - res # minimizeå‡½æ•°æ˜¯ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå€¼è¶Šå°è¶Šå¥½ # å‚æ•°ï¼šinverted_silhouetteè¦å¯»æ‰¾çš„å‡½æ•°ï¼›0.1å¼€å§‹æ—¶çŒœæµ‹çš„é˜ˆå€¼ï¼›options={'maxiter': 10} åªè¿›è¡Œ10è½®è¿­ä»£ï¼Œå¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œæ•ˆæœå¯èƒ½æ›´å¥½ï¼Œä½†è¿è¡Œæ—¶é—´ä¼šå¢åŠ ï¼Œmethod='nelder-mead'ä½¿ç”¨\"ä¸‹å±±å•çº¯å½¢æ³•\"ä¼˜åŒ–æ–¹æ³• result = minimize(inverted_silhouette, 0.1, args=(friends,), options={'maxiter': 10}) print(result.x) Output: [0.10005086] æœ¬ç« æ¢è®¨äº†ç¤¾äº¤ç½‘ç»œå’Œå›¾ä»¥åŠå¦‚ä½•å¯¹å…¶è¿›è¡Œèšç±»åˆ†æã€‚ç›®æ ‡æ˜¯æ¨èç”¨æˆ·ï¼Œä½¿ç”¨èšç±»åˆ†ææ–¹æ³•èƒ½å¤Ÿæ‰¾åˆ°ä¸åŒçš„ç”¨æˆ·ç°‡ï¼Œä¸»è¦æ­¥éª¤æœ‰æ ¹æ®ç›¸ä¼¼åº¦åˆ›å»ºåŠ æƒå›¾ï¼Œä»å›¾ä¸­å¯»æ‰¾è¿é€šåˆ†æ”¯ã€‚åˆ›å»ºå›¾æ—¶ç”¨åˆ°äº† NetworkX åº“ã€‚ è¿˜æ¯”è¾ƒäº†å‡ å¯¹æ„ä¹‰ç›¸åçš„æ¦‚å¿µã€‚å¯¹äºä¸¤è€…ä¹‹é—´çš„ç›¸ä¼¼åº¦è¿™ä¸ªæ¦‚å¿µï¼Œå€¼è¶Šå¤§ï¼Œè¡¨æ˜ä¸¤è€…ä¹‹é—´æ›´ç›¸åƒã€‚ç›¸åï¼Œå¯¹äºè·ç¦»è€Œè¨€ï¼Œå€¼è¶Šå°ï¼Œä¸¤è€…æ›´ç›¸åƒã€‚å¦å¤–ä¸€å¯¹æ˜¯æŸå¤±å‡½æ•°å’Œæ‰“åˆ†å‡½æ•°ã€‚å¯¹äºæŸå¤±å‡½æ•°ï¼Œå€¼è¶Šå°ï¼Œæ•ˆæœè¶Šå¥½ï¼ˆä¹Ÿå°±æ˜¯æŸå¤±è¶Šå°‘ï¼‰ã€‚è€Œå¯¹äºæ‰“åˆ†å‡½æ•°ï¼Œå€¼è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:7:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬å…«ç«  æœ¬ç« ä½¿ç”¨ç¥ç»ç½‘ç»œåˆ†æè‡ªå·±ç”Ÿæˆçš„éªŒè¯ç å›¾åƒ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"äººå·¥ç¥ç»ç½‘ç»œ ç¥ç»ç½‘ç»œç®—æ³•æœ€åˆæ˜¯æ ¹æ®äººç±»å¤§è„‘çš„å·¥ä½œæœºåˆ¶è®¾è®¡çš„ã€‚ç¥ç»ç½‘ç»œç”±ä¸€ç³»åˆ—ç›¸äº’è¿æ¥çš„ç¥ç»å…ƒç»„æˆã€‚æ¯ä¸ªç¥ç»å…ƒéƒ½æ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œæ¥æ”¶ä¸€å®šè¾“å…¥ï¼Œç»™å‡ºç›¸åº”è¾“å‡ºã€‚ ç¥ç»å…ƒå¯ä»¥ä½¿ç”¨ä»»ä½•æ ‡å‡†å‡½æ•°æ¥å¤„ç†æ•°æ®ï¼Œæ¯”å¦‚çº¿æ€§å‡½æ•°ï¼Œè¿™äº›å‡½æ•°ç»Ÿç§°ä¸ºæ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œç¥ç»ç½‘ç»œå­¦ä¹ ç®—æ³•è¦èƒ½æ­£å¸¸å·¥ä½œï¼Œæ¿€æ´»å‡½æ•°åº”å½“æ˜¯å¯å¯¼ï¼ˆderivableï¼‰å’Œå…‰æ»‘çš„ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æœ‰é€»è¾‘æ–¯è°›å‡½æ•°ï¼Œå‡½æ•°è¡¨è¾¾å¼å¦‚ä¸‹ï¼ˆx ä¸ºç¥ç»å…ƒçš„è¾“å…¥ï¼Œkã€L é€šå¸¸ä¸º 1ï¼Œè¿™æ—¶å‡½æ•°è¾¾åˆ°æœ€å¤§å€¼ï¼‰ã€‚ $$ f(x) = \\frac{L}{1+e^{-k(x-x_{0})}} $$ æ¯ä¸ªç¥ç»å…ƒæ¥æ”¶å‡ ä¸ªè¾“å…¥ï¼Œæ ¹æ®è¿™å‡ ä¸ªè¾“å…¥ï¼Œè®¡ç®—è¾“å‡ºã€‚è¿™æ ·çš„ä¸€ä¸ªä¸ªç¥ç»å…ƒè¿æ¥åœ¨ä¸€èµ·ç»„æˆäº†ç¥ç»ç½‘ç»œï¼Œå¯¹æ•°æ®æŒ–æ˜åº”ç”¨æ¥è¯´ï¼Œå®ƒéå¸¸å¼ºå¤§ã€‚è¿™äº›ç¥ç»å…ƒç´§å¯†è¿æ¥ï¼Œå¯†åˆ‡é…åˆï¼Œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ å¾—åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œæˆä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸæœ€å¼ºå¤§çš„æ¦‚å¿µä¹‹ä¸€ã€‚ æ•°æ®æŒ–æ˜åº”ç”¨çš„ç¥ç»ç½‘ç»œï¼Œç¥ç»å…ƒæŒ‰ç…§å±‚çº§è¿›è¡Œæ’åˆ—ï¼Œè‡³å°‘æœ‰ä¸‰å±‚ ç¬¬ä¸€å±‚ï¼šè¾“å…¥å±‚ã€‚ç”¨æ¥æ¥æ”¶æ•°æ®é›†çš„è¾“å…¥ã€‚ç¬¬ä¸€å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒå¯¹è¾“å…¥è¿›è¡Œè®¡ç®—ï¼ŒæŠŠå¾—åˆ°çš„ç»“æœä¼ ç»™ç¬¬äºŒå±‚çš„ç¥ç»å…ƒã€‚è¿™ç§å«ä½œå‰å‘ç¥ç»ç½‘ç»œ éšå«å±‚ï¼šæ•°æ®è¡¨ç°æ–¹å¼ä»¤äººéš¾ä»¥ç†è§£ï¼Œä¸€å±‚æˆ–å¤šå±‚ æœ€åä¸€å±‚ï¼šè¾“å‡ºå±‚ã€‚è¾“å‡ºç»“æœè¡¨ç¤ºçš„æ˜¯ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ç»™å‡ºçš„åˆ†ç±»ç»“æœ ç¥ç»å…ƒæ¿€æ´»å‡½æ•°é€šå¸¸ä½¿ç”¨é€»è¾‘æ–¯è°›å‡½æ•°ï¼Œæ¯å±‚ç¥ç»å…ƒä¹‹é—´ä¸ºå…¨è¿æ¥ï¼Œåˆ›å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œè¿˜éœ€è¦ç”¨åˆ°å…¶ä»–å‡ ä¸ªå‚æ•°ã€‚ åˆ›å»ºè¿‡ç¨‹ï¼ŒæŒ‡å®šç¥ç»ç½‘ç»œçš„è§„æ¨¡éœ€è¦ç”¨åˆ°ä¸¤ä¸ªå‚æ•°ï¼šç¥ç»ç½‘ç»œå…±æœ‰å¤šå°‘å±‚ï¼Œéšå«å±‚æ¯å±‚æœ‰å¤šå°‘ä¸ªç¥ç»å…ƒï¼ˆè¾“å…¥å±‚å’Œè¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡é€šå¸¸ç”±æ•°æ®é›†æ¥å®šï¼‰ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"åˆ›å»ºæ•°æ®é›† ä½¿ç”¨é•¿åº¦ä¸º 4 ä¸ªå­—æ¯çš„è‹±æ–‡å•è¯ä½œä¸ºéªŒè¯ç  Input: # -*- coding: utf-8 -*- import numpy as np from PIL import Image, ImageDraw, ImageFont from skimage import transform as tf from skimage.transform import resize from matplotlib import pyplot as plt from skimage.measure import label, regionprops # ç”¨äºå›¾åƒåˆ†å‰² from sklearn.utils import check_random_state from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from pybrain.datasets.supervised import SupervisedDataSet # ç¥ç»ç½‘ç»œæ•°æ®é›† from pybrain.tools.shortcuts import buildNetwork # æ„å»ºç¥ç»ç½‘ç»œ from pybrain.supervised.trainers.backprop import BackpropTrainer # åå‘ä¼ æ’­ç®—æ³• from sklearn.metrics import f1_score from nltk.corpus import words # å¯¼å…¥è¯­æ–™åº“ ç”¨äºç”Ÿæˆå•è¯ from sklearn.metrics import confusion_matrix # æ··æ·†çŸ©é˜µ from nltk.metrics import edit_distance # ç¼–è¾‘è·ç¦» from operator import itemgetter # ç”¨äºç”ŸæˆéªŒè¯ç ï¼Œæ¥æ”¶ä¸€ä¸ªå•è¯å’Œé”™åˆ‡å€¼ï¼Œè¿”å›ç”¨numpyæ•°ç»„æ ¼å¼è¡¨ç¤ºçš„å›¾åƒ def create_captcha(text, shear=0.0, size=(100, 26)): im = Image.new(\"L\", size, \"black\") draw = ImageDraw.Draw(im) # éªŒè¯ç æ–‡å­—æ‰€ç”¨å­—ä½“ï¼Œè¯¥å¼€æºå­—ä½“å¯åœ¨githubä¸‹è½½ font = ImageFont.truetype(\"FiraCode-Medium.otf\", 22) draw.text((0, 0), text, fill=1, font=font) # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œä»¥ä¾¿ç”¨scikit-imageåº“ä¸ºå›¾åƒæ·»åŠ é”™åˆ‡å˜åŒ–æ•ˆæœ image = np.array(im) # åº”ç”¨é”™åˆ‡å˜åŒ–æ•ˆæœ affine_tf = tf.AffineTransform(shear=shear) image = tf.warp(image, affine_tf) # å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œç¡®ä¿ç‰¹å¾å€¼è½åœ¨0åˆ°1ä¹‹é—´ return image / image.max() if __name__ == '__main__': image = create_captcha('GENE', shear=0.5) plt.imshow(image, cmap='Greys') plt.show() Output: å°†å›¾åƒåˆ‡åˆ†ä¸ºå•ä¸ªçš„å­—æ¯ Input: def segment_image(image): \"\"\" æ¥æ”¶å›¾åƒï¼Œè¿”å›å°å›¾åƒåˆ—è¡¨ :param image: :return: \"\"\" # æ‰¾å‡ºåƒç´ å€¼ç›¸åŒåˆè¿æ¥åœ¨ä¸€èµ·çš„åƒç´ å—ï¼Œç±»ä¼¼ä¸Šä¸€ç« çš„è¿é€šåˆ†æ”¯ labeled_image = label(image \u003e 0) subimages = [] for region in regionprops(labeled_image): # è·å–å½“å‰ä½ç½®çš„èµ·å§‹å’Œç»“æŸåæ ‡ start_x, start_y, end_x, end_y = region.bbox subimages.append(image[start_x:end_x, start_y:end_y]) # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å°å›¾åƒï¼Œåˆ™å°†åŸå›¾åƒä½œä¸ºå­å›¾è¿”å› if len(subimages) == 0: return [image, ] return subimages subimages = segment_image(image) f, axes = plt.subplots(1, len(subimages), figsize=(10, 3)) for i in range(len(subimages)): axes[i].imshow(subimages[i], cmap='gray') plt.show() Output: åˆ›å»ºè®­ç»ƒé›† Input: # æŒ‡å®šéšæœºçŠ¶æ€å€¼ random_state = check_random_state(14) letters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\") shear_values = np.arange(0, 0.5, 0.05) # ç”¨æ¥ç”Ÿæˆä¸€æ¡è®­ç»ƒæ•°æ® def generate_sample(random_state=None): random_state = check_random_state(random_state) letter = random_state.choice(letters) shear = random_state.choice(shear_values) return create_captcha(letter, shear=shear, size=(25, 25)), letters.index(letter) image, target = generate_sample(random_state) plt.imshow(image, cmap='Greys') print(\"The target for this image is {}\".format(target)) plt.show() # è°ƒç”¨3000æ¬¡æ­¤å‡½æ•°ï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®ä¼ åˆ°numpyçš„æ•°ç»„é‡Œ dataset, targets = zip(*(generate_sample(random_state) for i in range(3000))) dataset = np.array(dataset, dtype=float) targets = np.array(targets) # å¯¹26ä¸ªå­—æ¯ç±»åˆ«è¿›è¡Œç¼–ç  onehot = OneHotEncoder() y = onehot.fit_transform(targets.reshape(targets.shape[0], 1)) # å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ y = y.todense() # è°ƒæ•´å›¾åƒå¤§å° dataset = np.array([resize(segment_image(sample)[0], (20, 20)) for sample in dataset]) # å°†æœ€åä¸‰ç»´çš„datasetçš„åäºŒç»´æ‰å¹³åŒ– X = dataset.reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2])) X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9) Output: The target for this image is 11 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"è®­ç»ƒå’Œåˆ†ç±» åå‘ä¼ æ’­ç®—æ³•ï¼ˆback propagationï¼Œbackpropï¼‰çš„å·¥ä½œæœºåˆ¶ä¸ºå¯¹é¢„æµ‹é”™è¯¯çš„ç¥ç»å…ƒæ–½ä»¥æƒ©ç½šã€‚ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œå‘ä¸Šå±‚å±‚æŸ¥æ‰¾é¢„æµ‹é”™è¯¯çš„ç¥ç»å…ƒï¼Œå¾®è°ƒè¿™äº›ç¥ç»å…ƒè¾“å…¥å€¼çš„æƒé‡ï¼Œä»¥è¾¾åˆ°ä¿®å¤è¾“å‡ºé”™è¯¯çš„ç›®çš„ã€‚ ç¥ç»å…ƒä¹‹æ‰€ä»¥ç»™å‡ºé”™è¯¯çš„é¢„æµ‹ï¼ŒåŸå› åœ¨äºå®ƒå‰é¢ä¸ºå…¶æä¾›è¾“å…¥çš„ç¥ç»å…ƒï¼Œæ›´ç¡®åˆ‡æ¥è¯´æ˜¯ç”±è¿™ä¸¤ä¸ªç¥ç»å…ƒä¹‹é—´è¾¹çš„æƒé‡åŠè¾“å…¥å€¼å†³å®šçš„ã€‚æˆ‘ä»¬å¯ä»¥å°è¯•å¯¹æƒé‡è¿›è¡Œå¾®è°ƒã€‚æ¯æ¬¡è°ƒæ•´çš„å¹…åº¦å–å†³äºä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢ ç¥ç»å…ƒå„è¾¹æƒé‡çš„è¯¯å·®å‡½æ•°çš„åå¯¼æ•° ä¸€ä¸ªå«ä½œå­¦ä¹ é€Ÿç‡çš„å‚æ•°ï¼ˆé€šå¸¸ä½¿ç”¨å¾ˆå°çš„å€¼ï¼‰ è®¡ç®—å‡ºå‡½æ•°è¯¯å·®çš„æ¢¯åº¦ï¼Œå†ä¹˜ä»¥å­¦ä¹ é€Ÿç‡ï¼Œç”¨æ€»æƒé‡å‡å»å¾—åˆ°çš„å€¼ã€‚æ¢¯åº¦çš„ç¬¦å·ç”±è¯¯å·®å†³å®šï¼Œæ¯æ¬¡å¯¹æƒé‡çš„ä¿®æ­£éƒ½æ˜¯æœç€ç»™å‡ºæ­£ç¡®çš„é¢„æµ‹å€¼åŠªåŠ›ã€‚æœ‰æ—¶å€™ï¼Œä¿®æ­£ç»“æœä¸ºå±€éƒ¨æœ€ä¼˜ï¼ˆlocal optimaï¼‰ï¼Œæ¯”èµ·å…¶ä»–æƒé‡ç»„åˆè¦å¥½ï¼Œä½†æ‰€å¾—åˆ°çš„å„æƒé‡è¿˜ä¸æ˜¯æœ€ä¼˜ç»„åˆã€‚ åå‘ä¼ æ’­ç®—æ³•ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œå±‚å±‚å‘ä¸Šå›æº¯åˆ°è¾“å…¥å±‚ã€‚åˆ°è¾¾è¾“å…¥å±‚åï¼Œæ‰€æœ‰è¾¹çš„æƒé‡æ›´æ–°å®Œæ¯•ã€‚ è¿™é‡Œåœ¨å¯¼å…¥ SupervisedDataSet æ—¶å‘ç”Ÿäº†é”™è¯¯ï¼Œä½¿ç”¨ pip install pybrain å®‰è£…çš„åŒ…ä¼šæœ‰æ‰¾ä¸åˆ°æ–¹æ³•çš„ç°è±¡ï¼Œå› æ­¤æˆ‘ä» github-pybrain ä¸‹è½½äº†æºç åŒ…ï¼Œåœ¨è§£å‹åçš„æ–‡ä»¶å¤¹ä¸­è¾“å…¥ python setup.py install è¿›è¡Œå®‰è£…ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚è¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯åŸæ–‡ä½¿ç”¨ from pybrain.datasets import SupervisedDataSet æ¥å¯¼å…¥ SupervisedDataSet ä½†æ˜¯æˆ‘åœ¨å¯¼å…¥æ—¶å‘ç°å¹¶æ²¡æœ‰è¿™ä¸ªç±»ï¼Œäºæ˜¯çœ‹äº†é¡¹ç›®ç»“æ„åä½¿ç”¨ from pybrain.datasets.supervised import SupervisedDataSet è¿›è¡Œå¯¼å…¥ã€‚è¿˜æœ‰å‡ å¤„ç›¸åŒçš„é—®é¢˜å‡æ˜¯è¿™æ ·è§£å†³çš„ã€‚ è¿™é‡Œåœ¨ä½¿ç”¨ f1_score è¿›è¡Œè¯„ä¼°æ—¶ä¹Ÿå‡ºç°äº†é”™è¯¯ï¼ŒåŸå› è§ä»£ç æ³¨é‡Šã€‚ Input: # ä¸ºpybrainåº“åˆ›å»ºæ ¼å¼é€‚é…çš„æ•°æ®é›† training = SupervisedDataSet(X.shape[1], y.shape[1]) for i in range(X_train.shape[0]): training.addSample(X_train[i], y_train[i]) testing = SupervisedDataSet(X.shape[1], y.shape[1]) for i in range(X_test.shape[0]): testing.addSample(X_test[i], y_test[i]) # æŒ‡å®šç»´åº¦ï¼Œåˆ›å»ºç¥ç»ç½‘ç»œï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºè¾“å…¥å±‚ç¥ç»å…ƒæ•°é‡ï¼Œç¬¬äºŒä¸ªå‚æ•°éšå«å±‚ç¥ç»å…ƒæ•°é‡ï¼Œç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºè¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡ # biasåœ¨æ¯ä¸€å±‚ä½¿ç”¨ä¸€ä¸ªä¸€ç›´å¤„äºæ¿€æ´»çŠ¶æ€çš„åç½®ç¥ç»å…ƒ net = buildNetwork(X.shape[1], 100, y.shape[1], bias=True) # ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•è°ƒæ•´æƒé‡ trainer = BackpropTrainer(net, training, learningrate=0.01, weightdecay=0.01) # è®¾å®šä»£ç çš„è¿è¡Œæ­¥æ•° trainer.trainEpochs(epochs=20) # é¢„æµ‹å€¼ predictions = trainer.testOnClassData(dataset=testing) # f1_scoreçš„averageé»˜è®¤å€¼ä¸º'binary'ï¼Œå¦‚æœä¸æŒ‡å®šaverageåˆ™ä¼šå‘ç”ŸValueError print(\"F-score:{0:.2f}\".format(f1_score(y_test.argmax(axis=1), predictions, average='weighted'))) print(\"F-score:{0:.2f}\".format(f1_score(y_test.argmax(axis=1), predictions, average='micro'))) print(\"F-score:{0:.2f}\".format(f1_score(y_test.argmax(axis=1), predictions, average='macro'))) Output: F-score:1.00 F-score:1.00 F-score:1.00 é¢„æµ‹å•è¯ Input: # æ¥æ”¶éªŒè¯ç ï¼Œç”¨ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œè¿”å›å•è¯é¢„æµ‹ç»“æœ def predict_captcha(captcha_image, neural_network): subimages = segment_image(captcha_image) predicted_word = \"\" # éå†å››å¼ å°å›¾åƒ for subimage in subimages: # è°ƒæ•´æ¯å¼ å°å›¾åƒçš„å¤§å°ä¸º20*20åƒç´  subimage = resize(subimage, (20,20)) # æŠŠå°å›¾åƒæ•°æ®ä¼ å…¥ç¥ç»ç½‘ç»œçš„è¾“å…¥å±‚ï¼Œæ¿€æ´»ç¥ç»ç½‘ç»œã€‚è¿™äº›æ•°æ®å°†åœ¨ç¥ç»ç½‘ç»œä¸­è¿›è¡Œä¼ æ’­ï¼Œè¿”å›è¾“å‡ºç»“æœ outputs = net.activate(subimage.flatten()) # ç¥ç»ç½‘ç»œè¾“å‡º26ä¸ªå€¼ï¼Œæ¯ä¸ªå€¼éƒ½æœ‰ç´¢å¼•å·ï¼Œåˆ†åˆ«å¯¹åº”lettersåˆ—è¡¨ä¸­æœ‰ç€ç›¸åŒç´¢å¼•çš„å­—æ¯ï¼Œæ¯ä¸ªå€¼çš„å¤§å°è¡¨ç¤ºä¸å¯¹åº”å­—æ¯çš„ç›¸ä¼¼åº¦ã€‚ä¸ºäº†è·å¾—å®é™…çš„é¢„æµ‹å€¼ï¼Œæˆ‘ä»¬å–åˆ°æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå†é€šè¿‡lettersåˆ—è¡¨æ‰¾åˆ°å¯¹åº”çš„å­—æ¯ prediction = np.argmax(outputs) # æŠŠä¸Šé¢å¾—åˆ°çš„å­—æ¯æ·»åŠ åˆ°æ­£åœ¨é¢„æµ‹çš„å•è¯ä¸­ predicted_word += letters[prediction] return predicted_word word = \"GENE\" captcha = create_captcha(word, shear=0.2) print(predict_captcha(captcha, net)) Output: GENE nltk ä¸‹è½½è¯­æ–™åº“æ—¶å¯èƒ½ä¼šå¾ˆæ…¢ï¼Œéœ€è¦çš„å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½ã€‚å¦‚ä½•ç¦»çº¿å®‰è£… nltk è¯­æ–™åº“è‡ªè¡Œç™¾åº¦ã€‚ Input: def test_prediction(word, net, shear=0.2): captcha = create_captcha(word, shear=shear) prediction = predict_captcha(captcha, net) prediction = prediction[:4] # è¿”å›é¢„æµ‹ç»“æœæ˜¯å¦æ­£ç¡®ï¼ŒéªŒè¯ç ä¸­çš„å•è¯å’Œé¢„æµ‹ç»“æœçš„å‰å››ä¸ªå­—ç¬¦ return word == prediction, word, prediction # è¯­æ–™åº“ä¸­å­—é•¿ä¸º4çš„å•è¯åˆ—è¡¨ valid_words = [word.upper() for word in words.words() if len(word) == 4] num_correct = 0 num_incorrect = 0 for word in valid_words: correct, word, prediction = test_prediction(word, net, shear=0.2) if correct: num_correct += 1 else: num_incorrect += 1 print(\"Number correct is {}\".format(num_correct)) print(\"Number incorrect is {}\".format(num_incorrect)) # äºŒç»´æ··æ·†çŸ©é˜µï¼Œ æ¯è¡Œæ¯åˆ—å‡ä¸ºä¸€ä¸ªç±»åˆ« cm = confusion_matrix(np.argmax(y_test,axis=1), predictions) # æ··æ·†çŸ©é˜µä½œå›¾ plt.figure(figsize=(20, 20)) plt.imshow(cm) tick_marks = np.arange(len(letters)) plt.xticks(tick_marks, letters) plt.yticks(tick_marks, letters) plt.ylabel('Actual') plt.xlabel('Predicted') plt.show() Output: Number correct is 3738 Number incorrect is 1775 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç”¨è¯å…¸æå‡å‡†ç¡®ç‡ å‡è®¾éªŒè¯ç å…¨éƒ¨éƒ½æ˜¯è‹±è¯­å•è¯ åˆ—æ–‡æ–¯å¦ç¼–è¾‘è·ç¦»ï¼ˆLevenshtein edit distanceï¼‰æ˜¯ä¸€ç§é€šè¿‡æ¯”è¾ƒä¸¤ä¸ªçŸ­å­—ç¬¦ä¸²ï¼Œç¡®å®šå®ƒä»¬ç›¸ä¼¼åº¦çš„æ–¹æ³•ã€‚å®ƒä¸å¤ªé€‚åˆæ‰©å±•ï¼Œå­—ç¬¦ä¸²å¾ˆé•¿æ—¶é€šå¸¸ä¸ç”¨è¿™ç§æ–¹æ³•ã€‚ç¼–è¾‘è·ç¦»éœ€è¦è®¡ç®—ä»ä¸€ä¸ªå•è¯å˜ä¸ºå¦ä¸€ä¸ªå•è¯æ‰€éœ€è¦çš„æ­¥éª¤æ•°ã€‚ä»¥ä¸‹æ“ä½œéƒ½ç®—ä¸€æ­¥ åœ¨å•è¯çš„ä»»æ„ä½ç½®æ’å…¥ä¸€ä¸ªæ–°å­—æ¯ ä»å•è¯ä¸­åˆ é™¤ä»»æ„ä¸€ä¸ªå­—æ¯ æŠŠä¸€ä¸ªå­—æ¯æ›¿æ¢ä¸ºå¦å¤–ä¸€ä¸ªå­—æ¯ Input: # è·å¾—ä¸¤ä¸ªå•è¯çš„ç¼–è¾‘è·ç¦» steps = edit_distance(\"STEP\", \"STOP\") print(\"The num of steps needed is: {}\".format(steps)) # ç”¨è¯é•¿4å‡å»åŒç­‰ä½ç½®ä¸Šç›¸åŒçš„å­—æ¯æ•°é‡ï¼Œå¾—åˆ°çš„å€¼è¶Šå°è¡¨ç¤ºä¸¤ä¸ªè¯ç›¸ä¼¼åº¦è¶Šé«˜ def compute_distance(prediction, word): return len(prediction) - sum(prediction[i] == word[i] for i in range(len(prediction))) # æ”¹è¿›é¢„æµ‹å‡½æ•° def improved_prediction(word, net, dictionary, shear=0.2): captcha = create_captcha(word, shear=shear) prediction = predict_captcha(captcha, net) prediction = prediction[:4] # å¦‚æœå•è¯ä¸åœ¨è¯å…¸ä¸­åˆ™æ¯”è¾ƒå–è¯å…¸ä¸­è·ç¦»æœ€å°çš„å•è¯ if prediction not in dictionary: distance = sorted([(w, compute_distance(prediction, w)) for w in dictionary], key=itemgetter(1)) best_word = distance[0] prediction = best_word[0] return word == prediction, word, prediction num_correct = 0 num_incorrect = 0 for word in valid_words: correct, word, prediction = improved_prediction(word, net, valid_words,shear=0.2) if correct: num_correct += 1 else: num_incorrect += 1 print(\"Number correct is {}\".format(num_correct)) print(\"Number incorrect is {}\".format(num_incorrect)) Output: The num of steps needed is: 1 Number correct is 3785 Number incorrect is 1728 æ­£ç¡®ç‡ç¨æœ‰æå‡ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:4","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬ä¹ç«  æ˜¨å¤©è·‘å»æ wordpress æ­å»ºç½‘ç«™äº† (à¹‘â€¢Ì â‚ƒâ€¢Ì€à¹‘) ï¼ˆæ‘¸é±¼çœŸèˆ’æœ æœ¬ç« ä¸»è¦ä»‹ç»å¦‚ä¸‹å†…å®¹ ç‰¹å¾å·¥ç¨‹å’Œå¦‚ä½•æ ¹æ®åº”ç”¨é€‰æ‹©ç‰¹å¾ å¸¦ç€æ–°é—®é¢˜ï¼Œé‡æ–°å›é¡¾è¯è¢‹æ¨¡å‹ ç‰¹å¾ç±»å‹å’Œå­—ç¬¦ N å…ƒè¯­æ³•æ¨¡å‹ æ”¯æŒå‘é‡æœº æ•°æ®é›†æ¸…æ´— ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:9:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ä¸ºä½œå“æ‰¾åˆ°ä½œè€… ä½œè€…å½’å±å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§åˆ†ç±»é—®é¢˜ï¼Œå·²çŸ¥ä¸€éƒ¨åˆ†ä½œè€…ï¼Œæ•°æ®é›†ä¸ºå¤šä¸ªä½œè€…çš„ä½œå“ï¼ˆè®­ç»ƒé›†ï¼‰ï¼Œç›®æ ‡æ˜¯ç¡®å®šä¸€ç»„ä½œè€…ä¸è¯¦çš„ä½œå“ï¼ˆæµ‹è¯•é›†ï¼‰æ˜¯è°å†™çš„ã€‚å¦‚æœä½œè€…æ°å¥½æ˜¯å·²çŸ¥çš„ä½œè€…é‡Œé¢çš„ï¼Œè¿™ç§é—®é¢˜å«ä½œå°é—­é—®é¢˜ å¦‚æœä½œè€…å¯èƒ½ä¸åœ¨é‡Œé¢ï¼Œè¿™ç§é—®é¢˜å°±å«ä½œå¼€æ”¾é—®é¢˜ è·å–æ•°æ®ï¼Œä¹¦ä¸­çš„é“¾æ¥æœ‰å¾ˆå¤šå·²ç»å¤±æ•ˆï¼Œæˆ‘å‚è€ƒç½‘ä¸Šçš„å–å¾—äº†ä¸‹è½½æ–¹å¼ã€‚ Input: # -*- coding: utf-8 -*- # get_data.py import requests import os import time from collections import defaultdict titles = {'burton': [4657, 2400, 5760, 6036, 7111, 8821, 18506, 4658, 5761, 6886, 7113], 'dickens': [24022, 1392, 1414, 1467, 2324, 580, 786, 888, 963, 27924, 1394, 1415, 15618, 25985, 588, 807, 914, 967, 30127, 1400, 1421, 16023, 28198, 644, 809, 917, 968, 1023, 1406, 1422, 17879, 30368, 675, 810, 924, 98, 1289, 1413, 1423, 17880, 32241, 699, 821, 927], 'doyle': [2349, 11656, 1644, 22357, 2347, 290, 34627, 5148, 8394, 26153, 12555, 1661, 23059, 2348, 294, 355, 5260, 8727, 10446, 126, 17398, 2343, 2350, 3070, 356, 5317, 903, 10581, 13152, 2038, 2344, 244, 32536, 423, 537, 108, 139, 2097, 2345, 24951, 32777, 4295, 7964, 11413, 1638, 21768, 2346, 2845, 3289, 439, 834], 'gaboriau': [1748, 1651, 2736, 3336, 4604, 4002, 2451, 305, 3802, 547], 'nesbit': [34219, 23661, 28804, 4378, 778, 20404, 28725, 33028, 4513, 794], 'tarkington': [1098, 15855, 1983, 297, 402, 5798, 8740, 980, 1158, 1611, 2326, 30092, 483, 5949, 8867, 13275, 18259, 2595, 3428, 5756, 6401, 9659], 'twain': [1044, 1213, 245, 30092, 3176, 3179, 3183, 3189, 74, 86, 1086, 142, 2572, 3173, 3177, 3180, 3186, 3192, 76, 91, 119, 1837, 2895, 3174, 3178, 3181, 3187, 3432, 8525]} assert len(titles) == 7 assert len(titles['tarkington']) == 22 assert len(titles['dickens']) == 44 assert len(titles['nesbit']) == 10 assert len(titles['doyle']) == 51 assert len(titles['twain']) == 29 assert len(titles['burton']) == 11 assert len(titles['gaboriau']) == 10 url_base = 'http://www.gutenberg.org/files/' url_format = '{url_base}{id}/{id}-0.txt' # ä¿®å¤URL url_fix_format = 'http://www.gutenberg.org/cache/epub/{id}/pg{id}.txt' fiexes = defaultdict(list) # fixes = {} # fixes[4657] = 'http://www.gutenberg.org/cache/epub/4657/pg4657.txt' # make parent folder if not exists # data_folder = os.path.join(os.path.expanduser('~'),'Data','books') # # è¿™æ˜¯åœ¨ç”¨æˆ·userç›®å½•ä¸­å­˜å‚¨ data_folder = os.path.dirname(os.path.abspath(__file__)) if __name__ == '__main__': if not os.path.exists(data_folder): os.makedirs(data_folder) print(data_folder) for author in titles: print('Downloading titles from', author) # make author's folder if not exists author_folder = os.path.join(data_folder, author) if not os.path.exists(author_folder): os.makedirs(author_folder) # download each title to this folder for bookid in titles[author]: # if bookid in fixes: # print(' - Applying fix to book with id', bookid) # url = fixes[bookid] # else: # print(' - Getting book with id', bookid) # url = url_format.format(url_base=url_base, id=bookid) url = url_format.format(url_base=url_base, id=bookid) print(' - ', url) filename = os.path.join(author_folder, '%s.txt' % bookid) if os.path.exists(filename): print(' - File already exists, skipping') else: r = requests.get(url) if r.status_code == 404: print('url 404:', author, bookid, 'add to fixes list') fiexes[author].append(bookid) else: txt = r.text with open(filename, 'w', encoding='utf-8') as f: f.write(txt) time.sleep(1) print('Download complete') print('å¼€å§‹ä¸‹è½½ä¿®å¤åˆ—è¡¨') for author in fiexes: print('å¼€å§‹ä¸‹è½½\u003c%s\u003eçš„ä½œå“' % author) author_folder = os.path.join(data_folder, author) if not os.path.exists(author_folder): os.makedirs(author_folder) for bookid in fiexes[author]: filename = os.path.join(author_folder, '%s.txt' % bookid) if os.path.exists(filename): print('æ–‡ä»¶å·²ç»ä¸‹è½½ï¼Œè·³è¿‡') else: url_fix = url_fix_format.format(id=bookid) print(' - ', url_fix) r = requests.get(url_fix) if r.status_code == 404: print('åˆå‡ºé”™äº†ï¼', author, bookid) else: with open(filename, 'w', encoding='utf-8') as f: f.write(r.text) time.sleep(1) print('ä¿®å¤åˆ—è¡¨ä¸‹è½½å®Œæ¯•') æœ€åä¸‹è½½å®Œæˆæœ‰ 177 æœ¬ä¹¦ æ”¯æŒå‘é‡æœºæ˜¯ä¸€ç§äºŒç±»åˆ†ç±»å™¨ï¼Œæ‰©å±•åå¯ç”¨æ¥å¯¹å¤šä¸ªç±»åˆ«è¿›è¡Œåˆ†ç±» 9ï¼ˆå¯¹äºå¤šç§ç±»åˆ«çš„åˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºå¤šä¸ª SVM åˆ†ç±»å™¨â€”â€”æ¯ä¸ªè¿˜æ˜¯äºŒç±»åˆ†ç±»å™¨ï¼‰ C å‚æ•°å¯¹äºè®­ç»ƒ SVM æ¥è¯´å¾ˆé‡è¦ï¼ŒC å‚æ•°ä¸åˆ†ç±»å™¨æ­£ç¡®åˆ†ç±»æ¯”ä¾‹ç›¸å…³ï¼Œä½†å¯èƒ½å¸¦æ¥è¿‡æ‹Ÿåˆçš„é£é™©ã€‚C å€¼è¶Šé«˜ï¼Œé—´éš”è¶Šå°ï¼Œè¡¨ç¤ºè¦å°½å¯èƒ½æŠŠæ‰€æœ‰æ•°æ®æ­£ç¡®åˆ†ç±»ã€‚C å€¼è¶Šå°ï¼Œé—´éš”è¶Šå¤§â€”â€”æœ‰äº›æ•°æ®å°†æ— æ³•æ­£ç¡®åˆ†ç±»ã€‚C å€¼ä½ï¼Œè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„å¯èƒ½æ€§å°±ä½ï¼Œä½†æ˜¯åˆ†ç±»æ•ˆæœå¯èƒ½ä¼šç›¸å¯¹è¾ƒå·® SV","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:9:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"N å…ƒè¯­æ³• N å…ƒè¯­æ³•ç”±ä¸€ç³»åˆ—çš„ N ä¸ªä¸ºä¸€ç»„çš„å¯¹è±¡ç»„æˆï¼ŒN ä¸ºæ¯ç»„å¯¹è±¡çš„ä¸ªæ•° Input: # ç”¨Nå…ƒè¯­æ³•åˆ†ç±» pipeline = Pipeline([('feature_extraction', CountVectorizer(analyzer='char', ngram_range=(3, 3))), # é•¿åº¦ä¸º3çš„Nå…ƒè¯­æ³• ('classifier', grid) ]) scores = cross_val_score(pipeline, documents, classes, scoring='f1_macro') print(\"Score: {:.3f}\".format(np.mean(scores))) Output: Score: 0.813 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:9:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"å®‰ç„¶é‚®ä»¶æ•°æ®é›† è¯»å–æ•°æ®é›† æ¸…æ´—æ•°æ® ç»„è£…æµæ°´çº¿ ä½¿ç”¨ F å€¼è¯„ä¼° # -*- coding: utf-8 -*- import os from email.parser import Parser # é‚®ä»¶è§£æå™¨ from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.utils import check_random_state # éšæœºçŠ¶æ€å®ä¾‹ from sklearn.metrics import confusion_matrix from matplotlib import pyplot as plt import numpy as np import quotequail enron_data_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"maildir\") if __name__ == '__main__': p = Parser() def get_enron_corpus(num_authors=10, data_folder=enron_data_folder, min_docs_author=10, max_docs_author=100, random_state=None): random_state = check_random_state(random_state) # éšæœºå¯¹å¾—åˆ°çš„é‚®ç®±åˆ—è¡¨è¿›è¡Œæ’åº # os.listdirå‡½æ•°æ¯æ¬¡è¿”å›ç»“æœä¸ä¸€å®šç›¸åŒï¼Œåœ¨ä½¿ç”¨è¯¥å‡½æ•°å‰å…ˆæ’åºï¼Œä»è€Œä¿æŒè¿”å›ç»“æœçš„ä¸€è‡´æ€§ email_addresses = sorted(os.listdir(data_folder)) random_state.shuffle(email_addresses) documents = [] classes = [] author_num = 0 authors = {} # éå†é‚®ç®±æ–‡ä»¶å¤¹ï¼ŒæŸ¥æ‰¾å®ƒä¸‹é¢åå­—ä¸­å«æœ‰â€œsentâ€çš„è¡¨ç¤ºå‘ä»¶ç®±çš„å­æ–‡ä»¶å¤¹ for user in email_addresses: users_email_folder = os.path.join(data_folder, user) mail_folders = [os.path.join(users_email_folder, subfolder) for subfolder in os.listdir(users_email_folder) if \"sent\" in subfolder] try: # è·å–å­æ–‡ä»¶å¤¹ä¸­çš„æ¯ä¸€å°é‚®ä»¶ï¼Œè·³è¿‡å…¶ä¸­çš„å­æ–‡ä»¶å¤¹ authored_emails = [open(os.path.join(mail_folder, email_filename), encoding='cp1252').read() for mail_folder in mail_folders for email_filename in os.listdir(mail_folder)] except IsADirectoryError: continue # è·å¾—è‡³å°‘åå°é‚®ä»¶ if len(authored_emails) \u003c min_docs_author: continue # æœ€å¤šè·å–å‰100å°é‚®ä»¶ if len(authored_emails) \u003e max_docs_author: authored_emails = authored_emails[:max_docs_author] # è§£æé‚®ä»¶ï¼Œè·å–é‚®ä»¶å†…å®¹ contents = [p.parsestr(email)._payload for email in authored_emails] documents.extend(contents) # å°†å‘ä»¶äººæ·»åŠ åˆ°ç±»åˆ—è¡¨ä¸­ï¼Œæ¯å°é‚®ä»¶æ·»åŠ ä¸€æ¬¡ classes.extend([author_num] * len(authored_emails)) # è®°å½•æ”¶ä»¶äººç¼–å·ï¼Œå†æŠŠç¼–å·+1 authors[user] = author_num author_num += 1 # æ”¶ä»¶äººæ•°é‡è¾¾åˆ°è®¾ç½®çš„å€¼è·³å‡ºå¾ªç¯ if author_num \u003e= num_authors or author_num \u003e= len(email_addresses): break # è¿”å›é‚®ä»¶æ•°æ®é›†ä»¥åŠæ”¶ä»¶äººå­—å…¸ return documents, np.array(classes), authors documents, classes, authors = get_enron_corpus(data_folder=enron_data_folder, random_state=14) # ç§»é™¤é‚®ä»¶çš„å›å¤ä¿¡æ¯ def remove_replies(email_contents): r = quotequail.unwrap(email_contents) if r is None: return email_contents if 'text_top' in r: return r['text_top'] # å­—å…¸rä¸­å­˜åœ¨text_topï¼Œè¿”å›å®ƒçš„å€¼ elif 'text' in r: return r['text'] return email_contents documents = [remove_replies(document) for document in documents] parameters = {'kernel': ('linear', 'rbf'), 'C': [1, 10]} svr = SVC() grid = GridSearchCV(svr, parameters) pipeline = Pipeline([('feature_extraction', CountVectorizer(analyzer='char', ngram_range=(3, 3))), ('classifier', grid) ]) scores = cross_val_score(pipeline, documents, classes, scoring='f1_macro') print(\"Score: {:.3f}\".format(np.mean(scores))) Output: Score: 0.664 ä»æµæ°´çº¿ä¸­è·å¾—æœ€å¥½çš„å‚æ•°ç»„åˆ Input: training_documents, test_documents, y_train, y_test = train_test_split(documents, classes, random_state=14) pipeline.fit(training_documents, y_train) y_pred = pipeline.predict(test_documents) print(pipeline.named_steps['classifier'].best_params_) Output: {'C': 10, 'kernel': 'rbf'} ç»˜åˆ¶æ··æ·†çŸ©é˜µæŸ¥çœ‹åˆ†ç±»æƒ…å†µ Input: cm = confusion_matrix(y_test, y_pred) cm = cm / cm.astype(np.float).sum(axis=1) sorted_authors = sorted(authors.keys(), key=lambda x: authors[x]) plt.figure(figsize=(20, 20)) plt.imshow(cm, cmap='Blues') tick_marks = np.arange(len(sorted_authors)) plt.xticks(tick_marks, sorted_authors) plt.yticks(tick_marks, sorted_authors) plt.ylabel('Actual') plt.xlabel('Predicted') plt.show() Output: ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:9:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬åç«  è¿™ä¸¤å¤©åœ¨é¼“æ£ jupyterlabï¼Œä¸€å¼€å§‹åœ¨æœåŠ¡å™¨ä¸Šå»ºäº†ä¸€ä¸ª lab ç¯å¢ƒï¼Œå¯æ¯æ¬¡è¿æ¥éƒ½è¦ç™»ä¸Šå‡ åˆ†é’Ÿï¼Œä¸çŸ¥é“æ˜¯æœåŠ¡å™¨ CPU ä¸è¡Œè¿˜æ˜¯ç½‘ç»œä¸è¡Œã€‚ç„¶ååˆåœ¨æœ¬åœ°é¼“æ£ï¼Œåœ¨ debian è£… nodejs å’Œ npm çš„æ—¶å€™æŠŠç³»ç»Ÿä¾èµ–æå´©äº†ï¼Œäºæ˜¯ç‹ ä¸‹å¿ƒæ¥é‡è£…äº†ç”µè„‘ã€‚ã€‚ã€‚å‘ç”Ÿçš„äº‹æƒ…å¤ªå¤šï¼Œå¿ƒç´¯ã€‚ã€‚ æ˜¨å¤©é‡è£…äº† Ubuntuï¼Œæäº†ä¸‹ç¾åŒ–ï¼Œå®‰è£…äº†å¿…é¡»çš„è½¯ä»¶ï¼ˆåˆ«è¯´ Ubuntu è¿˜æŒºå¥½ç”¨ï¼ŒçœŸé¦™ï¼‰ æˆ‘ä¿è¯è¿™æ˜¯æœ€åä¸€å¥åæ§½äº†ï¼Œä¸€å®š æœ¬ç« ä»‹ç»å¦‚ä½•å¯¹æ–°é—»è¯­æ–™è¿›è¡Œèšç±»ï¼Œä»¥å‘ç°å…¶ä¸­çš„è¶‹åŠ¿å’Œä¸»é¢˜ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"è·å–æ–°é—»æ–‡ç«  è¿™ä¸€ç« çš„æ•°æ®é›†æ˜¯ä» reddit è·å¾—çš„ç½‘é¡µé“¾æ¥ï¼Œreddit çš„ app å®¡æ ¸æœºåˆ¶ä¸æ˜¯å¾ˆä¸¥æ ¼(?)å› æ­¤æˆ‘ç»ˆäºæ‹¿åˆ°äº†å¢™å¤–çš„ apiï¼Œä½¿ç”¨ requests ä¸‹è½½åˆè´¹äº†ä¸€ç•ªåŠŸå¤«ï¼Œä½¿ç”¨ä¹¦ä¸Šæºç çš„ url ä¸‹è½½æ€»æ˜¯ 403 é”™è¯¯ï¼Œç ”ç©¶äº†å¥½åŠå¤© reddit çš„ apiï¼Œå‘ç° reddit çš„ url æ”¹æˆäº†(new, top, â€¦)ï¼Œä¿®æ”¹ä¹‹åæ€»ç®—å®Œæˆäº†é“¾æ¥çš„ç´¢å¼• Input: # get_links.py # -*- coding: utf-8 -*- import json import os import requests import getpass import time # éœ€è¦çš„ä¸€äº›å‡­è¯ CLIENT_ID = \"xxxxxxxxxxx\" CLIENT_SECRET = \"xxxxxxxxxxx\" USER_AGENT = \"python:xxxxxxxxx (by /u/xxxxxxxxx)\" USERNAME = \"xxxxxxxx\" PASSWORD = \"xxxxxxxxxxxxxx\" # requestsä½¿ç”¨ä»£ç† proxies = {\"http\": \"socks5://xxxxxx\", \"https\": \"socks5://xxxxxx\"} def login(username, password): if password is None: password = getpass.getpass( \"Enter reddit password for user {}: \".format(username) ) headers = {\"User-Agent\": USER_AGENT} # ä½¿ç”¨å‡­æ®è®¾ç½®èº«ä»½éªŒè¯å¯¹è±¡ client_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET) post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password} response = requests.post( \"https://www.reddit.com/api/v1/access_token\", proxies=proxies, auth=client_auth, data=post_data, headers=headers, ) return response.json() if __name__ == \"__main__\": # è°ƒç”¨loginè·å–token # token = login(USERNAME, PASSWORD) # print(token) token = { \"access_token\": \"xxxxxxxxxxxxxxxxxxxxxxxx\", \"token_type\": \"xxxxx\", \"expires_in\": 3600, \"scope\": \"*\", } def get_links(subreddit, token, n_pages=5): # å­˜æ”¾é“¾æ¥ä¿¡æ¯ stories = [] after = None for page_number in range(n_pages): # è¿›è¡Œè°ƒç”¨ä¹‹å‰ç­‰å¾…ï¼Œä»¥é¿å…è¶…è¿‡APIé™åˆ¶ print(\"ç­‰å¾…2s...\") time.sleep(2) # è®¾ç½®æ ‡å¤´è¿›è¡Œè°ƒç”¨ headers = { \"Authorization\": \"bearer {}\".format(token[\"access_token\"]), \"User-Agent\": USER_AGENT, } # topä¸ºæœ€çƒ­é“¾æ¥ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥æ¢æˆnew url = \"https://oauth.reddit.com/r/{}/top?limit=100\".format(subreddit) if after: url += \"\u0026after={}\".format(after) while True: try: response = requests.get( url, proxies=proxies, headers=headers, timeout=10 ) result = response.json() # è·å–ä¸‹ä¸€ä¸ªå¾ªç¯çš„cursor after = result[\"data\"][\"after\"] except: print(\"requestså‡ºé”™ç­‰å¾…...\") time.sleep(2) else: break # å°†æ‰€æœ‰æ–°é—»é¡¹æ·»åŠ åˆ°storyåˆ—è¡¨ä¸­ for story in result[\"data\"][\"children\"]: stories.append( ( story[\"data\"][\"title\"], story[\"data\"][\"url\"], story[\"data\"][\"score\"], ) ) return stories stories = get_links(\"worldnews\", token) base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \"raw\") # è¿™é‡Œæˆ‘å°†æ‰€æœ‰çš„é“¾æ¥éƒ½å­˜åœ¨äº†æ–‡ä»¶é‡Œï¼Œå› ä¸ºè·å–è¿™äº›ç½‘ç«™çš„å†…å®¹è¦å¾ˆä¹… with open(os.path.join(base_folder, \"stories2.txt\"), \"w\") as f: for link in stories: f.write(json.dumps(list(link))) f.write(\"\\n\") ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ä»ç½‘ç«™æŠ½å–æ–‡æœ¬ api/top æ€»å…±æœ‰ 500 ä¸ªç½‘ç«™ï¼Œæˆ‘åˆè·å–äº† api/new çš„ 490 ä¸ªï¼Œæ€»å…±ä¸‹è½½äº†åŠä¸ªå°æ—¶ï¼Œå¤±è´¥äº† 300ã€‚ã€‚ã€‚ æœ€åæˆåŠŸä¸‹è½½çš„ç½‘ç«™æ•°ä¸º 365 # get_data.py # -*- coding: utf-8 -*- import hashlib import os import requests import json proxies = {\"http\": \"socks5://xxxxxxxxxxxx\", \"https\": \"socks5://xxxxxxxxxxxxx\"} if __name__ == \"__main__\": base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \"raw\") # è¯»å–é“¾æ¥æ•°æ® with open(os.path.join(base_folder, \"stories1.txt\"), \"r\") as f: temp = f.readlines() stories = [] for l in temp: stories.append(json.loads(l)) # è·å–ç½‘é¡µå†…å®¹ number_errors = 0 for title, url, score in stories: print(url) output_filename = hashlib.md5(url.encode()).hexdigest() fullpath = os.path.join(data_folder, output_filename + \".txt\") try: response = requests.get(url, proxies=proxies, timeout=10) data = response.text with open(fullpath, \"w\") as outf: outf.write(data) except Exception as e: number_errors += 1 # è¾“å‡ºå‡ºé”™æ•°é‡ print(\"å‡ºé”™ï¼š{}\".format(number_errors)) ä¸‹è½½ä¸‹æ¥çš„ç½‘é¡µå…¨æ˜¯ html æ–‡ä»¶ï¼Œè¦ä»ä¸­æå–å‡ºæœ‰ç”¨çš„ä¿¡æ¯ï¼Œè¿™é‡Œä½¿ç”¨è¾ƒä¸ºé€šç”¨çš„ lxml åº“ï¼Œå…¶å®ƒå¤„ç† html çš„åº“è¿˜æœ‰ BeautifulSoup ç­‰ã€‚ # get_content.py # -*- coding: utf-8 -*- import os from lxml import html, etree if __name__ == \"__main__\": base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \"raw\") # è¾“å‡ºå˜æˆçº¯æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„ text_output_folder = os.path.join(base_folder, \"textonly\") filenames = [ os.path.join(data_folder, filename) for filename in os.listdir(data_folder) ] # å­˜æ”¾ä¸å¯èƒ½åŒ…å«æ–°é—»å†…å®¹çš„èŠ‚ç‚¹ skip_node_types = [\"script\", \"head\", \"style\", etree.Comment] # æŠŠhtmlæ–‡ä»¶è§£ææˆlxmlå¯¹è±¡ def get_text_from_file(filename): with open(filename, \"r\") as inf: html_tree = html.parse(inf) return get_text_from_node(html_tree.getroot()) # æŠ½å–å­èŠ‚ç‚¹ä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œæœ€åè¿”å›æ‹¼æ¥åœ¨ä¸€èµ·çš„æ‰€æœ‰å­èŠ‚ç‚¹çš„æ–‡æœ¬ def get_text_from_node(node): if len(node) == 0: # æ²¡æœ‰å­èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›å†…å®¹ if node.text: return node.text else: return \"\" else: # æœ‰å­èŠ‚ç‚¹ï¼Œé€’å½’è°ƒç”¨å¾—åˆ°å†…å®¹ results = ( get_text_from_node(child) for child in node if child.tag not in skip_node_types ) result = str.join(\"\\n\", (r for r in results if len(r) \u003e 1)) # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ if len(result) \u003e= 100: return result else: return \"\" for filename in os.listdir(data_folder): text = get_text_from_file(os.path.join(data_folder, filename)) with open(os.path.join(text_output_folder, filename), \"w\") as outf: outf.write(text) ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æ–°é—»è¯­æ–™èšç±» k-means ç®—æ³• k-means èšç±»ç®—æ³•è¿­ä»£å¯»æ‰¾æœ€èƒ½å¤Ÿä»£è¡¨æ•°æ®çš„èšç±»è´¨å¿ƒç‚¹ã€‚ç®—æ³•å¼€å§‹æ—¶ä½¿ç”¨ä»è®­ç»ƒæ•°æ®ä¸­éšæœºé€‰å–çš„å‡ ä¸ªæ•°æ®ç‚¹ä½œä¸ºè´¨å¿ƒç‚¹ã€‚k-means ä¸­çš„ k è¡¨ç¤ºå¯»æ‰¾å¤šå°‘ä¸ªè´¨å¿ƒç‚¹ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ç®—æ³•å°†ä¼šæ‰¾åˆ°çš„ç°‡çš„æ•°é‡ã€‚æ­¥éª¤ï¼š ä¸ºæ¯ä¸€ä¸ªæ•°æ®ç‚¹åˆ†é…ç°‡æ ‡ç­¾ ä¸ºæ¯ä¸ªä¸ªä½“è®¾ç½®ä¸€ä¸ªæ ‡ç­¾ï¼Œå°†å®ƒå’Œæœ€è¿‘çš„è´¨å¿ƒç‚¹è”ç³»èµ·æ¥ï¼Œæ ‡ç­¾ç›¸åŒçš„ä¸ªä½“å±äºåŒä¸€ä¸ªç°‡ æ›´æ–°å„ç°‡çš„è´¨å¿ƒç‚¹ æ¯æ¬¡æ›´æ–°è´¨å¿ƒç‚¹æ—¶ï¼Œæ‰€æœ‰è´¨å¿ƒç‚¹å°†ä¼šå°èŒƒå›´ç§»åŠ¨ï¼Œè¿™ä¼šè½»å¾®æ”¹å˜æ¯ä¸ªæ•°æ®ç‚¹åœ¨ç°‡å†…çš„ä½ç½®ï¼Œä»è€Œå¼•å‘ä¸‹ä¸€æ¬¡è¿­ä»£æ—¶è´¨å¿ƒç‚¹çš„å˜åŠ¨ # -*- coding: utf-8 -*- import os from sklearn.cluster import KMeans # TfidfVectorizerå‘é‡åŒ–å·¥å…·ï¼Œæ ¹æ®è¯è¯­å‡ºç°åœ¨å¤šå°‘ç¯‡æ–‡ç« ä¸­ï¼Œå¯¹è¯è¯­è®¡æ•°è¿›è¡ŒåŠ æƒ # å‡ºç°åœ¨è¾ƒå¤šæ–‡æ¡£ä¸­çš„è¯è¯­æƒé‡è¾ƒä½ï¼ˆç”¨æ–‡æ¡£é›†æ•°é‡é™¤ä»¥è¯è¯­å‡ºç°åœ¨çš„æ–‡æ¡£çš„æ•°é‡ï¼Œç„¶åå–å¯¹æ•°ï¼‰ from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.pipeline import Pipeline from collections import Counter from scipy.sparse import csr_matrix # ç¨€ç–çŸ©é˜µ import numpy as np from scipy.sparse.csgraph import minimum_spanning_tree # è®¡ç®—æœ€å°ç”Ÿæˆæ ‘MST from scipy.sparse.csgraph import connected_components # è¿é€šåˆ†æ”¯ from sklearn.base import BaseEstimator, ClusterMixin from sklearn.cluster import MiniBatchKMeans from sklearn.feature_extraction.text import HashingVectorizer base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \"raw\") text_output_folder = os.path.join(base_folder, \"textonly\") if __name__ == \"__main__\": # åˆ†ç°‡çš„æ•°é‡ n_clusters = 10 pipeline = Pipeline( [ (\"feature_extraction\", TfidfVectorizer(max_df=0.4)), # ç‰¹å¾æŠ½å–ï¼Œå¿½ç•¥å‡ºç°åœ¨40%æ–‡æ¡£ä¸­çš„è¯è¯­ï¼ˆåˆ é™¤åŠŸèƒ½è¯ï¼‰ (\"clusterer\", KMeans(n_clusters=n_clusters)), # è°ƒç”¨k-meansç®—æ³• ] ) documents = [ open(os.path.join(text_output_folder, filename)).read() for filename in os.listdir(text_output_folder) ] # ä¸ä¸ºfitå‡½æ•°æŒ‡å®šç›®æ ‡ç±»åˆ«ï¼Œè¿›è¡Œè®­ç»ƒ pipeline.fit(documents) # ä½¿ç”¨è®­ç»ƒè¿‡çš„ç®—æ³•é¢„æµ‹ # labelsåŒ…å«æ¯ä¸ªæ•°æ®ç‚¹çš„ç°‡æ ‡ç­¾ï¼Œæ ‡ç­¾ç›¸åŒçš„æ•°æ®ç‚¹å±äºåŒä¸€ä¸ªç°‡ï¼Œæ ‡ç­¾æœ¬èº«æ²¡æœ‰å«ä¹‰ labels = pipeline.predict(documents) # ä½¿ç”¨Counterç±»æŸ¥çœ‹æ¯ä¸ªç°‡çš„æ•°æ®ç‚¹æ•°é‡ c = Counter(labels) for cluster_number in range(n_clusters): print( \"Cluster {}contains {}samples\".format(cluster_number, c[cluster_number]) ) Output: Cluster 0 contains 2 samples Cluster 1 contains 4 samples Cluster 2 contains 1 samples Cluster 3 contains 2 samples Cluster 4 contains 329 samples Cluster 5 contains 7 samples Cluster 6 contains 2 samples Cluster 7 contains 13 samples Cluster 8 contains 3 samples Cluster 9 contains 2 samples èšç±»åˆ†æä¸»è¦æ˜¯æ¢ç´¢æ€§åˆ†æï¼Œå› æ­¤å¾ˆéš¾æœ‰æ•ˆåœ°è¯„ä¼°ç»“æœçš„å¥½åï¼Œå¦‚æœæœ‰æµ‹è¯•é›†ï¼Œå¯ä»¥å¯¹å…¶åˆ†ææ¥è¯„ä»·æ•ˆæœ å¯¹äº k-means ç®—æ³•ï¼Œå¯»æ‰¾æ–°è´¨å¿ƒç‚¹çš„æ ‡å‡†æ˜¯ï¼Œæœ€å°åŒ–æ¯ä¸ªæ•°æ®ç‚¹åˆ°æœ€è¿‘è´¨å¿ƒç‚¹çš„è·ç¦»ã€‚è¿™å«ä½œç®—æ³•çš„æƒ¯æ€§æƒé‡ï¼ˆinertiaï¼‰ï¼Œä»»ä½•ç»è¿‡è®­ç»ƒçš„ KMeans å®ä¾‹éƒ½æœ‰è¯¥å±æ€§ ä¸‹é¢å°† n_clusters ä¾æ¬¡å– 2 åˆ° 20 ä¹‹é—´çš„å€¼ï¼Œæ¯å–ä¸€ä¸ªå€¼ï¼Œk-means ç®—æ³•è¿è¡Œ 10 æ¬¡ã€‚æ¯æ¬¡è¿è¡Œç®—æ³•éƒ½è®°å½•æƒ¯æ€§æƒé‡ã€‚ Input: # æƒ¯æ€§æƒé‡ï¼Œè¿™ä¸ªå€¼æ²¡æœ‰æ„ä¹‰ï¼Œä½†æ˜¯å¯ä»¥ç”¨æ¥ç¡®å®šn_clusters print(pipeline.named_steps[\"clusterer\"].inertia_) print() inertia_scores = [] n_clusters_values = list(range(2, 20)) for n_clusters in n_clusters_values: # å½“å‰çš„æƒ¯æ€§æƒé‡ç»„ cur_inertia_scores = [] X = TfidfVectorizer(max_df=0.4).fit_transform(documents) for i in range(10): km = KMeans(n_clusters=n_clusters).fit(X) cur_inertia_scores.append(km.inertia_) inertia_scores.append(cur_inertia_scores) print(\"{}: {}\".format(n_clusters, np.mean(cur_inertia_scores))) Output: 291.45747555507467 2 : 310.72961350285766 3 : 305.7904332223444 4 : 302.18859768191396 5 : 300.28785590112705 6 : 297.48005120447067 7 : 294.226862724111 8 : 292.340968109182 9 : 291.18707107605024 10 : 289.46981977256536 11 : 287.9333326469133 12 : 285.0561596766078 13 : 284.33745019948356 14 : 282.71178879028537 15 : 280.94991762471807 16 : 279.9555799316599 17 : 278.3825941905214 18 : 274.94616060558434 19 : 275.0297854253871 å°†ä¸Šè¡¨ä½œå›¾ Input: import plotly data = plotly.graph_objs.Scatter( x=list(range(18)), y=[ 310.73, 305.79, 302.18, 300.28, 297.48, 294.22, 292.34, 291.18, 289.46, 287.93, 285.05, 284.33, 282.71, 280.94, 279.95, 278.38, 274.94, 275.02, ], ) fig = plotly.graph_objs.Figure(data) fig.show() Output: æ ¹æ®ä¸Šå›¾å¯ä»¥å‘ç°åœ¨ n_clusters=9 å’Œ 15 æ—¶æ‹ç‚¹æ¯”è¾ƒæ˜æ˜¾ï¼Œè¿™é‡Œä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œæˆ‘ä»¬æŒ‰ç…§ä¹¦ä¸Šé€‰æ‹© 6 Input: # è®¾ç½®n_clusterså€¼ä¸º6ï¼Œ é‡æ–°è¿è¡Œç®—æ³• n_clusters = 6 pipeline = Pipeline( [ (\"feature_extraction\", TfidfVectorizer(max_df=0.4)), (\"clusterer\", KMeans(n_clusters=n_clusters)), ] ) pipeline.fit(documents) labels = pipeline.predict(documents) # è·å–ç‰¹å¾çš„æ‰€å¯¹åº”çš„è¯ terms = pipeline.named_steps[\"feature_extraction\"].get_feature_names() # ç»Ÿè®¡6ä¸ªç°‡ä¸­æ¯ä¸ªç°‡çš„å…ƒç´ ä¸ªæ•° c = Counter(labels) for cluster_number in range(n_clusters): print( \"Cluster {}contains {}samples\".format(cluster_number, c[cluster_number]) ) print(\" Most important terms\") centroid = pipeli","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"èšç±»èåˆ èšç±»ç®—æ³•ä¹Ÿå¯ä»¥è¿›è¡Œèåˆï¼Œè¿™æ ·åšçš„ä¸»è¦åŸå› æ˜¯ï¼Œèåˆåå¾—åˆ°çš„ç®—æ³•èƒ½å¤Ÿå¹³æ»‘ç®—æ³•å¤šæ¬¡è¿è¡Œæ‰€å¾—åˆ°çš„ä¸åŒç»“æœã€‚å¤šæ¬¡è¿è¡Œ k-means ç®—æ³•å¾—åˆ°çš„ç»“æœå› æœ€åˆé€‰æ‹©çš„è´¨å¿ƒç‚¹ä¸åŒè€Œä¸åŒã€‚å¤šæ¬¡è¿è¡Œç®—æ³•ï¼Œç»¼åˆè€ƒè™‘æ‰€å¾—åˆ°çš„å¤šä¸ªç»“æœï¼Œå¯ä»¥å‡å°‘æ³¢åŠ¨ã€‚èšç±»èåˆæ–¹æ³•è¿˜å¯ä»¥é™ä½å‚æ•°é€‰æ‹©å¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚å¤§å¤šæ•°èšç±»ç®—æ³•å¯¹å‚æ•°é€‰æ‹©å¾ˆæ•æ„Ÿ,å‚æ•°ç¨æœ‰ä¸åŒå°†å¸¦æ¥ä¸åŒçš„èšç±»ç»“æœ æœ€åŸºæœ¬çš„èåˆæ–¹æ³•æ˜¯å¯¹æ•°æ®è¿›è¡Œå¤šæ¬¡èšç±»ï¼Œæ¯æ¬¡éƒ½è®°å½•å„ä¸ªæ•°æ®ç‚¹çš„ç°‡æ ‡ç­¾ã€‚ç„¶åè®¡ç®—æ¯ä¸¤ä¸ªæ•°æ®ç‚¹è¢«åˆ†åˆ°åŒä¸€ä¸ªç°‡çš„æ¬¡æ•°ã€‚è¿™å°±æ˜¯è¯æ®ç´¯ç§¯ç®—æ³•ï¼ˆEvidence Accumulation Clusteringï¼ŒEACï¼‰çš„ç²¾é«“ ç¬¬ä¸€æ­¥ï¼Œä½¿ç”¨ k-means ç­‰ä½æ°´å¹³çš„èšç±»ç®—æ³•å¯¹æ•°æ®é›†è¿›è¡Œå¤šæ¬¡èšç±»ï¼Œè®°å½•æ¯ä¸€æ¬¡è¿­ä»£ä¸¤ä¸ªæ•°æ®ç‚¹å‡ºç°åœ¨åŒä¸€ç°‡çš„é¢‘ç‡ï¼Œå°†ç»“æœä¿å­˜åˆ°å…±åçŸ©é˜µï¼ˆcoassociationï¼‰ä¸­ ç¬¬äºŒæ­¥ï¼Œä½¿ç”¨å¦å¤–ä¸€ç§èšç±»ç®—æ³•â€”â€”åˆ†çº§èšç±»å¯¹ç¬¬ä¸€æ­¥å¾—åˆ°çš„å…±åçŸ©é˜µè¿›è¡Œèšç±»åˆ†æã€‚åˆ†çº§èšç±»ä¸€ä¸ªæ¯”è¾ƒæœ‰è¶£çš„ç‰¹æ€§æ˜¯ï¼Œå®ƒç­‰ä»·äºå¯»æ‰¾ä¸€æ£µæŠŠæ‰€æœ‰èŠ‚ç‚¹è¿æ¥åˆ°ä¸€èµ·çš„æ ‘ï¼Œå¹¶æŠŠæƒé‡ä½çš„è¾¹å»æ‰ã€‚ Input: # éå†æ‰€æœ‰æ ‡ç­¾ï¼Œè®°å½•å…·æœ‰ç›¸åŒæ ‡ç­¾çš„ä¸¤ä¸ªæ•°æ®ç‚¹çš„ä½ç½®ï¼Œåˆ›å»ºå…±åçŸ©é˜µ def create_coassociation_matrix(labels): rows = [] cols = [] # labelsç§ç±» unique_labels = set(labels) for label in unique_labels: # æ‰¾å‡ºlabelå€¼ç›¸åŒçš„æ•°æ®ç‚¹ indices = np.where(labels == label)[0] # è®°å½•ä»–ä»¬çš„ä½ç½®ï¼šå¦‚1ã€3ç‚¹çš„æ•°æ®å‡ä¸º1ï¼Œå³1å’Œ1ç›¸åŒï¼Œ1å’Œ3ç›¸åŒï¼Œ3å’Œ1ç›¸åŒï¼Œ3å’Œ3ç›¸åŒ # è¡Œå’Œåˆ—å‡å¢åŠ äº†4ä¸ªindices*indicesä¸ªæ•°å­— for index1 in indices: for index2 in indices: rows.append(index1) cols.append(index2) # è¿”å›ç»™å®šshapeå’Œtypeçš„å€¼å…¨ä¸º1çš„çŸ©é˜µ data = np.ones((len(rows),)) # åˆ›å»ºç¨€ç–çŸ©é˜µæ»¡è¶³ï¼ša[rows[k], cols[k]] = data[k] return csr_matrix((data, (rows, cols)), dtype=\"float\") # ä½¿ç”¨æ ‡ç­¾ç”Ÿæˆå…±åçŸ©é˜µ C = create_coassociation_matrix(labels) # è¿™é‡Œä¹¦ä¸Šè¯´å¤šè¾“å…¥å‡ æ¬¡Cçœ‹çœ‹ç»“æœï¼Œæˆ‘æ²¡æœ‰ç”¨notebookï¼Œä½†æ˜¯ä½¿ç”¨printè¾“å‡ºæ˜¯ä¸€æ ·çš„ï¼Œå› æ­¤æ²¡æœ‰ææ‡‚ä¹¦ä¸Šçš„å«ä¹‰ print(C) print((365 ** 2 - create_coassociation_matrix(labels).nnz) / 365 ** 2) mst = minimum_spanning_tree(C) mst = minimum_spanning_tree(-C) pipeline.fit(documents) labels2 = pipeline.predict(documents) C2 = create_coassociation_matrix(labels2) C_sum = (C + C2) / 2 mst = minimum_spanning_tree(-C_sum) # åˆ é™¤ä½äºé˜ˆå€¼çš„è¾¹ mst.data[mst.data \u003e -1] = 0 number_of_clusters, labels = connected_components(mst) Output: (0, 0) 1.0 (0, 1) 1.0 (0, 2) 1.0 (0, 3) 1.0 (0, 4) 1.0 (0, 5) 1.0 (0, 6) 1.0 (0, 7) 1.0 (0, 8) 1.0 (0, 9) 1.0 (0, 10) 1.0 (0, 11) 1.0 (0, 12) 1.0 (0, 13) 1.0 : : (364, 350) 1.0 (364, 351) 1.0 (364, 352) 1.0 (364, 353) 1.0 (364, 354) 1.0 (364, 355) 1.0 (364, 356) 1.0 (364, 357) 1.0 (364, 358) 1.0 (364, 359) 1.0 (364, 360) 1.0 (364, 361) 1.0 (364, 362) 1.0 (364, 363) 1.0 (364, 364) 1.0 0.11092512666541565 ä»å›¾çš„ç†è®ºè§’åº¦çœ‹ï¼Œç”Ÿæˆæ ‘ä¸ºæ‰€æœ‰èŠ‚ç‚¹éƒ½è¿æ¥åˆ°ä¸€èµ·çš„å›¾ã€‚æœ€å°ç”Ÿæˆæ ‘ï¼ˆMinimum Spanning Treeï¼ŒMSTï¼‰å³æ€»æƒé‡æœ€ä½çš„ç”Ÿæˆæ ‘ã€‚ç»“åˆæˆ‘ä»¬çš„åº”ç”¨æ¥è®²ï¼Œå›¾ä¸­çš„èŠ‚ç‚¹å¯¹åº”æ•°æ®é›†ä¸­çš„ä¸ªä½“ï¼Œè¾¹çš„æƒé‡å¯¹åº”ä¸¤ä¸ªé¡¶ç‚¹è¢«åˆ†åˆ°åŒä¸€ç°‡çš„æ¬¡æ•°â€”â€”ä¹Ÿå°±æ˜¯å…±åçŸ©é˜µæ‰€è®°å½•çš„å€¼ã€‚ çŸ©é˜µ C ä¸­ï¼Œå€¼è¶Šé«˜è¡¨ç¤ºä¸€ç»„æ•°æ®ç‚¹è¢«åˆ†åˆ°åŒä¸€ç°‡çš„æ¬¡æ•°è¶Šå¤šâ€”â€”è¿™ä¸ªå€¼è¡¨ç¤ºç›¸ä¼¼åº¦ã€‚ç›¸åï¼Œminimum_spanning_tree å‡½æ•°çš„è¾“å…¥ä¸ºè·ç¦»ï¼Œé«˜çš„å€¼åè€Œè¡¨ç¤ºç›¸ä¼¼åº¦è¶Šå°ã€‚è¿™é‡Œåˆç”¨åˆ°äº†ä¸€æ¬¡å–å Input: mst = minimum_spanning_tree(C) # å¯¹Cå–åå†è®¡ç®—æœ€å°ç”Ÿæˆæ ‘ mst = minimum_spanning_tree(-C) # åˆ›å»ºé¢å¤–çš„æ ‡ç­¾ pipeline.fit(documents) labels2 = pipeline.predict(documents) C2 = create_coassociation_matrix(labels2) C_sum = (C + C2) / 2 # ç”Ÿæˆé˜ˆå€¼ä¸å…¨ä¸º1å’Œ0çš„æœ€å°ç”Ÿæˆæ ‘ mst = minimum_spanning_tree(-C_sum) # åˆ é™¤ä½äºé˜ˆå€¼çš„è¾¹ mst.data[mst.data \u003e -1] = 0 number_of_clusters, labels = connected_components(mst) print(number_of_clusters) print(labels) Output: 2 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0, 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0, 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] k-means ç®—æ³•ä¸è€ƒè™‘ç‰¹å¾çš„æƒé‡ï¼Œå®ƒå¯»æ‰¾çš„æ˜¯åœ†å½¢ç°‡ï¼ˆcircular clustersï¼‰ è¯æ®ç´¯ç§¯ç®—æ³•çš„å·¥ä½œåŸç†ä¸ºé‡æ–°æŠŠç‰¹å¾æ˜ å°„åˆ°æ–°ç©ºé—´ï¼Œæ¯æ¬¡è¿è¡Œ k-means ç®—æ³•éƒ½ç›¸å½“äºä½¿ç”¨è½¬æ¢å™¨å¯¹ç‰¹å¾è¿›è¡Œä¸€æ¬¡è½¬æ¢ã€‚ è¯æ®ç´¯ç§¯ç®—æ³•åªå…³å¿ƒæ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»è€Œä¸æ˜¯å®ƒä»¬åœ¨åŸæ¥ç‰¹å¾ç©ºé—´çš„ä½ç½®ã€‚å¯¹äºæ²¡æœ‰è§„èŒƒåŒ–è¿‡çš„ç‰¹å¾ï¼Œä»ç„¶å­˜åœ¨é—®é¢˜ã€‚å› æ­¤ï¼Œç‰¹å¾è§„èŒƒå¾ˆé‡è¦ï¼Œæ— è®ºå¦‚ä½•éƒ½è¦åšï¼ˆæˆ‘ä»¬ç”¨ tf-idf è§„èŒƒç‰¹å¾å€¼ï¼Œä»è€Œä½¿ç‰¹å¾å…·æœ‰ç›¸åŒçš„å€¼åŸŸï¼‰ Input: # åˆ›å»ºè¯æ®ç´¯ç§¯ç®—æ³•ç±» class EAC(BaseEstimator, ClusterMixin): def __init__( self, n_clusterings=10, cut_threshold=0.5, n_clusters_range=(3, 10) ): self.n_clusterings = n_clusterings # k-meansç®—æ³•è¿è¡Œæ¬¡æ•° self.cut_threshold = cut_threshold # ç”¨æ¥åˆ é™¤è¾¹çš„é˜ˆå€¼ self.n_clusters_range = n_clusters_range # æ¯æ¬¡è¿è¡Œk-meansç®—æ³•è¦æ‰¾åˆ°çš„ç°‡çš„æ•°é‡ def fit(self, X, y=None): # è¿›è¡ŒæŒ‡å®šæ¬¡æ•°çš„å…±åçŸ©é˜µç´¯åŠ  C = sum( ( create_coassociation_matrix(self._single_clustering(X)) for _ in range(self.n_clusterings) ) ) mst = minimum_spanning_tree(-C) mst.data[mst.data \u003e -self.cut_threshold] = 0 self.n_components, self.labels_ = connected_components(mst) return self # è¿›è¡Œä¸€æ¬¡é›†ç¾¤ def _single_clustering(self, X): # åœ¨ç»™å®šèŒƒå›´ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªé›†ç¾¤æ•° n_clusters = np.random.randint(*self.n_clusters_range) km = KMeans(n_clusters=n_clusters) # è¿”å›ç”±k-meansè®¡ç®—å¾—åˆ°çš„ç°‡æ ‡ç­¾ return km.fit_predict(X) pipeline = Pipeline( [(\"feature_extraction\", TfidfVectorizer(max_df=0.4)), (\"clusterer\", EAC())] ) pipeline.fit(documents) number_of_clusters, labels = ( pipeline[\"clusterer\"].n_components, pipeline[\"clusterer\"].labels_, ) print(numb","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:4","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"çº¿ä¸Šå­¦ä¹  çº¿ä¸Šå­¦ä¹ æ˜¯æŒ‡ç”¨æ–°æ•°æ®å¢é‡åœ°æ”¹è¿›æ¨¡å‹ã€‚æ”¯æŒçº¿ä¸Šå­¦ä¹ çš„ç®—æ³•å¯ä»¥å…ˆç”¨ä¸€æ¡æˆ–å°‘é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œéšç€æ›´å¤šæ–°æ•°æ®çš„æ·»åŠ ï¼Œæ›´æ–°æ¨¡å‹ã€‚ çº¿ä¸Šå­¦ä¹ ä¸æµå¼å­¦ä¹ ï¼ˆstreaming-based learningï¼‰æœ‰å…³ï¼Œä½†æœ‰å‡ ä¸ªé‡è¦çš„ä¸åŒç‚¹ã€‚çº¿ä¸Šå­¦ä¹ èƒ½å¤Ÿé‡æ–°è¯„ä¼°å…ˆå‰åˆ›å»ºæ¨¡å‹æ—¶æ‰€ç”¨åˆ°çš„æ•°æ®ï¼Œè€Œå¯¹äºåè€…ï¼Œæ‰€æœ‰æ•°æ®éƒ½åªä½¿ç”¨ä¸€æ¬¡ã€‚ scikit-learn æä¾›äº† MiniBatchKMeans ç®—æ³•ï¼Œå¯ä»¥ç”¨å®ƒæ¥å®ç°çº¿ä¸Šå­¦ä¹ åŠŸèƒ½ã€‚è¿™ä¸ªç±»å®ç°äº† partial_fit å‡½æ•°ï¼Œæ¥æ”¶ä¸€ç»„æ•°æ®ï¼Œæ›´æ–°æ¨¡å‹ã€‚è°ƒç”¨fit()å°†ä¼šåˆ é™¤ä¹‹å‰çš„è®­ç»ƒç»“æœï¼Œé‡æ–°æ ¹æ®æ–°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ Input: # ä½¿ç”¨TfIDFVectorizerä»æ•°æ®é›†ä¸­æŠ½å–ç‰¹å¾ï¼Œåˆ›å»ºçŸ©é˜µX n_clusters = 6 vec = TfidfVectorizer(max_df=0.4) X = vec.fit_transform(documents) mbkm = MiniBatchKMeans(random_state=14, n_clusters=3) batch_size = 10 # éšæœºä»XçŸ©é˜µä¸­é€‰æ‹©æ•°æ®ï¼Œæ¨¡æ‹Ÿæ¥è‡ªå¤–éƒ¨çš„æ–°æ•°æ® for iteration in range(int(X.shape[0] / batch_size)): start = batch_size * iteration end = batch_size * (iteration + 1) mbkm.partial_fit(X[start:end]) # è·å–æ•°æ®é›†èšç±»ç»“æœ labels = mbkm.predict(X) c = Counter(labels) for cluster_number in range(n_clusters): print( \"Cluster {}contains {}samples\".format(cluster_number, c[cluster_number]) ) Output: Cluster 0 contains 2 samples Cluster 1 contains 362 samples Cluster 2 contains 1 samples Cluster 3 contains 0 samples Cluster 4 contains 0 samples Cluster 5 contains 0 samples ç”±äº TfIDFVectorizer ä¸æ˜¯åœ¨çº¿ç®—æ³•ï¼Œå› æ­¤æ— æ³•åœ¨æµæ°´çº¿ä¸­ä½¿ç”¨ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ HashingVectorizer ç±»ï¼Œå®ƒå·§å¦™åœ°ä½¿ç”¨æ•£åˆ—ç®—æ³•æå¤§åœ°é™ä½äº†è®¡ç®—è¯è¢‹æ¨¡å‹æ‰€éœ€çš„å†…å­˜å¼€é”€ï¼Œå°†æ•°æ®çš„å†…å®¹è½¬æ¢æˆæ•£åˆ—å€¼ Input: class PartialFitPipeline(Pipeline): def partial_fit(self, X, y=None): Xt = X # ç»è¿‡æœ€åä¸€æ­¥ä¹‹å‰çš„æ‰€æœ‰æ­¥è½¬æ¢ for name, transform in self.steps[:-1]: Xt = transform.transform(Xt) #ã€€è°ƒç”¨MiniBatchKMeansçš„partial_fitå‡½æ•° return self.steps[-1][1].partial_fit(Xt, y=y) pipeline = PartialFitPipeline( [ (\"feature_extraction\", HashingVectorizer()), (\"clusterer\", MiniBatchKMeans(random_state=14, n_clusters=3)), ] ) batch_size = 10 for iteration in range(int(len(documents) / batch_size)): start = batch_size * iteration end = batch_size * (iteration + 1) pipeline.partial_fit(documents[start:end]) labels = pipeline.predict(documents) c = Counter(labels) for cluster_number in range(n_clusters): print( \"Cluster {}contains {}samples\".format(cluster_number, c[cluster_number]) ) Output: Cluster 0 contains 4 samples Cluster 1 contains 76 samples Cluster 2 contains 285 samples Cluster 3 contains 0 samples Cluster 4 contains 0 samples Cluster 5 contains 0 samples è¿™ä¸€ç« çš„å†…å®¹æ¯”è¾ƒå¤šï¼Œä¹Ÿå­¦äº†æŒºä¹…ï¼Œè™½ç„¶ä¸­é—´ç»“æœè·Ÿä¹¦ä¸Šçš„å·®çš„æœ‰ç‚¹å¤šã€‚ã€‚å¯èƒ½æ˜¯å› ä¸ºæœ€è¿‘æ–°å† è‚ºç‚å§(ï¿£_,ï¿£ ) ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:5","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬åä¸€ç«  æœ¬ç« ä»‹ç»å¦‚ä½•ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:11:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æ·±åº¦ç¥ç»ç½‘ç»œ æ·±åº¦ç¥ç»ç½‘ç»œå’Œç¬¬ 8 ç« ä¸­çš„åŸºæœ¬ç¥ç»ç½‘ç»œçš„å·®åˆ«åœ¨äºè§„æ¨¡å¤§å°ã€‚è‡³å°‘åŒ…å«ä¸¤å±‚éšå«å±‚çš„ç¥ç»ç½‘ç»œè¢«ç§°ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œã€‚ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒå…¶å®å°±æ˜¯ä¸€ç³»åˆ—çŸ©é˜µè¿ç®—ï¼Œä¸¤ä¸ªç½‘ç»œä¹‹é—´è¿æ¥çš„æƒé‡å¯ä»¥ç”¨çŸ©é˜µæ¥è¡¨ç¤ºã€‚å…¶ä¸­è¡Œè¡¨ç¤ºå‰ä¸€å±‚ç¥ç»å…ƒï¼Œåˆ—è¡¨ç¤ºåä¸€å±‚ç¥ç»å…ƒï¼Œä¸€ä¸ªç¥ç»ç½‘ç»œå°±å¯ä»¥ç”¨ä¸€ç»„è¿™æ ·çš„çŸ©é˜µæ¥è¡¨ç¤ºã€‚é™¤äº†ç¥ç»å…ƒå¤–ï¼Œæ¯å±‚å¢åŠ ä¸€ä¸ªåç½®é¡¹ï¼Œå®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ç¥ç»å…ƒï¼Œæ°¸è¿œå¤„äºæ¿€æ´»çŠ¶æ€ï¼Œå¹¶ä¸”è·Ÿä¸‹ä¸€å±‚çš„æ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½æœ‰è¿æ¥ã€‚ ç¥ç»ç½‘ç»œä½¿ç”¨å·ç§¯å±‚ï¼ˆä¸€èˆ¬æ¥è¯´ï¼Œä»…å·ç§¯ç¥ç»ç½‘ç»œåŒ…å«è¯¥å±‚ï¼‰å’Œæ± åŒ–å±‚ï¼ˆpooling layerï¼‰ï¼Œæ± åŒ–å±‚æ¥æ”¶æŸä¸ªåŒºåŸŸæœ€å¤§è¾“å‡ºå€¼ï¼Œå¯ä»¥é™ä½å›¾åƒä¸­çš„å¾®å°å˜åŠ¨å¸¦æ¥çš„å™ªéŸ³ï¼Œå‡å°‘ï¼ˆdown-sampleï¼Œé™é‡‡æ ·ï¼‰ä¿¡æ¯é‡ï¼Œè¿™æ ·åç»­å„å±‚æ‰€éœ€å·¥ä½œé‡ä¹Ÿä¼šç›¸åº”å‡å°‘ã€‚ ä½¿ç”¨ Iris æ•°æ®é›†è¿›è¡Œå¯¹æ¯”å®éªŒ Input: import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from keras.layers import Dense from keras.models import Sequential from matplotlib import pyplot as plt iris = load_iris() X = iris.data.astype(np.float32) y_true = iris.target.astype(np.int32) # é¢„å¤„ç†æ•°æ®é›† y_onehot = OneHotEncoder().fit_transform(y_true.reshape(-1, 1)) y_onehot = y_onehot.astype(np.int64).todense() X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, random_state=14) input_layer_size, hidden_layer_size, output_layer_size = 4, 6, 3 # éšå«å±‚ hidden_layer = Dense(output_dim=hidden_layer_size, input_dim=input_layer_size, activation='relu') # è¾“å‡ºå±‚ output_layer = Dense(output_layer_size, activation='sigmoid') # åˆ›å»ºé¡ºåºæ¨¡å‹ model = Sequential(layers=[hidden_layer, output_layer]) # ä¸ºè®­ç»ƒç¥ç»ç½‘ç»œé…ç½®æ¨¡å‹ # æŸå¤±å‡½æ•°è®¾ç½®ä¸ºå‡æ–¹è¯¯å·®ï¼Œä¼˜åŒ–å™¨è®¾ç½®ä¸ºadam(äºšå½“)å³éµå¾ªåŸå§‹æ–‡ä»¶ä¸­çš„é»˜è®¤å‚æ•°ï¼ŒæŒ‡å®šç²¾åº¦è¡¡é‡æ ‡å‡† model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy']) # å½“ä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†é€šè¿‡äº†ç¥ç»ç½‘ç»œä¸€æ¬¡å¹¶ä¸”è¿”å›äº†ä¸€æ¬¡ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºä¸€æ¬¡epoch # ä¸ºæ¨¡å‹è®­ç»ƒå›ºå®šçš„epochï¼ˆæ•°æ®é›†ä¸Šçš„è¿­ä»£ï¼‰ # è¾“å‡ºæ¨¡å¼ã€‚0ä¸è¾“å‡ºï¼Œ1æ¯ä¸ªepochä¸€ä¸ªè¿›åº¦æ¡ï¼Œ2ä¸€è¡Œæ¯ä¸ªepochã€‚ history = model.fit(X_train, y_train, nb_epoch=100, verbose=2) # è®°å½•äº†è¿ç»­å‡ ä¸ªepochçš„è®­ç»ƒæŸå¤±å€¼å’Œåº¦é‡å€¼ï¼Œä»¥åŠéªŒè¯æŸå¤±å€¼å’ŒéªŒè¯åº¦é‡å€¼(å¦‚æœé€‚ç”¨çš„è¯) history.history # ä½œå›¾ï¼Œç»˜åˆ¶å‡ºepochå’Œlosså…³ç³»å›¾ plt.figure(figsize=(10, 10)) plt.plot(history.epoch, history.history['loss']) plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() # ä¸ºè¾“å…¥æ ·æœ¬ç”Ÿæˆè¾“å‡ºé¢„æµ‹ï¼Œè®¡ç®—æ˜¯åˆ†æ‰¹è¿›è¡Œçš„ # è¿”å›çš„æ˜¯æ•°å€¼[0.9356668, 0.20588416, 0.00021186471],ä»£è¡¨æ ·æœ¬å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ y_pred = model.predict(X_test) # è¿”å›ä¸€ä¸²é¢„æµ‹ç»“æœï¼Œæ ·æœ¬å±äºå“ªä¸€ä¸ªç±»åˆ« y_pred = model.predict_classes(X_test) y_pred = model.predict_classes(X_test) print(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred)) Output: precision recall f1-score support 0 1.00 1.00 1.00 17 1 1.00 0.08 0.14 13 2 0.40 1.00 0.57 8 accuracy 0.68 38 macro avg 0.80 0.69 0.57 38 weighted avg 0.87 0.68 0.62 38 é‡å¤ä¸Šé¢çš„æ“ä½œï¼Œè¿™æ¬¡è¿è¡Œ 1000 æ­¥ï¼Œå¯¹æ¯”å®éªŒç»“æœ Input: hidden_layer = Dense(output_dim=hidden_layer_size, input_dim=input_layer_size, activation='relu') output_layer = Dense(output_layer_size, activation='sigmoid') model = Sequential(layers=[hidden_layer, output_layer]) model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy']) history = model.fit(X_train, y_train, nb_epoch=1000, verbose=False) plt.figure(figsize=(12, 8)) plt.plot(history.epoch, history.history['loss']) plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show(\"keras_on_iris_2.png\") y_pred = model.predict_classes(X_test) print(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred)) Output: precision recall f1-score support 0 1.00 1.00 1.00 17 1 1.00 1.00 1.00 13 2 1.00 1.00 1.00 8 accuracy 1.00 38 macro avg 1.00 1.00 1.00 38 weighted avg 1.00 1.00 1.00 38 ä»ç»“æœå¯ä»¥çœ‹å‡ºï¼Œç»è¿‡ 100 æ­¥è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ­£ç¡®ç‡è¾¾åˆ°äº† 68%ï¼Œç»è¿‡ 1000 æ­¥è®­ç»ƒåæ­£ç¡®ç‡è¾¾åˆ°äº† 100% éªŒè¯ç è¯†åˆ«å®éªŒ Input: import numpy as np from PIL import Image, ImageDraw, ImageFont from skimage import transform as tf from matplotlib import pyplot as plt from sklearn.utils import check_random_state from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from keras.layers import Dense from keras.models import Sequential from skimage.measure import label, regionprops def create_captcha(text, shear=0, size=(100, 30), scale=1): im = Image.new(\"L\", size, \"black\") draw = ImageDraw.Draw(im) font = ImageFont.truetype( \"/home/saltfish/Programming/Python/data_mining/ch11/FiraCode-Medium.otf\", 22 ) draw.text((0, 0), text, fill=1, font=font) image = np.array(im) affine_tf = tf.AffineTransform(shear=shear) image = tf.warp(image, affine_tf) image = image / image.max() shape = image.shape # Apply scale shapex, shapey = (","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:11:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ä½¿ç”¨ GPU ä¼˜åŒ– ä¸ºäº†è®©æˆ‘çš„ GPU èƒ½è·‘ç¨‹åºï¼Œå¯è´¹äº†æˆ‘å¥½å¤§åŠŸå¤«ï¼Œç»“æœæˆ‘è¿™ 960M çš„ 2G å†…å­˜è¿˜è·‘ä¸äº†å¤ªå¤§çš„ç¨‹åº/(ã„’ o ã„’)/~~ ç¬¬ 101 æ¬¡æƒ³å¿µæˆ‘çš„å°å¼æœºï¼Œå¯æ¶çš„ç—…æ¯’ é…ç½®çš„è¿‡ç¨‹è·Ÿ TensorFlow å®˜ç½‘ç»™çš„æ–¹æ³•æ²¡å•¥åŒºåˆ«ï¼Œåœ¨è¿™å°±ä¸å¤šè¯´äº†ï¼ˆå®˜ç½‘ç»™å‡ºçš„ NVIDIA æ˜¾å¡é©±åŠ¨ç‰ˆæœ¬æ˜¯ 430ï¼Œæˆ‘è¿™é‡Œæ˜¯ 440ï¼ŒCUDA ç‰ˆæœ¬æ˜¯ 10.2ï¼Œä¾ç„¶èƒ½è¿è¡Œç¨‹åºï¼Œå¯èƒ½åªéœ€è¦ development and runtime libraries æ­£ç¡®å®‰è£…å°±è¡Œï¼Ÿï¼‰ ä½¿ç”¨ tensorflow åœ¨æ‰§è¡Œ modle.compile() çš„æ—¶å€™éœ€è¦è¾ƒé•¿çš„æ—¶é—´ï¼Œè¿è¡Œæ—¶çš„é€Ÿåº¦è¿˜æ˜¯å¾ˆå¿«çš„ åˆæ¬¡æ¥è§¦ç¥ç»ç½‘ç»œï¼Œä¸äº†è§£çš„ä¸œè¥¿å¤ªå¤šäº†ï¼Œè¿˜æ˜¯å…ˆå¤šåšå‡ ä¸ªè®­ç»ƒå†è¯´å§ã€‚ã€‚ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:11:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"åº”ç”¨ ä¹¦ä¸Šä½¿ç”¨ CIFAR å›¾åƒæ•°æ®é›†çš„ä»£ç å¤ªè€äº†ï¼ˆåŸè°…æˆ‘å¤ªèœäº†è§£å†³ä¸äº†ä¾èµ–é—®é¢˜ï¼‰ï¼Œå› æ­¤æˆ‘è·Ÿç€ Tensorflow å®˜ç½‘çš„ä»£ç åšå®Œäº†è¿™ä¸ªå®éªŒ æœè£…è¯†åˆ« Input: # -*- coding: utf-8 -*- from __future__ import absolute_import, division, print_function, unicode_literals import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt if __name__ == \"__main__\": # --------åŠ è½½ã€äº†è§£ã€é¢„å¤„ç†æ•°æ®é›†-------- fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = [ \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\", ] # æŸ¥çœ‹æ•°æ®é›† print( train_images.shape, # (60000ï¼Œ28ï¼Œ28) len(train_labels), # 60000 train_labels, # [9 0 0 ... 3 0 5] test_images.shape, # (10000, 28, 28) len(test_labels), # 10000 ) Output: (60000, 28, 28) 60000 [9 0 0 ... 3 0 5] (10000, 28, 28) 10000 Input: # æŸ¥çœ‹å›¾åƒ plt.figure() plt.imshow(train_images[0]) plt.colorbar() plt.grid(False) plt.show() # é¢„å¤„ç†æ ‡å‡†åŒ– train_images = train_images / 255.0 test_images = test_images / 255.0 # æŸ¥çœ‹æ•°æ®é›† plt.figure(figsize=(10, 10)) for i in range(25): plt.subplot(5, 5, i + 1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]]) plt.show() Output: Input: # --------å»ºç«‹æ¨¡å‹-------- # å»ºç«‹ç¥ç»ç½‘ç»œæ‰€éœ€è¦æ¨¡å‹çš„å„å±‚ # tf.keras.layers.Flattenå°†å›¾åƒçš„æ ¼å¼ä»äºŒç»´æ•°ç»„(28 * 28)è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„(28 * 28 = 784) # å¯ä»¥å°†è¿™ä¸ªå›¾å±‚çœ‹ä½œæ˜¯å›¾åƒä¸­å–æ¶ˆå †å çš„åƒç´ è¡Œï¼Œå¹¶å°†å®ƒä»¬æ’åˆ—èµ·æ¥ # è¿™ä¸ªå±‚æ²¡æœ‰å‚æ•°éœ€è¦å­¦ä¹ ; å®ƒåªæ˜¯é‡æ–°æ ¼å¼åŒ–æ•°æ®ã€‚ # # ç„¶åæ˜¯ä¸¤ä¸ªç¨ å¯†å±‚ï¼ˆå®Œå…¨è¿æ¥çš„å±‚ï¼‰ï¼Œä¸­é—´ä¸€å±‚æœ‰128ä¸ªèŠ‚ç‚¹ï¼Œ # æœ€åä¸€å±‚è¿”å›é•¿åº¦ä¸º10çš„å¯¹æ•°æ•°ç»„ã€‚æ¯ä¸ªç¥ç»å…ƒåŒ…å«ä¸€ä¸ªå¾—åˆ†ï¼ŒæŒ‡ç¤ºå½“å‰å›¾åƒå¯¹è¿™ä¸€ç±»çš„è¯„åˆ† model = keras.Sequential( [ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=\"relu\"), keras.layers.Dense(10), ] ) # --------ç¼–è¯‘æ¨¡å‹-------- # æŸå¤±å‡½æ•°ï¼šè¿™å¯ä»¥è¡¡é‡è®­ç»ƒæœŸé—´æ¨¡å‹çš„å‡†ç¡®ç¨‹åº¦ï¼Œå¸Œæœ›æœ€å°åŒ–è¿™ä¸ªå‡½æ•°ï¼Œä»¥ä¾¿å°†æ¨¡å‹â€œå¼•å¯¼â€åˆ°æ­£ç¡®çš„æ–¹å‘ # ä¼˜åŒ–å™¨ï¼šå¦‚ä½•åŸºäºå®ƒçœ‹åˆ°çš„æ•°æ®å’Œå®ƒçš„æŸå¤±å‡½æ•°æ›´æ–°æ¨¡å‹ # æŒ‡æ ‡ï¼šç”¨äºæ£€æµ‹è®­ç»ƒå’Œæµ‹è¯•æ­¥éª¤ã€‚ä¸‹é¢çš„ä¾‹å­ä½¿ç”¨ç²¾ç¡®åº¦ï¼Œå³æ­£ç¡®åˆ†ç±»çš„å›¾åƒçš„åˆ†æ•° model.compile( optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"], ) # --------è®­ç»ƒæ¨¡å‹-------- model.fit(train_images, train_labels, epochs=10) # --------è¯„ä¼°è¡¨ç°-------- test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print(\"\\nTest accuracy:\", test_acc) probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()]) # predictionæ˜¯ç”±10ä¸ªæ•°å­—ç»„æˆçš„æ•°ç»„ã€‚å®ƒä»¬è¡¨ç¤ºæ¨¡å‹å¯¹å›¾åƒå¯¹åº”äº10ç§ä¸åŒè¡£æœå„è‡ªçš„ç½®ä¿¡åº¦ predictions = probability_model.predict(test_images) print(predictions[0]) Output: pciBusID: 0000:02:00.0 name: GeForce GTX 960M computeCapability: 5.0 coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s Epoch 1/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4919 - accuracy: 0.8271 Epoch 2/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.3758 - accuracy: 0.8648 Epoch 3/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3346 - accuracy: 0.8770 Epoch 4/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3099 - accuracy: 0.8860 Epoch 5/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2927 - accuracy: 0.8927 Epoch 6/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2807 - accuracy: 0.8962 Epoch 7/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2655 - accuracy: 0.9010 Epoch 8/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2548 - accuracy: 0.9044 Epoch 9/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2440 - accuracy: 0.9095 Epoch 10/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2373 - accuracy: 0.9113 313/313 - 0s - loss: 0.3479 - accuracy: 0.8798 Test accuracy: 0.879800021648407 [1.3496768e-07 1.5826453e-10 1.7375668e-09 2.1999605e-10 5.5648923e-07 1.9829762e-03 1.9957926e-07 1.8424643e-04 9.3086570e-09 9.9783188e-01] ä»è¾“å‡ºå¯ä»¥çœ‹å‡º loss å‡½æ•°æ­£åœ¨é€æ¸å‡å°ï¼Œè®­ç»ƒçš„å‡†ç¡®ç‡åœ¨ä¸æ–­çš„å¢åŠ ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æ‰€è¦çš„ åœ¨è®­ç»ƒé›†ä¸­çš„å‡†ç¡®ç‡ä¸º 91.1%ï¼Œ è€Œåœ¨æµ‹è¯•é›†ä¸­åªæœ‰ 88%ï¼Œè¿™æ˜¯å‡ºç°äº†è¿‡æ‹Ÿåˆ(overfitting)ï¼Œå…³äºè¿‡æ‹Ÿåˆçš„è¯æ˜å’Œé¿å…è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼Œç­‰è¿‡å‡ å¤©å•ç‹¬å†™ä¸€ä¸ª post å­¦ä¹ ä¸€ä¸‹ å®šä¹‰ä¸¤ä¸ªå‡½æ•°ç”¨æ¥ç»˜å›¾ï¼Œæ›´ç›´è§‚åœ°çœ‹å‡ºé¢„æµ‹ç»“æœ Input: # --------éªŒè¯æ¨¡å‹-------- # åˆ¶ä½œå›¾è¡¨æ¥è§‚å¯Ÿåä¸ªç±»åˆ«é¢„æµ‹çš„å®Œæ•´é›†åˆ def plot_image(i, predictions_array, tru","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:11:3","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"ç¬¬åäºŒç«  æœ¬ç« ä¸»è¦ä»‹ç»äº† python ä½¿ç”¨ MapReduce æ¥è¿›è¡Œå¤§æ•°æ®å¤„ç† ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:12:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"MapReduce ä¾‹å­ MapReduce ä¸»è¦åˆ†ä¸ºä¸¤æ­¥ï¼šæ˜ å°„ï¼ˆMapï¼‰å’Œè§„çº¦ï¼ˆReduceï¼‰ Input: # -*- coding: utf-8 -*- from collections import defaultdict from sklearn.datasets import fetch_20newsgroups from joblib import Parallel, delayed import timeit # è®¡ç®—documentsä¸­çš„å•è¯å‡ºç°è¯ç´  def map_word_count(document_id, document): counts = defaultdict(int) for word in document.split(): counts[word] += 1 for word in counts: yield word, counts[word] # å°†mapå¾—åˆ°çš„ç»“æœï¼Œå³æ¯ç¯‡æ–‡ç« ä¸­å•è¯å‡ºç°çš„æ¬¡æ•°æ•´åˆèµ·æ¥ # å¦‚æ–‡ç« 1ä¸­å•è¯\"apple\"å‡ºç°äº†2æ¬¡ï¼Œæ–‡ç« 2ä¸­å•è¯\"apple\"å‡ºç°äº†5æ¬¡ï¼Œåˆ™è¿”å›ç»“æœä¸º[\"apple\":[2,5],...] def shuffle_words(results_generators): records = defaultdict(list) # éå†æ¯ä¸€ç¯‡æ–‡ç«  for results in results_generators: # éå†æ¯ä¸ªå•è¯ for word, count in results: records[word].append(count) # æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªå•è¯ for word in records: yield word, records[word] # å°†å•è¯æ‰€å¯¹åº”çš„åˆ—è¡¨å åŠ èµ·æ¥å¾—åˆ°å•è¯å‡ºç°æ¬¡æ•° def reduce_counts(word, list_of_counts): return word, sum(list_of_counts) if __name__ == \"__main__\": dataset = fetch_20newsgroups(subset=\"train\") documents = dataset.data start = timeit.default_timer() # ç”Ÿæˆå™¨ï¼Œè¾“å‡º(å•è¯ï¼Œå‡ºç°æ¬¡æ•°çš„é”®å€¼å¯¹) map_results = map(map_word_count, range(len(documents)), documents) shuffle_results = shuffle_words(map_results) reduce_results = [ reduce_counts(word, list_of_counts) for word, list_of_counts in shuffle_results ] end = timeit.default_timer() print(reduce_results[:5]) print(len(reduce_results)) print(\"----------\", str(end - start)) Output: pydev debugger: process 7540 is connecting [('From:', 11536), ('lerxst@wam.umd.edu', 2), (\"(where's\", 3), ('my', 7679), ('thing)', 9)] 280308 ---------- 4.087287616999674 æ¥ä¸‹æ¥å¯¼å…¥ joblib åº“ï¼Œå°† map å·¥ä½œåˆ†é…å‡ºå»ï¼Œä½¿ç”¨ 4 ä¸ªè¿›ç¨‹è¿›è¡Œè®¡ç®— Input: def map_word_count(document_id, document): counts = defaultdict(int) for word in document.split(): counts[word] += 1 return list(counts.items()) start = timeit.default_timer() map_results = Parallel(n_jobs=4)( delayed(map_word_count)(i, document) for i, document in enumerate(documents) ) shuffle_results = shuffle_words(map_results) reduce_results = [ reduce_counts(word, list_of_counts) for word, list_of_counts in shuffle_results ] end = timeit.default_timer() print(reduce_results[:5]) print(len(reduce_results)) print(\"----------\", str(end - start)) Output: pydev debugger: process 7566 is connecting pydev debugger: process 7556 is connecting pydev debugger: process 7552 is connecting pydev debugger: process 7561 is connecting [('From:', 11536), ('lerxst@wam.umd.edu', 2), (\"(where's\", 3), ('my', 7679), ('thing)', 9)] 280308 ---------- 3.5958340090001 å¯ä»¥çœ‹åˆ°è¿è¡Œæ—¶é—´ç¡®å®å‡å°‘äº†ï¼ˆæ•°æ®é›†å¤ªå°‘äº†æ•ˆæœä¸æ€ä¹ˆæ ·ï¼‰ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:12:1","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"MapReduce åº”ç”¨ ä¹¦ä¸­ä½¿ç”¨ blogs çš„æ•°æ®é›†ï¼Œæœ‰ 19320 ä¸ªäººçš„ blog ä¿¡æ¯ æ‰‹å¤´ä¸Šçš„ç”µè„‘é…ç½®æœ‰ç‚¹ä¸è¡Œäº†ï¼Œè·‘çš„å±å®è´¹åŠ²ï¼Œå°±æ”¾åœ¨è¿™äº†ï¼ˆå…¶å®æ˜¯è¿«ä¸åŠå¾…æƒ³å»åšåš tensorflow çš„ç»ƒä¹ äº†å˜¿å˜¿ï¼‰ ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:12:2","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"æ¥ä¸‹æ¥çš„æ–¹å‘ ä¹¦ä¸­æ ¹æ®æ¯ä¸€ç« çš„å†…å®¹ï¼Œéƒ½æœ‰æ›´è¿›ä¸€æ­¥çš„å®è·µï¼Œæˆ‘ä¼šé€‰å‡ ä¸ªå•ç‹¬åšä¸€ä¸‹ç»ƒä¹  Done ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:13:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"å‚è€ƒé“¾æ¥ python-3.8.2-doc pandas-doc numpy-doc scikit-learn tensorflow ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:14:0","tags":["data science"],"title":"æ•°æ®æŒ–æ˜å…¥é—¨ä¸å®è·µ","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"Pythonä¸­çš„å­—ç¬¦ä¸²å’Œç¼–ç é—®é¢˜","date":"2020-03-06","objectID":"/posts/2020-03-06-python-encode/","tags":null,"title":"Python å­—ç¬¦ä¸²å’Œç¼–ç ","uri":"/posts/2020-03-06-python-encode/"},{"categories":["python"],"content":"ç¼–ç  ç¼–ç æ˜¯æ•°æ®ä»ä¸€ç§æ ¼å¼å˜ä¸ºå¦ä¸€ç§æ ¼å¼çš„è¿‡ç¨‹ã€‚é€šè¿‡ç¼–ç ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ•°æ®ä»¥ä¸åŒçš„æ ¼å¼ä¿å­˜å’Œè½¬ç§»ã€‚ Unicode æŠŠæ‰€æœ‰è¯­è¨€éƒ½ç»Ÿä¸€åˆ°ä¸€å¥—ç¼–ç é‡Œï¼Œè¿™æ ·å°±ä¸ä¼šå†æœ‰ä¹±ç é—®é¢˜äº†ã€‚ ","date":"2020-03-06","objectID":"/posts/2020-03-06-python-encode/:1:0","tags":null,"title":"Python å­—ç¬¦ä¸²å’Œç¼–ç ","uri":"/posts/2020-03-06-python-encode/"},{"categories":["python"],"content":"Python3 å­—ç¬¦ä¸²ç±»å‹ Unicode â€“encode(â€œutf-8â€)-\u003e UTF-8 ç¼–ç çš„äºŒè¿›åˆ¶æ•°æ® UTF-8 ç¼–ç çš„äºŒè¿›åˆ¶æ•°æ® â€“decode(â€œutf-8â€)-\u003e Unicode ç¤ºä¾‹ 1ï¼š s1 = \"ä¸­æ–‡\" s2 = s1.encode(\"utf-8\") print(type(s1), s1) print(type(s2), s2) output 1: \u003cclass 'str'\u003e ä¸­æ–‡ \u003cclass 'bytes'\u003e b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' æ–‡ä»¶ä¸­å­˜å‚¨çš„éƒ½æ˜¯ byte è¿™æ ·ä¸€ä¸ªä¸€ä¸ªäºŒè¿›åˆ¶æ•°ï¼Œæ‰€ä»¥åœ¨å°† str å­˜å…¥æ–‡ä»¶æˆ–ä»æ–‡ä»¶è¯»å–å†…å®¹æ—¶éœ€è¦æŒ‡æ˜ç¼–ç ç±»å‹ã€‚ ","date":"2020-03-06","objectID":"/posts/2020-03-06-python-encode/:2:0","tags":null,"title":"Python å­—ç¬¦ä¸²å’Œç¼–ç ","uri":"/posts/2020-03-06-python-encode/"},{"categories":["python"],"content":"ç»éªŒæ€»ç»“ å†™ python ç¨‹åºçš„æ—¶å€™ï¼ŒæŠŠç¼–ç å’Œè§£ç æ“ä½œæ”¾åœ¨ç•Œé¢çš„æœ€å¤–å›´æ¥åšï¼Œç¨‹åºçš„æ ¸å¿ƒéƒ¨åˆ†ä½¿ç”¨ Unicode å­—ç¬¦ç±»å‹(Python3 ä¸­çš„ str)ã€‚ â€”â€”Effective+Python åœ¨å†™æ–‡ä»¶æ—¶æ³¨æ˜ç¼–ç ç±»å‹ with open(file_path, 'w',encode='utf-8') as f: f.write(data) åœ¨ä½¿ç”¨ requests åº“è·å–å“åº”ä¿¡æ¯æ—¶ï¼ŒæŒ‡æ˜ç¼–ç ç±»å‹ def send_req(url): headers = {} req = requests.get(url, headers) req.encoding = \"utf-8\" if req.ok: print(req.text) return req.text else: return Exception(\"è®¿é—®å¤±è´¥\") base64 ç¼–ç  Base64 æ˜¯ç½‘ç»œä¸Šæœ€å¸¸è§çš„ç”¨äºä¼ è¾“ 8Bit å­—èŠ‚ç çš„ç¼–ç æ–¹å¼ä¹‹ä¸€ï¼Œå¯ç”¨äºåœ¨ HTTP ç¯å¢ƒä¸‹ä¼ é€’è¾ƒé•¿çš„æ ‡è¯†ä¿¡æ¯ã€‚é‡‡ç”¨ Base64 ç¼–ç å…·æœ‰ä¸å¯è¯»æ€§ï¼Œéœ€è¦è§£ç åæ‰èƒ½é˜…è¯»ã€‚ å°†æ–‡ä»¶è½¬æ¢æˆ base64 ç¼–ç å½¢å¼è¿›è¡Œä¼ è¾“ã€‚ import base64 with open(file_path, 'rb') as f: res = base64.b64encode(f.read()) python3 ä¸­ str å’Œ bytes äº’æ¢å‡½æ•° def to_str(bytes_or_str): if isinstance(bytes_or_str, bytes): value = bytes_or_str.decode('utf-8') else: value = bytes_or_str return value def to_bytes(bytes_or_str): if isinstance(bytes_or_str, str): value = bytes_or_str.encode('utf-8') else: value = bytes_or_str return value å‚è€ƒèµ„æ–™ï¼š base64 ç¼–ç  Effective+Python ","date":"2020-03-06","objectID":"/posts/2020-03-06-python-encode/:3:0","tags":null,"title":"Python å­—ç¬¦ä¸²å’Œç¼–ç ","uri":"/posts/2020-03-06-python-encode/"},{"categories":["technique"],"content":"åˆ†å‰²çº¿ --- ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:1:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"æ ‡é¢˜ # ä¸€çº§æ ‡é¢˜ ## äºŒçº§æ ‡é¢˜ ### ä¸‰çº§æ ‡é¢˜ ###### å…­çº§æ ‡é¢˜ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:2:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"æ–œä½“æ–‡å­— _æ–œä½“_ æ–œä½“ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:3:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"åŠ ç²—æ–‡å­— **åŠ ç²—** åŠ ç²— ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:4:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"åˆ é™¤çº¿ ~~åˆ é™¤å†…å®¹~~ åˆ é™¤å†…å®¹ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:5:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"æ®µè½å’Œæ¢è¡Œ æ–°ä¸€æ®µè½éœ€è¦ç©ºä¸€è¡Œ æ–°çš„ä¸€æ®µ æˆ–è€…åœ¨æœ€ååŠ ä¸Šä¸¤ä¸ªç©ºæ ¼ æ¢è¡Œ æ–°ä¸€æ®µè½éœ€è¦ç©ºä¸€è¡Œ æ–°çš„ä¸€æ®µ æˆ–è€…åœ¨æœ€ååŠ ä¸Šä¸¤ä¸ªç©ºæ ¼ æ¢è¡Œ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:6:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"åˆ—è¡¨ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"åŸºæœ¬ç”¨æ³• - åˆ—è¡¨ - åˆ—è¡¨ - åˆ—è¡¨ 1. åˆ—è¡¨ 2. åˆ—è¡¨ 3. åˆ—è¡¨ - [ ] åˆ—è¡¨ - [ ] åˆ—è¡¨ - [ ] åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:1","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å¤šçº§åˆ—è¡¨ - ä¸€çº§åˆ—è¡¨ - äºŒçº§åˆ—è¡¨ - ä¸€çº§åˆ—è¡¨ ä¸€çº§åˆ—è¡¨ äºŒçº§åˆ—è¡¨ ä¸€çº§åˆ—è¡¨ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:2","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"åˆ—è¡¨ä¸­åˆ†æ®µ - é¡¹ç›®ä¸€ï¼Œæ®µè½ä¸€ é¡¹ç›®ä¸€ï¼Œæ®µè½äºŒ é¡¹ç›®ä¸€ï¼Œæ®µè½ä¸€ é¡¹ç›®ä¸€ï¼Œæ®µè½äºŒ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:3","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"åˆ—è¡¨ä¸­æ¢è¡Œ - é¡¹ç›®äºŒï¼Œç¬¬ä¸€è¡Œ é¡¹ç›®äºŒï¼Œç¬¬äºŒè¡Œ é¡¹ç›®äºŒï¼Œç¬¬ä¸€è¡Œ é¡¹ç›®äºŒï¼Œç¬¬äºŒè¡Œ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:4","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å¼•ç”¨ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"åŸºæœ¬ç”¨æ³• \u003e æˆ‘äº¦é£˜é›¶ä¹…ï¼Œåå¹´æ¥ï¼Œæ·±æ©è´Ÿå°½ï¼Œæ­»ç”Ÿå¸ˆå‹ã€‚ â€”â€”é¡¾è´è§‚ æˆ‘äº¦é£˜é›¶ä¹…ï¼Œåå¹´æ¥ï¼Œæ·±æ©è´Ÿå°½ï¼Œæ­»ç”Ÿå¸ˆå‹ã€‚ â€”â€”é¡¾è´è§‚ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:1","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å¤šçº§å¼•ç”¨ \u003e å¼•ç”¨ 1 \u003e \u003e \u003e å¼•ç”¨ 2 å¼•ç”¨ 1 å¼•ç”¨ 2 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:2","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å¼•ç”¨ä¸­åˆ†æ®µ \u003e å¼•ç”¨ \u003e \u003e å¼•ç”¨ å¼•ç”¨ å¼•ç”¨ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:3","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å¼•ç”¨ä¸­æ¢è¡Œ \u003e å¼•ç”¨ \u003e å¼•ç”¨ å¼•ç”¨ å¼•ç”¨ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:4","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"é“¾æ¥ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:9:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"æ–‡å†…é“¾æ¥ è¿™æ˜¯ä¸€ä¸ªæ–‡å†…é“¾æ¥çš„[ä¾‹å­](http://example.com/ \"é¼ æ ‡æ‚¬æµ®æ­¤å¤„æ˜¾ç¤ºçš„æ ‡é¢˜\")ã€‚ [è¿™ä¸ª](http://example.net/)é“¾æ¥åœ¨é¼ æ ‡æ‚¬æµ®æ—¶æ²¡æœ‰æ ‡é¢˜ã€‚ [è¿™ä¸ª](/about/)é“¾æ¥æ˜¯æœ¬åœ°èµ„æºã€‚ è¿™æ˜¯ä¸€ä¸ªæ–‡å†…é“¾æ¥çš„ä¾‹å­ã€‚ è¿™ä¸ªé“¾æ¥åœ¨é¼ æ ‡æ‚¬æµ®æ—¶æ²¡æœ‰æ ‡é¢˜ã€‚ è¿™ä¸ªé“¾æ¥æ˜¯æœ¬åœ°èµ„æºã€‚ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:9:1","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å¼•ç”¨é“¾æ¥ è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨é“¾æ¥çš„[ä¾‹å­][id]ã€‚ [id]: http://example.com/ \"é¼ æ ‡æ‚¬æµ®æ ‡é¢˜\" è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨é“¾æ¥çš„ä¾‹å­ã€‚ æ³¨æ„ï¼Œè¿™é‡Œçš„ id æ²¡æœ‰å¤§å°å†™åŒºåˆ†ï¼Œå¦‚æœçœç•¥ idï¼Œåˆ™å‰é¢æ–¹æ‹¬å·çš„å†…å®¹ä¼šè¢«ç”¨ä½œ idã€‚ æˆ‘å¸¸ç”¨çš„ç½‘ç«™åŒ…æ‹¬[Google][1]ï¼Œ[Yahoo][]å’Œ[MSN][]ã€‚ [1]: http://google.com/ \"Google\" [yahoo]: http://search.yahoo.com/ \"Yahoo Search\" [msn]: http://search.msn.com/ \"MSN Search\" æˆ‘å¸¸ç”¨çš„ç½‘ç«™åŒ…æ‹¬Googleï¼ŒYahooå’ŒMSNã€‚ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:9:2","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å›¾ç‰‡ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:10:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"åŸºæœ¬ç”¨æ³• ![åœ°é“](/img/post-bg-2015.jpg \"å›¾ç‰‡ä¸‹æ–¹æè¿°\") å›¾ç‰‡ä¸‹æ–¹æè¿°åœ°é“ \" å›¾ç‰‡ä¸‹æ–¹æè¿° ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:10:1","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"base64 å›¾ç‰‡æ•°æ® ![åœ°é“][base64str] [base64str]: data:image/jpg;base64,xxxxxxxxxxxxxxx... \"å›¾ç‰‡ä¸‹æ–¹æè¿°\" å›¾ç‰‡ä¸‹æ–¹æè¿°åœ°é“ \" å›¾ç‰‡ä¸‹æ–¹æè¿° ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:10:2","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"ä»£ç å— ```python print('Hello,World') ``` print('Hello,World') ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:11:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"é“¾æ¥ \u003chttps://www.baidu.com/\u003e https://www.baidu.com/ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:12:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"è½¬ä¹‰ åœ¨ä¸å¸Œæœ›ç¬¦å·è¢«å½“æˆ markdown æ ‡è¯†ç¬¦æ—¶ï¼Œç”¨\\è½¬ä¹‰ \\_ä¸æ˜¯æ–œä½“\\_ _ä¸æ˜¯æ–œä½“_ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:13:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"Karmdowm æ‰©å±• è¡¨æ ¼ | å·¦å¯¹é½ | ä¸­é—´å¯¹é½ | å³å¯¹é½ | | :----- | :------: | -----: | | å·¦ 1 | ä¸­ 1 | å³ 1 | | å·¦ 2 | ä¸­ 2 | å³ 3 | å·¦å¯¹é½ ä¸­é—´å¯¹é½ å³å¯¹é½ å·¦ 1 ä¸­ 1 å³ 1 å·¦ 2 ä¸­ 2 å³ 3 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:14:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"è„šæ³¨ è¯·å‚é˜…è„šæ³¨ 1. [^1] [^1]: è„šæ³¨ 1 å†…å®¹ã€‚ # å‡ºç°åœ¨æ–‡æœ« è¯·å‚é˜…è„šæ³¨ 2. [^2] [^2]: è„šæ³¨ 2 å†…å®¹ã€‚ # å‡ºç°åœ¨æ–‡æœ« ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:15:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"HTML æ‰©å±• ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"ä¸‹åˆ’çº¿ \u003cu\u003eä¸‹åˆ’å†…å®¹\u003c/u\u003e ä¸‹åˆ’å†…å®¹ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:1","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"ä¸Šæ ‡ S = Ï€r\u003csup\u003e2\u003c/sup\u003e S = Ï€r2 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:2","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"ä¸‹æ ‡ Water: H\u003csub\u003e2\u003c/sub\u003eO Water: H2O ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:3","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"é¦–è¡Œç¼©è¿› \u0026emsp;\u0026emsp;å¼€å§‹å†…å®¹ â€ƒå¼€å§‹å†…å®¹ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:4","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å†…éƒ¨è·³è½¬ ç‚¹æ­¤[æ ‡ç­¾](#j1)è·³è½¬ã€‚ \u003ca name=\"é”šç‚¹\" id=\"j1\" href=\"https://saltfishpr.github.io/\"\u003e\u003c/a\u003e ç‚¹æ­¤æ ‡ç­¾è·³è½¬ã€‚ id è¦åŒ¹é…ï¼Œ name å’Œ href éƒ½ä¸æ˜¯å¿…é¡»çš„ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:5","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"æ’å…¥è§†é¢‘ \u003cdiv\u003e \u003ca href=\"//player.bilibili.com/player.html?aid=93178052\u0026cid=159088790\u0026page=1\" target=\"_blank\"\u003e\u003cimg src=\"å°é¢å›¾ç‰‡è·¯å¾„\" alt=\"æ²¡æœ‰å›¾ç‰‡æ—¶æ˜¾ç¤ºæ­¤æ–‡å­—\" width=\"100%\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" /\u003e\u003c/a\u003e \u003cscript\u003e var em = document.getElementById(\"video-iframe\"); console.log(em.clientWidth); em.height = em.clientWidth * 0.75 \u003c/script\u003e \u003c/div\u003e var em = document.getElementById(\"video-iframe\"); console.log(em.clientWidth); em.height = em.clientWidth * 0.75 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:6","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"æ³¨é‡Š [^_^]: # (æ³¨é‡Šï¼Œä¸ä¼šåœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤ºã€‚) \u003cdiv style='display: none'\u003e æ³¨é‡Šå†…å®¹ \u003c/div\u003e æ³¨é‡Šå†…å®¹ \u003c!-- å¤šæ®µ æ³¨é‡Šï¼Œ ä¸ä¼šåœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤ºã€‚ --\u003e ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:7","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"å‚è€ƒèµ„æ–™ https://www.jianshu.com/p/d7d6da4b7c60#fnref1 https://guides.github.com/features/mastering-markdown/ emoji ç›®å½• ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:17:0","tags":["markdown"],"title":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":null,"content":"Helloï¼Œä¸–ç•Œ!","date":"2020-03-03","objectID":"/posts/2020-03-03-first-blog/","tags":null,"title":"ç¬¬ä¸€ç¯‡åšå®¢","uri":"/posts/2020-03-03-first-blog/"},{"categories":null,"content":"åšå®¢ï¼Œæ–¯è¾¾æ‰˜ï¼ ","date":"2020-03-03","objectID":"/posts/2020-03-03-first-blog/:0:0","tags":null,"title":"ç¬¬ä¸€ç¯‡åšå®¢","uri":"/posts/2020-03-03-first-blog/"},{"categories":null,"content":"ğŸ‘‹ ä½ å¥½ï¼Œæˆ‘æ˜¯ç¡•ã€‚ ğŸ˜ æŠ€èƒ½ï¼šä¸»è¦æ˜¯ Go, ä¼šä¸€äº› Flutter, å¾ˆä¹…æ²¡å†™ Pythonã€‚ ğŸŒ± ä¸“æ³¨äº Go å¼€å‘ã€‚ å¯¹æ¸¸æˆåˆ¶ä½œã€æ•°æ®æŒ–æ˜ã€AIã€IOT ç­‰æ–°äº‹ç‰©ä¹Ÿæ„Ÿå…´è¶£ã€‚ ğŸ“« è”ç³»æ–¹å¼ 526191197@qq.com / saltfishpr@gmail.com ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"å…³äº","uri":"/about/"}]