[{"categories":["micro service"],"content":"Consul, 服务网格的控制平面","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"服务网格 服务网格是为本地、云或混合云基础设施提供安全的服务到服务通信的专用层。服务网格通常与微服务架构模式一起使用，但可以在涉及复杂网络的任何场景中提供价值。 服务网格通常由控制平面和数据平面组成。控件平面维护一个中央注册表，可跟踪所有服务及其各自的 IP 地址，这被称为服务发现。只要应用程序在控制平面注册，控制平面就可以与服务网格的其他成员共享如何与应用程序通信并强制规定谁能彼此通信。 控制平面负责保护网格，促进服务发现、健康检查、策略执行和其他类似的操作问题。数据平面处理服务之间的通信。许多服务网格解决方案采用 sidecar 代理来处理数据平面通信，因此限制了服务对网络环境的感知水平。 ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:1:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"服务发展 ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:2:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"单体服务 模块间函数调用，以纳秒为单位 参数传递，数据无需加密 导入模块，增加功能 子系统出现问题，得重新发布整个系统 ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:2:1","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"微服务 为了提升开发效率，降低特性发布周期，开始走向微服务架构 模块间网络调用，以毫秒为单位 数据在网络中传输，需要加密 访问 IP:Port 获取服务 挑战： 服务发现 服务配置 服务分界：外部，业务，数据 服务地址由负载均衡器（如 nginx）硬编码，微服务通过负载均衡器地址调用其依赖的服务 ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:2:2","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"服务网格 服务发现：微服务服务在中央服务进行注册 服务配置：中央服务 K/V 数据库存储配置 服务分界：中央服务定义微服务之间的通信规则（Service Graph） 服务鉴权：不同的服务有不同的 TLS 证书，服务间进行双向 TLS 鉴权 sidecar proxy ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:2:3","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"部署 Consul 前置要求： docker minikube helm 克隆示例存储库 git clone https://github.com/hashicorp/learn-consul-kubernetes.git --depth=1 切换到教程目录 cd learn-consul-kubernetes/service-mesh/deploy 添加 chart 仓库 helm repo add hashicorp https://helm.releases.hashicorp.com 更新 chart 仓库 helm repo update 部署 consul helm install -f config.yaml consul hashicorp/consul --create-namespace -n consul --version \"0.39.0\" 验证安装 kubectl get pods --namespace consul --selector app=consul Output: NAME READY STATUS RESTARTS AGE consul-bz8xw 1/1 Running 0 23m consul-connect-injector-webhook-deployment-8d4f5c97b-56vk5 1/1 Running 1 (22m ago) 23m consul-connect-injector-webhook-deployment-8d4f5c97b-lp9zh 1/1 Running 0 23m consul-controller-7b588d978-2thrf 1/1 Running 0 23m consul-server-0 1/1 Running 0 23m consul-webhook-cert-manager-78669db499-wnlq8 1/1 Running 0 23m 转发 UI 端口 kubectl --namespace consul port-forward service/consul-ui 18500:80 --address 0.0.0.0 这里推荐使用 VSCode Remote SSH，提供的端口转发功能可以轻易的访问服务器的服务。 ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:3:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"更新 consul 配置 修改 config.yaml 文件 global:name:consuldatacenter:dc1metrics:enabled:trueenableAgentMetrics:trueacls:manageSystemACLs:truegossipEncryption:autoGenerate:truetls:enabled:trueenableAutoEncrypt:trueverify:trueserver:replicas:1ui:enabled:trueservice:type:\"NodePort\"connectInject:enabled:truecontroller:enabled:trueprometheus:enabled:true 使用 helm 升级 helm upgrade consul hashicorp/consul --namespace consul --version \"0.39.0\" --values ./config.yaml --wait ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:4:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"部署微服务 部署示例服务 依然是在刚刚的目录下，应用 hashicups 文件夹中的配置文件 kubectl apply -f hashicups/ Output: service/frontend created serviceaccount/frontend created servicedefaults.consul.hashicorp.com/frontend created configmap/nginx-configmap created deployment.apps/frontend created service/postgres created serviceaccount/postgres created servicedefaults.consul.hashicorp.com/postgres created deployment.apps/postgres created service/product-api created serviceaccount/product-api created servicedefaults.consul.hashicorp.com/product-api created configmap/db-configmap created deployment.apps/product-api created service/public-api created serviceaccount/public-api created servicedefaults.consul.hashicorp.com/public-api created deployment.apps/public-api created 查看 pods kubectl get pods --selector consul.hashicorp.com/connect-inject-status=injected Output: NAME READY STATUS RESTARTS AGE frontend-699cb4546-vj78k 2/2 Running 0 25m postgres-54966b4458-7gdk2 2/2 Running 0 25m product-api-688d79df6c-fk7zw 2/2 Running 0 25m public-api-5975bd4f4c-vkhrs 2/2 Running 0 25m 转发服务端口 kubectl port-forward service/frontend 18080:80 --address 0.0.0.0 浏览器打开 localhost:18080 ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:5:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["micro service"],"content":"配置零信任网络 拒绝所有请求 进入 zero-trust-network 文件夹 cd service-mesh/zero-trust-network 应用规则 kubectl apply -f deny-all.yaml 查看管理界面 访问 http://127.0.0.1:18080/ 得到 Error :( 允许特定服务间的通信 kubectl apply -f service-to-service.yaml 该文件允许下列请求的发生 frontend -\u003e public-api public-api -\u003e product-api product-api -\u003e postgres 查看管理界面 intention intention 包含四个部分 源服务 - 指定发出请求的服务。它可以是一个服务的全称，也可以*指代所有的服务。 目标服务 - 指定接收请求的服务。即在服务定义中配置的上游。它可以是一个服务的全称，也可以*指代所有的服务。 权限 - 定义是否允许源和目标之间的通信。这可以设置为 allow 或 deny。 描述 - 可选，描述 intention 的字段。 apiVersion:consul.hashicorp.com/v1alpha1kind:ServiceIntentionsmetadata:name:product-api-to-postgresspec:destination:name:postgressources:- name:product-apiaction:allow ","date":"2022-03-11","objectID":"/posts/2022-03-11-consul/:6:0","tags":["service mesh"],"title":"Consul","uri":"/posts/2022-03-11-consul/"},{"categories":["linux"],"content":"查看 查看 sh 路径 which sh /usr/bin/sh 查看默认 sh ll /usr/bin/sh lrwxrwxrwx 1 root root 4 Feb 16 21:03 /usr/bin/sh -\u003e dash* ","date":"2022-02-16","objectID":"/posts/2022-02-16-ubuntu-change-sh/:1:0","tags":null,"title":"Ubuntu 切换默认 sh","uri":"/posts/2022-02-16-ubuntu-change-sh/"},{"categories":["linux"],"content":"切换 sudo dpkg-reconfigure dash 选择 [是] 设置 sh 为 dash 选择 [否] 设置 sh 为 bash 查看 sh： ll /usr/bin/sh lrwxrwxrwx 1 root root 4 Feb 16 21:03 /usr/bin/sh -\u003e bash* ","date":"2022-02-16","objectID":"/posts/2022-02-16-ubuntu-change-sh/:2:0","tags":null,"title":"Ubuntu 切换默认 sh","uri":"/posts/2022-02-16-ubuntu-change-sh/"},{"categories":["golang"],"content":"Go 语言文档，规范，开源组件","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/","tags":null,"title":"Go 语言资源整合","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"编程规范 uber-guide: uber go guide clean-go-article: go clean code ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:1","tags":null,"title":"Go 语言资源整合","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"常用包 viper: 配置读取 zap: 日志输出 lumberjack: 日志滚动记录器 validator: 数据校验 cast: 类型转换 lo: 工具库，切片，映射，元组，集合… carbon: 时间库的扩展 cron: 定时任务 ants: goroutine 池 wire: 依赖注入 cobra: 命令行程序 resty: HTTP/REST 客户端 testify: 测试库，断言、mock go-funk: Go 实用工具库（map、find、contains、filter、chunk、reverse，…） jsoniter: 标准库 encoding/json 的升级，可直接替换，高性能 gjson: 快速从 json 中获取值 carbon: 一个轻量级、语义化、对开发者友好的 golang 时间处理库，支持链式调用 afero: Go 的文件系统抽象 fsm: 有限状态机库 fasthttp: 标准库 net/http 的升级，更快 agollo: 连接 apollo 配置中心 msgp: MessagePack 序列化，比 json 更快，数据量更小 redsync: Redis 分布式锁 sqlx: 标准库 database/sql 的扩展 kafka-go: kafka 库 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:2","tags":null,"title":"Go 语言资源整合","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"框架 rpcx: rpc 框架 gin: web 框架 fiber: web 框架 iris: HTTP/2 web 框架 cooly: 爬虫框架 watermill: 事件驱动框架 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:3","tags":null,"title":"Go 语言资源整合","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"其它 fync: GUI bubbletea: 终端应用 TUI plot: 绘图 smocker: HTTP mock server primitive: 使用几何形状将图片变为抽象画 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:4","tags":null,"title":"Go 语言资源整合","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"工具 golines: 代码格式化，处理行过长的行 安装：go install github.com/segmentio/golines@latest 使用：golines -w . ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-open-source-packages/:0:5","tags":null,"title":"Go 语言资源整合","uri":"/posts/2021-12-02-go-open-source-packages/"},{"categories":["golang"],"content":"Go 语言介绍与入门","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"简介 Go 是 Google 开发的一种静态强类型、编译型、并发型，并具有垃圾回收功能的编程语言。于 2009 年 11 月正式宣布推出， 1.0 版本在 2012 年 3 月发布。之后，Go 广泛应用于 Google 的产品以及许多其他组织和开源项目。Go 程序员常常被称为地鼠（gopher），因此地鼠也是 Go 的吉祥物。 在 Go 语言出现之前，开发者们总是面临非常艰难的抉择，究竟是使用执行速度快但是编译速度并不理想的语言（如：C++），还是使用编译速度较快但执行效率不佳的语言（如：.NET、Java），或者说开发难度较低但执行速度一般的动态语言呢？显然，Go 语言在这 3 个条件之间做到了最佳的平衡：快速编译，高效执行，易于开发。 特性： 为并发设计：协程（goroutine）、通道（channel）、同步原语（标准库 sync 包）： 节省内存、程序启动快和代码执行速度快 良好的跨平台性 开源 其它吸引我的地方： 25 个保留关键字，学习快，上手快 go 工具链，提供构建、执行、依赖管理、代码检查、代码格式化等功能 代码可读性高，大家代码风格一致 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:1:0","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"Hello World 只使用标准库构建 http 服务。 Hello World 向每个访问的客户端打印中文的“你好, 世界!”和当前的时间信息。 package main import ( \"fmt\" \"log\" \"net/http\" \"time\" ) func main() { fmt.Println(\"Please visit http://127.0.0.1:12345/\") http.HandleFunc(\"/\", func(w http.ResponseWriter, req *http.Request) { s := fmt.Sprintf(\"你好, 世界! -- Time: %s\", time.Now().String()) fmt.Fprintf(w, \"%v\\n\", s) log.Printf(\"%v\\n\", s) }) if err := http.ListenAndServe(\":12345\", nil); err != nil { log.Fatal(\"ListenAndServe: \", err) } } 运行 Hello World： ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:2:0","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"性能比较 CPU 密集型工作负载：正则表达式计算 四核 3.0GHz Intel®i5-3330® CPU 15.8 GiB RAM 2TB SATA 磁盘驱动器 Ubuntu™ 21.04 x86_64 GNU/Linux 5.11.0-18-generic 语言 运行时间 使用内存(byte) 源码大小(byte) 总执行时间 cpu 负载 Go 3.85 324,200 810 6.01 27% 19% 20% 91% Java 5.31 793,572 929 17.50 79% 78% 83% 89% web 框架对比：Java(Spring) vs Go(Fiber) 测试内容包括 JSON 序列化(JSON)、单查询(1-query)、数据更新(Updates)和纯文本返回(Plaintext)的峰值性能，单位 response/second 。以及响应时延。 物理机器： Intel® Xeon Gold 5120 CPU 32 GB 内存 企业级 SSD 思科万兆以太网交换机 Framework 返回 json 单查询 数据更新 返回纯文本 平均时延 最大时延 fiber 1,317,695 395,902 11,806 6,413,651 0.8ms 20.3ms spring 150,259 127,114 10,498 183,737 14.3ms 128.1ms 云上部署： Microsoft Azure D3v2 实例 千兆以太网 Framework 返回 json 单查询 数据更新 返回纯文本 平均时延 最大时延 fiber 167,451 52,125 1,888 1,135,808 0.7ms 17.2ms spring 20,140 18,899 1,420 28,780 11.0ms 248.9ms ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:3:0","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"Go 工具链安装与简介 windows 环境下：下载安装文件并运行。 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:4:0","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"检测安装 输入： go version 输出： go version go1.17.2 linux/arm64 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:4:1","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"工具介绍 go run: 编译并运行文件/包 go build: 编译生成可执行文件（默认当前平台当前架构） go get: 从远程拉取包 go mod：依赖管理 go install: 编译生成可执行文件并将其移动到 $GOPATH/bin 中，可以是远程包 go test: 执行测试 go fmt: 代码格式化 go vet: 代码检查 go env: 查看/设置 Go 环境变量 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:4:2","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"Go 基础 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:0","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"25 个关键字 package, import, const, var, type, func 用来声明各种代码元素 interface, chan, map 和 struct 用做 一些组合类型的字面表示中 break, case, continue, default, else, fallthrough, for, goto, if, range, return, select 和 switch 用在流程控制语句中 defer 和 go 也可以看作是流程控制关键字 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:1","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"类型系统 基本类型 字符串：string 布尔：bool 数值： uint8(byte), uint16, uint32, uint64, int8, int16, int32(rune), int64 float32, float64 complex64, complex128 uintptr、int 以及 uint 类型的值的尺寸依赖于具体编译器实现。在 64 位的架构上，int 和 uint 类型的值是 64 位的；在 32 位的架构上，它们是 32 位的。编译器必须保证 uintptr 类型的值的尺寸能够存下任意一个内存地址。 组合类型 指针类型 结构体类型 函数类型 容器类型 数组：定长容器 切片：动态长度容器 映射：又称字典类型，在标准编译器中映射是使用哈希表实现的 通道类型 接口类型 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:2","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"一个简单的栗子 数据类型，常量、变量声明，流程控制语句 // 单行注释 /* 多行 注释 */ // 导入包的子句在每个源文件的开头。 // main比较特殊，它用来声明可执行文件，而不是一个库。 package main // import语句声明了当前文件引用的包。 import ( \"fmt\" // Go语言标准库中的包 \"os\" // 系统底层函数，如文件读写 ) // main是程序执行的入口。 func main() { // 向标准输出打印一行。 fmt.Println(\"Hello, 世界\") // 调用当前包的另一个函数。 beyondHello() } // 声明函数`func functionName(parameter type, [parameter type]) ([resultValue] type, [[resultValue] type]) { body }`。 func beyondHello() { var x int // 变量声明，变量必须在使用之前声明。 x = 3 // 变量赋值。 y := 4 // 可以用:=来偷懒，它把变量类型、声明和赋值都搞定了。 fmt.Println(\"x:\", x, \"y:\", y) // 简单输出。 learnTypes() } // 内置变量类型和关键词。 func learnTypes() { str := \"少说话多读书!\" // String类型。 s2 := `这是一个 可以换行的字符串` // 同样是String类型。 // 非ascii字符。Go使用UTF-8编码。 g := 'Σ' // rune类型 f := 3.14195 // float64类型，IEEE-754，64位浮点数。 c := 3 + 4i // complex128类型，内部使用两个float64表示。 // Var变量可以直接初始化 var u uint = 7 // unsigned 无符号变量，但是实现依赖int型变量的长度。 var pi float32 = 22. / 7 // 字符转换 n := byte('\\n') // byte是uint8的别名。 // 数组（array）类型的大小在编译时即确定。 a3 := [...]int{3, 1, 5} // 有3个int变量的数组，同时进行了初始化。 var a4 [4]int // 有4个int变量的数组，初始为0。 // 切片（slice）的大小是动态的，它的长度可以按需增长。 // 用内置函数 append() 向切片末尾添加元素。 s := []int{1, 2, 3} // 这是个长度3的slice。 s = append(s, 4, 5, 6) // 再加仨元素，长度变为6了。 fmt.Println(s) // 更新后的数组是 [1 2 3 4 5 6]。 // array和slice各有所长，但是slice可以动态的增删，所以更多时候还是使用slice。 s3 := []int{4, 5, 9} // 这里没有省略号。 s4 := make([]int, 4) // 分配4个int大小的内存并初始化为0。 var d2 [][]float64 // 这里只是声明，并未分配内存空间。 bs := []byte(\"a slice\") // 进行类型转换。 // 除了向append()提供一组原子元素（写死在代码里的）以外，我们 // 还可以用如下方法传递一个slice常量或变量，并在后面加上省略号， // 用以表示我们将引用一个slice、解包其中的元素并将其添加到s数组末尾。 s = append(s, []int{7, 8, 9}...) // 第二个参数是一个slice常量。 fmt.Println(s) // 更新后的数组是 [1 2 3 4 5 6 7 8 9] p, q := learnMemory() // 声明p, q为int型变量的指针。 fmt.Println(*p, *q) // * 取值 // Map是动态可增长关联数组，和其他语言中的hash或者字典相似。 m := map[string]int{\"three\": 3, \"four\": 4} m[\"one\"] = 1 // 在Go语言中未使用的变量在编译的时候会报错，而不是warning。 // 空标识符 _ 可以使你“使用”一个变量。 _, _, _, _, _, _, _, _, _, _ = str, s2, g, f, u, pi, n, a3, s4, bs // 通常的用法是，在调用拥有多个返回值的函数时，用下划线抛弃其中的一个参数。 // 调用os.Create并用下划线变量扔掉它的错误代码，因为我们觉得这个文件一定会成功创建。 file, _ := os.Create(\"output.txt\") fmt.Fprint(file, \"这句代码还示范了如何写入文件呢\") file.Close() // 写完后关闭文件。 // 输出变量 fmt.Println(s, c, a4, s3, d2, m) learnFlowControl() } // Go全面支持垃圾回收。Go有指针，但是不支持指针运算。 func learnMemory() (p, q *int) { p = new(int) // 定义一个指向int类型的指针，空值为nil。 s := make([]int, 20) // 给20个int变量分配一块内存 s[3] = 7 // 给slice内的元素赋值 r := -2 // 声明一个局部变量 return \u0026s[3], \u0026r // \u0026 取地址，p为s的第4个元素的地址，q为r的地址。 } func learnFlowControl() { // if需要花括号，括号就免了。 if true { fmt.Println(\"这句话肯定被执行\") } // 如果太多嵌套的if语句，推荐使用switch。 x := 1 switch x = 1; x { // 在头部添加赋值表达式。 case 0: // 隐式调用break语句，匹配上一个即停止。 case 1: fallthrough // 如果想继续向下执行，需要加上 fallthrough 语句。 case 2: // 继续执行。 } // 和if一样，for也不用括号。 for x := 0; x \u003c 3; x++ { // ++ 自增。 fmt.Println(\"遍历\", x) // 内部的x覆盖了外部的x。 } // for 是go里唯一的循环关键字，不过它有很多变种。 for { break // 跳出。 continue // 继续下一个循环，这里不会运行。 } // 用range可以枚举 array、slice、string、map、channel等不同类型。 for key, value := range map[string]int{\"one\": 1, \"two\": 2, \"three\": 3} { // 打印map中的每一个键值对。 fmt.Printf(\"索引：%s, 值为：%d\\n\", key, value) } // 如果你只想要值，那就用前面讲的下划线扔掉不用的值。 for _, name := range []string{\"Bob\", \"Bill\", \"Joe\"} { fmt.Printf(\"你是。。 %s\\n\", name) } // 和for一样，在头部添加赋值表达式给y赋值，然后再和x作比较。 if y := expensiveComputation(); y \u003e x { x = y } // 闭包函数。 xBig := func() bool { return x \u003e 100 // x是上面声明的变量引用。 } fmt.Println(\"xBig:\", xBig()) // true（上面把y赋给x了）。 x /= 1e5 // x变成10。 fmt.Println(\"xBig:\", xBig()) // 现在是false。 // 除此之外，函数体可以在其他函数中定义并调用， // 满足下列条件时，也可以作为参数传递给其他函数： // a) 定义的函数被立即调用。 // b) 函数返回值符合调用者对类型的要求。 fmt.Println(\"两数相加乘二: \", func(a, b int) int { return (a + b) * 2 }(10, 2)) // 传入 10, 2 两个参数。 // 当你需要goto的时候，你会爱死它的。 // break和continue同样可以带label。 goto love love: return } func expensiveComputation() int { return 1e6 } ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:3","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"包（package） Go 使用代码包（package）来组织管理代码。 我们必须先引入一个代码包（除了 builtin 标准库包）才能使用其中导出的资源（比如函数、类型、变量和有名常量等）。 代码包目录的名称并不要求一定要和其对应的代码包的名称相同。 但是，库代码包目录的名称最好设为和其对应的代码包的名称相同。 因为一个代码包的引入路径中包含的是此包的目录名，但是此包的默认引入名为此包的名称。 如果两者不一致，会使人感到困惑。 在我们创建一个新项目时，只需要项目文件夹执行 go mod init \u003cproject name\u003e，即可使用 go mod 管理依赖 之后就可以使用 go get 拉取远程包，或使用 go mod tidy 一键处理依赖。 标准库概述 unsafe: 包含了一些打破 Go 语言“类型安全”的命令，一般的程序中不会被使用，可用在 C/C++ 程序的调用中。 syscall-os-os/exec: os: 提供给我们一个平台无关性的操作系统功能接口，采用类 Unix 设计，隐藏了不同操作系统间的差异，让不同的文件系统和操作系统对象表现一致。 os/exec: 提供我们运行外部操作系统命令和程序的方式。 syscall: 底层的外部包，提供了操作系统底层调用的基本接口。 archive/tar 和 /zip-compress：压缩（解压缩）文件功能。 fmt-io-bufio-path/filepath-flag: fmt: 提供了格式化输入输出功能。 io: 提供了基本输入输出功能，大多数是围绕系统功能的封装。 bufio: 缓冲输入输出功能的封装。 path/filepath: 用来操作在当前系统中的目标文件名路径。 flag: 对命令行参数的操作。 strings-strconv-unicode-regexp-bytes: strings: 提供对字符串的操作。 strconv: 提供将字符串转换为基础类型的功能。 unicode: 为 unicode 型的字符串提供特殊的功能。 regexp: 正则表达式功能。 bytes: 提供对字符型分片的操作。 index/suffixarray: 子字符串快速查询。 math-math/cmath-math/big-math/rand-sort: math: 基本的数学函数。 math/cmath: 对复数的操作。 math/rand: 伪随机数生成。 sort: 为数组排序和自定义集合。 math/big: 大数的实现和计算。 container-/list-ring-heap: 实现对集合的操作。 list: 双链表。 ring: 环形链表。 time-log: time: 日期和时间的基本操作。 log: 记录程序运行时产生的日志，我们将在后面的章节使用它。 encoding/json-encoding/xml-text/template: encoding/json: 读取并解码和写入并编码 JSON 数据。 encoding/xml: 简单的 XML1.0 解析器，有关 JSON 和 XML 的实例请查阅第 12.9/10 章节。 text/template:生成像 HTML 一样的数据与文本混合的数据驱动模板（参见第 15.7 节）。 net-net/http-html:（参见第 15 章） net: 网络数据的基本操作。 http: 提供了一个可扩展的 HTTP 服务器和基础客户端，解析 HTTP 请求和回复。 html: HTML5 解析器。 runtime: Go 程序运行时的交互操作，例如垃圾回收和协程创建。 reflect: 实现通过程序运行时反射，让程序操作任意类型的变量。 程序资源初始化 在一个代码包中，甚至一个源文件中，可以声明若干名为 init 的函数。 这些 init 函数必须不带任何输入参数和返回结果。在程序运行时刻，在进入 main 入口函数之前，每个 init 函数在此包加载的时候将被（串行）执行并且只执行一遍。 Go 语言程序的初始化和执行总是从 main.main 函数开始的。但是如果 main 包导入了其它的包，则会按照顺序将它们包含进 main 包里（可能是以文件名或包路径名的字符串顺序导入）。如果某个包被多次导入的话，在执行的时候只会导入一次。当一个包被导入时，如果它还导入了其它的包，则先将其它的包包含进来，然后创建和初始化这个包的常量和变量,再调用包里的 init 函数，如果一个包有多个 init 函数的话，调用顺序未定义（可能是以文件名的顺序调用），同一个文件内的多个 init 则是以出现的顺序依次调用。最后，当 main 包的所有包级常量、变量被创建和初始化完成，并且 init 函数被执行后，才会进入 main.main 函数，程序开始正常执行。下图是 Go 程序函数启动顺序的示意图： ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:4","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"函数、结构体、方法和接口 函数 在 Go 语言中，函数是第一类对象，我们可以将函数储存在变量中。函数主要有有名和匿名之分，包级函数一般都是有名函数。 // 有名函数 func Add(a, b int) int { return a + b } // 匿名函数 var Add = func(a, b int) int { return a + b } // 多个参数和多个返回值 func Swap(a, b int) (int, int) { return b, a } // 命名返回值 func ReturnErr1() (err error) { return } // 上面的函数与该函数等同 func ReturnErr2() error { var err error return err } // 可变数量的参数 // more 为 []int 切片类型 func Sum(a int, more ...int) int { for _, v := range more { a += v } return a } defer 将函数调用推迟到外层函数返回之前（执行 return 语句或者发生异常后） 下面这段代码使用 defer 实现代码执行追踪。 package main import \"fmt\" func trace(s string) { fmt.Println(\"entering:\", s) } func untrace(s string) { fmt.Println(\"leaving:\", s) } func a() { trace(\"a\") defer untrace(\"a\") fmt.Println(\"in a\") } func b() { trace(\"b\") defer untrace(\"b\") fmt.Println(\"in b\") a() } func main() { b() } 输出： entering: b in b entering: a in a leaving: a leaving: b 结构体 使用 type \u003cstruct_name\u003e struct {} 声明结构体 input: type student struct { name string grade int } func main() { var s student = student{name: \"SaltFish\", grade: 4} fmt.Printf(\"%v\\n\", s) } output: {SaltFish 4} %T 值类型的 Go 语法表示 main.student %v 值 {SaltFish 4} %+v 值，添加字段名 {name:SaltFish grade:4} %#v 值，Go 语法表示 main.student{name:\"SaltFish\", grade:4} 方法 Go 语言中每个类型（除了指针类型和接口类型）还可以有自己的方法，方法也是函数的一种。 // 文件对象 type File struct { fd int } // 读文件数据 func (f *File) Read(offset int64, data []byte) int { // ... } // 关闭文件 func (f *File) Close() error { // ... } func main() { f := new(File) f.Read() f.Close() } 接口 接口类型是 Go 中的一种很特别的类型。接口类型在 Go 中扮演着重要的角色。 首先，在 Go 中，接口值可以用来包裹非接口值；然后，通过值包裹，反射和多态得以实现。 一个接口类型定义了一个方法集。接口类型中指定的任何方法原型中的方法名称都不能为空标识符_。 // 定义 Aboutable 接口，方法集中只有 About() string 一个方法。 type Aboutable interface { About() string } // 定义 Book 结构体。 type Book struct { name string } // Book 拥有原型为 About() string 的方法，因此它实现了 Aboutable 接口。 func (b Book) About() string { return b.name } func main() { // *Book 类型的字面量值被包裹在 Aboutable 类型的变量 a 中。 var a Aboutable = \u0026Book{\"Go\"} fmt.Println(a.About()) // 调用接口的方法 // i是一个空接口值。任何类型都实现了空接口类型。 var i interface{} = \u0026Book{\"Go\"} fmt.Println(i) // Aboutable 实现了空接口类型 interface{}。 i = a fmt.Println(i) } ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:5","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"错误处理 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:6","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["golang"],"content":"协程与通道 ","date":"2021-12-02","objectID":"/posts/2021-12-02-go-intro/:5:7","tags":null,"title":"Go 语言入门","uri":"/posts/2021-12-02-go-intro/"},{"categories":["linux"],"content":"Linux 命令解释器（Shell）是有很多快捷键的，熟练掌握可以极大的提高操作效率。 下面列出最常用的快捷键，这还不是完全版。 命令行快捷键： 常用： Ctrl L ：清屏 Ctrl M ：等效于回车 Ctrl C : 中断正在当前正在执行的程序 历史命令： Ctrl P : 上一条命令，可以一直按表示一直往前翻 Ctrl N : 下一条命令 Ctrl R，再按历史命令中出现过的字符串：按字符串寻找历史命令（重度推荐） 命令行编辑： Tab : 自动补齐（重度推荐） Ctrl A ： 移动光标到命令行首 Ctrl E : 移动光标到命令行尾 Ctrl B : 光标后退 Ctrl F : 光标前进 Alt F : 光标前进一个单词 Alt B : 光标后退一格单词 Ctrl ] : 从当前光标往后搜索字符串，用于快速移动到该字符串 Ctrl Alt ] : 从当前光标往前搜索字符串，用于快速移动到该字符串 Ctrl H : 删除光标的前一个字符 Ctrl D : 删除当前光标所在字符 Ctrl K ：删除光标之后所有字符 Ctrl U : 清空当前键入的命令 Ctrl W : 删除光标前的单词(Word, 不包含空格的字符串) Ctrl \\ : 删除光标前的所有空白字符 Ctrl Y : 粘贴Ctrl W或Ctrl K删除的内容 Alt . : 粘贴上一条命令的最后一个参数（很有用） Alt [0-9] Alt . 粘贴上一条命令的第[0-9]个参数 Alt [0-9] Alt . Alt. 粘贴上上一条命令的第[0-9]个参数 Ctrl X Ctrl E : 调出系统默认编辑器编辑当前输入的命令，退出编辑器时，命令执行 其他： Ctrl Z : 把当前进程放到后台（之后可用’‘fg’‘命令回到前台） Shift Insert : 粘贴（相当于 Windows 的Ctrl V） 在命令行窗口选中即复制 在命令行窗口中键即粘贴，可用Shift Insert代替 Ctrl PageUp : 屏幕输出向上翻页 Ctrl PageDown : 屏幕输出向下翻页 ","date":"2021-12-01","objectID":"/posts/2021-12-01-linux-shortcut-keymap/:0:0","tags":null,"title":"Linux 常用快捷键","uri":"/posts/2021-12-01-linux-shortcut-keymap/"},{"categories":["technique"],"content":"初次运行 Git 前的配置 设置用户名和邮箱 git config --global user.name \"Salt Fish\" git config --global user.email saltfishpr@gmail.com 配置默认文本编辑器 git config --global core.editor code 检查配置信息 git config --list ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:1:0","tags":["git"],"title":"Git 常用命令","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"设置保存密码 # 记住密码（默认15分钟） git config --global credential.helper cache # 自己设置时间 git config --global credential.helper cache --timeout=3600 # 长期存储密码 git config --global credential.helper store 增加远程地址的时候带上密码 git remote -v # https://yourname:password@github.com/saltfishpr/go-learning.git ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:2:0","tags":["git"],"title":"Git 常用命令","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"Git clone/push 太慢 在国内，github 域名被限制，导致 git clone 很慢，只有 40KB/s 的速度 ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:3:0","tags":["git"],"title":"Git 常用命令","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"Windows 使用 nslookup 查询 github 对应的 IP 地址 nslookup github.global.ssl.fastly.net nslookup github.com 把查询到的结果添加到 C:\\Windows\\System32\\drivers\\etc\\hosts 中 github.global.ssl.fastly.net 69.63.184.14 github.com 140.82.112.3 刷新 DNS 缓存 ipconfig /flushdns ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:3:1","tags":["git"],"title":"Git 常用命令","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"linux 使用 nslookup 查询 github 对应的 IP 地址 nslookup github.global.ssl.fastly.net nslookup github.com 把查询到的结果添加到 /etc/hosts 中 github.global.ssl.fastly.net 69.63.184.14 github.com 140.82.112.3 刷新 DNS 缓存 sudo nscd -i hosts ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:3:2","tags":["git"],"title":"Git 常用命令","uri":"/posts/2021-11-25-git/"},{"categories":["technique"],"content":"清除远程分支的本地缓存 当修改/删除远程分支后，本地的远程分支缓存未被删除，再 checkout 会出现 remote ref does not exist 错误，这时要先清除本地缓存 git fetch -p origin ","date":"2021-11-25","objectID":"/posts/2021-11-25-git/:4:0","tags":["git"],"title":"Git 常用命令","uri":"/posts/2021-11-25-git/"},{"categories":["python"],"content":"Kaggle 上的 Pandas 入门教程","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/","tags":["data science"],"title":"Pandas 入门","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"读取数据集 import pandas as pd reviews = pd.read_csv(\"~/Programming/datasets/winemag-data-130k-v2.csv\") ","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:1:0","tags":["data science"],"title":"Pandas 入门","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"分组和排序 对分数分组求和 reviews.groupby(\"points\").points.count() Output: points 80 397 81 692 82 1836 ... 98 77 99 33 100 19 Name: points, dtype: int64 获得相同评分葡萄酒的最低价格 reviews.groupby(\"points\").price.min() Output: points 80 5.0 81 5.0 82 4.0 ... 98 50.0 99 44.0 100 80.0 Name: price, dtype: float64 从每个酒庄中选择第一瓶葡萄酒的名称 reviews.groupby('winery').apply(lambda df: df.title.iloc[0]) Output: winery 1+1=3 1+1=3 NV Rosé Sparkling (Cava) 10 Knots 10 Knots 2010 Viognier (Paso Robles) 100 Percent Wine 100 Percent Wine 2015 Moscato (California) 1000 Stories 1000 Stories 2013 Bourbon Barrel Aged Zinfande... 1070 Green 1070 Green 2011 Sauvignon Blanc (Rutherford) ... Órale Órale 2011 Cabronita Red (Santa Ynez Valley) Öko Öko 2013 Made With Organically Grown Grapes Ma... Ökonomierat Rebholz Ökonomierat Rebholz 2007 Von Rotliegenden Spät... àMaurice àMaurice 2013 Fred Estate Syrah (Walla Walla V... Štoka Štoka 2009 Izbrani Teran (Kras) Length: 16757, dtype: object 根据国家和省份挑选最好的葡萄酒 reviews.groupby([\"country\", \"province\"]).apply(lambda df:df.loc[df.points.idxmax()]) Output: Unnamed: 0 ... winery country province ... Argentina Mendoza Province 82754 ... Bodega Catena Zapata Other 78303 ... Colomé Armenia Armenia 66146 ... Van Ardi Australia Australia Other 37882 ... Marquis Philips New South Wales 85337 ... De Bortoli ... ... ... Uruguay Juanico 9133 ... Familia Deicas Montevideo 15750 ... Bouza Progreso 93103 ... Pisano San Jose 39898 ... Castillo Viejo Uruguay 39361 ... Narbona [425 rows x 14 columns] agg() 方法可以在一个 dataframe 上运行一些一维的函数 reviews.groupby(['country']).price.agg([len, min, max]) Output: len min max country Argentina 3800.0 4.0 230.0 Armenia 2.0 14.0 15.0 Australia 2329.0 5.0 850.0 ... US 54504.0 4.0 2013.0 Ukraine 14.0 6.0 13.0 Uruguay 109.0 10.0 130.0 多索引： 使用 groupby() 可能会产生多索引 countries_reviewed = reviews.groupby(['country', 'province']).description.agg([len]) countries_reviewed Output: len country province Argentina Mendoza Province 3264 Other 536 Armenia Armenia 2 Australia Australia Other 245 New South Wales 85 ... Uruguay Juanico 12 Montevideo 11 Progreso 11 San Jose 3 Uruguay 24 [425 rows x 1 columns] 多索引转回常规索引 countries_reviewed.reset_index() Output: country province len 0 Argentina Mendoza Province 3264 1 Argentina Other 536 2 Armenia Armenia 2 3 Australia Australia Other 245 4 Australia New South Wales 85 .. ... ... ... 420 Uruguay Juanico 12 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 423 Uruguay San Jose 3 424 Uruguay Uruguay 24 [425 rows x 3 columns] 使用 sort_values() 对数据排序（默认为升序排序） countries_reviewed = countries_reviewed.reset_index() countries_reviewed.sort_values(by='len') Output: country province len 179 Greece Muscat of Kefallonian 1 192 Greece Sterea Ellada 1 194 Greece Thraki 1 354 South Africa Paardeberg 1 40 Brazil Serra do Sudeste 1 .. ... ... ... 409 US Oregon 5373 227 Italy Tuscany 5897 118 France Bordeaux 5941 415 US Washington 8639 392 US California 36247 [425 rows x 3 columns] 降序排序 countries_reviewed.sort_values(by='len', ascending=False) Output: country province len 392 US California 36247 415 US Washington 8639 118 France Bordeaux 5941 227 Italy Tuscany 5897 409 US Oregon 5373 .. ... ... ... 101 Croatia Krk 1 247 New Zealand Gladstone 1 357 South Africa Piekenierskloof 1 63 Chile Coelemu 1 149 Greece Beotia 1 [425 rows x 3 columns] 重新按照索引排序 countries_reviewed.sort_index() Output: country province len 0 Argentina Mendoza Province 3264 1 Argentina Other 536 2 Armenia Armenia 2 3 Australia Australia Other 245 4 Australia New South Wales 85 .. ... ... ... 420 Uruguay Juanico 12 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 423 Uruguay San Jose 3 424 Uruguay Uruguay 24 [425 rows x 3 columns] 同时使用多个列排序 countries_reviewed.sort_values(by=['country', 'len']) Output: country province len 1 Argentina Other 536 0 Argentina Mendoza Province 3264 2 Armenia Armenia 2 6 Australia Tasmania 42 4 Australia New South Wales 85 .. ... ... ... 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 420 Uruguay Juanico 12 424 Uruguay Uru","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:2:0","tags":["data science"],"title":"Pandas 入门","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"数据类型和缺失值 获取 price 列的数据类型 reviews.price.dtype Output: dtype('float64') 查看 dataframe 中每一列的数据类型 reviews.dtypes Output: Unnamed: 0 int64 country object description object designation object points int64 price float64 province object region_1 object region_2 object taster_name object taster_twitter_handle object title object variety object winery object dtype: object 使用 astype() 转换数据类型 reviews.points.astype('float64') Output: 0 87.0 1 87.0 2 87.0 3 87.0 4 87.0 ... 129966 90.0 129967 90.0 129968 90.0 129969 90.0 129970 90.0 Name: points, Length: 129971, dtype: float64 缺少值的条目被赋予值 NaN，缩写为 “Not a Number” 查看 country 为 NaN 的行 reviews[pd.isnull(reviews.country)] Output: Unnamed: 0 country ... variety winery 913 913 NaN ... Chinuri Gotsa Family Wines 3131 3131 NaN ... Red Blend Barton \u0026 Guestier 4243 4243 NaN ... Ojaleshi Kakhetia Traditional Winemaking 9509 9509 NaN ... White Blend Tsililis 9750 9750 NaN ... Chardonnay Ross-idi ... ... ... ... ... 124176 124176 NaN ... Red Blend Les Frères Dutruy 129407 129407 NaN ... Cabernet Sauvignon El Capricho 129408 129408 NaN ... Tempranillo El Capricho 129590 129590 NaN ... Red Blend Büyülübağ 129900 129900 NaN ... Merlot Psagot [63 rows x 14 columns] 使用 fillna() 填充缺失的行 reviews.region_2.fillna(\"Unknown) Output: 0 Unknown 1 Unknown 2 Willamette Valley 3 Unknown 4 Willamette Valley ... 129966 Unknown 129967 Oregon Other 129968 Unknown 129969 Unknown 129970 Unknown Name: region_2, Length: 129971, dtype: object 使用 replace(old_val, new_val) 方法替换非空值 reviews.taster_twitter_handle.replace(\"@kerinokeefe\", \"@kerino\") Output: 0 @kerino 1 @vossroger 2 @paulgwine 3 NaN 4 @paulgwine ... 129966 NaN 129967 @paulgwine 129968 @vossroger 129969 @vossroger 129970 @vossroger Name: taster_twitter_handle, Length: 129971, dtype: object ","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:3:0","tags":["data science"],"title":"Pandas 入门","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"重命名和合并 重命名一列 reviews.rename(columns={'points': 'score'}) Output: Unnamed: 0 ... winery 0 0 ... Nicosia 1 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben Müller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] 重命名可以选择 index 或者 colum 参数 reviews.rename(index={0: 'firstEntry', 1: 'secondEntry'}) Output: Unnamed: 0 ... winery firstEntry 0 ... Nicosia secondEntry 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben Müller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] 因为很少重命名索引值，通常情况下 set_index() 更好用 给行索引和列索引添加名称属性 reviews.rename_axis(\"wines\", axis='rows').rename_axis(\"fields\", axis='columns') Output: fields Unnamed: 0 ... winery wines ... 0 0 ... Nicosia 1 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben Müller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] 组合有三个核心方法concat(), join() 和 merge() concat(): 给定一个元素列表，此函数将沿着一个 axis 将这些元素混合在一起 canadian_youtube british_youtube pd.concat([canadian_youtube, british_youtube]) Output: video_id ... description 0 n1WpP7iowLc ... Eminem's new track Walk on Water ft. Beyoncé i... 1 0dBIkQ4Mz1M ... STill got a lot of packages. Probably will las... 2 5qpjK5DgCt4 ... WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http... 3 d380meD0W0M ... I know it's been a while since we did this sho... 4 2Vv-BfVoq4g ... 🎧: https://ad.gt/yt-perfect\\n💰: https://atlant... ... ... ... 40876 sGolxsMSGfQ ... 🚨 NEW MERCH! http://amzn.to/annoyingorange 🚨➤ ... 40877 8HNuRNi8t70 ... ► Retrouvez vos programmes préférés : https://... 40878 GWlKEM3m2EE ... Find out more about Kingdom Hearts 3: https://... 40879 lbMKLzQ4cNQ ... Peter Navarro isn’t talking so tough now. Ana ... 40880 POTgw38-m58 ... 藝人：李妍瑾、玉兔、班傑、LaLa、小優、少少專家：陳筱屏(律師)、Wendy(心理師)、羅... [40881 rows x 16 columns] video_id ... description 0 Jw1Y-zhQURU ... Click here to continue the story and make your... 1 3s1rvMFUweQ ... Musical guest Taylor Swift performs …Ready for... 2 n1WpP7iowLc ... Eminem's new track Walk on Water ft. Beyoncé i... 3 PUTEiSjKwJU ... Salford drew 4-4 against the Class of 92 and F... 4 rHwDegptbI4 ... Dashcam captures truck's near miss with child ... ... ... ... 38911 l884wKofd54 ... NEW SONG - MOVE TO MIAMI feat. Pitbull (Click ... 38912 IP8k2xkhOdI ... THE OFFICIAL UP WITH IT MUSIC VIDEO!Get my new... 38913 Il-an3K9pjg ... Get 2002 by Anne-Marie HERE ▶ http://ad.gt/200... 38914 -DRsfNObKIQ ... Eleni Foureira represented Cyprus at the first... 38915 4YFo4bdMO8Q ... Debut album 'Light of Mine' out now: http://ky... [38916 rows x 16 columns] video_id ... description 0 n1WpP7iowLc ... Eminem's new track Walk on Water ft. Beyoncé i... 1 0dBIkQ4Mz1M ... STill got a lot of packages. Probably will las... 2 5qpjK5DgCt4 ... WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http... 3 d380meD0W0M ... I know it's been a while since we did this sho... 4 2Vv-BfVoq4g ... 🎧: https://ad.gt/yt-perfect\\n💰: https://atlant... ... ... ... 38911 l884wKofd54 ... NEW SONG - MOVE TO MIAMI feat. Pitbull (Click ... 38912 IP8k2xkhOdI ... THE OFFICIAL UP WITH IT MUSIC VIDEO!Get my new... 38913 Il-an3K9pjg ... Get 2002 by Anne-Marie HERE ▶ http://ad.gt/200... 38914 -DRsfNObKIQ ... Eleni Foureira represented Cyprus at the first... 38915 4YFo4bdMO8Q ... Debut album 'Light of Mine' out now: http://ky... [79797 rows x 16 columns] join(): 可以组合具有共同索引的不同 dataframe 对象 left = canadian_youtube.set_index(['title', 'trending_date']) right = british_youtube.set_index(['title', 't","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:4:0","tags":["data science"],"title":"Pandas 入门","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"其他 显示所有列 pd.set_option(\"display.max_columns\", None) ","date":"2020-05-25","objectID":"/posts/2020-05-25-pandas-kaggle/:5:0","tags":["data science"],"title":"Pandas 入门","uri":"/posts/2020-05-25-pandas-kaggle/"},{"categories":["python"],"content":"conda ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:1:0","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"换源 创建 .condarc 配置文件 conda config --set show_channel_urls yes 用文本编辑器打开 ~/.condarc 填入以下内容 channels: - defaults show_channel_urls: true channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:1:1","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"命令 创建有最新版本的 python 的环境 conda create -n \u003cenv-name\u003e python=3 删除环境 conda remove -n \u003cenv-name\u003e --all 清除缓存 conda clean -a ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:1:2","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"pip ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:2:0","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"换源 在 ~/.config/pip/pip.conf 中添加如下内容 [global] index-url = https://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:2:1","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"删除缓存 找到 ~/.cache/pip 文件夹，删除即可 在安装时使用 pip install \u003cpackage-name\u003e --no-cache-dir ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:2:2","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"导入导出 python 环境 今天在学习数据挖掘的时候，nolearn 和 lasagne 两个库的时候给我的 jypyterlab 环境搞崩了，只好 remove –all 重新配起，真后悔没有先搞个环境备份( ´•︵•` ) ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:3:0","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"conda conda 是个好东西，可以自己处理环境依赖，缺点就是。。包有点老，有些包还找不到 导入环境 conda create --name \u003cyour env name\u003e --file \u003cthis file\u003e --yes 导出环境 conda list -e \u003e requirements.txt ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:3:1","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["python"],"content":"pip pip 也有很多优点，我一般是在 conda 找不到模块的时候使用 pip 导入环境 pip install -r requirements.txt 导出环境 pip freeze \u003e requirements.txt ","date":"2020-03-26","objectID":"/posts/2020-03-26-python-environment-management/:3:2","tags":null,"title":"Python 环境管理","uri":"/posts/2020-03-26-python-environment-management/"},{"categories":["technique"],"content":"应朋友的强烈要求在这里记录下使用 github pages + jekyll 搭建个人网站的步骤。我自己花了两天梳理了一下 jekyll 模板的目录结构和使用方法，只会些简单的修改但是基本够用了（毕竟不是前端人）。 ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:0:0","tags":["jekyll"],"title":"Github Pages 个人博客搭建","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"准备 注册 github 账号 创建一个仓库，命名必须为\u003cusername\u003e.github.io，百度上任意搜索都有介绍过程，这里就不再赘述了 ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:1:0","tags":["jekyll"],"title":"Github Pages 个人博客搭建","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"windows: 安装 git，下载安装最新版本即可 安装 msys2 安装好 msys2 后，首先记得切换镜像源，不然会因为网速太慢心态崩溃。在设置镜像源的时候，可以将除了清华源的其他源全部注释掉，提高下载速度。 安装 ruby，这里下载 2.6.5 版本，因为最新版后面会提示版本不匹配。 下载过程可能很久…如果觉得慢可以下载不包含 devkit 的版本。 安装时在 Select Components 界面不用勾选 msys2，因为在上一步中已经安装好了。 在 Finish 时勾选 Run ‘ridk install’ to setup msys2… 弹出配置界面，在这里我选择 3 并按回车。由于配置过 msys2 的源，这里下载安装速度很快。 安装完成后，打开 msys2 命令行窗口，为 rubygems配置源。 安装rubygems，下载后解压缩，在文件夹中打开 git bash 输入 ruby setup.rb 安装 bundler：在 msys2 或者 git bash 命令行中输入gem install bundler，等待安装完成 再次使用清华源为 bundle 配置镜像源。 安装 jekyll：在命令行中输入gem install jekyll 至此准备工作全部完成～ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:1:1","tags":["jekyll"],"title":"Github Pages 个人博客搭建","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"linux(Debian 10.3) debian 系统自带大部分基础包 git: sudo apt-get install git 为 gem 配置镜像源，步骤与 Windows 一样 安装 bundler: gem install bundler 为 bundler 配置镜像源，同上 安装 jekyll: gem install jekyll 至此准备工作全部完成～ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:1:2","tags":["jekyll"],"title":"Github Pages 个人博客搭建","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"搭建 blog 使用 git clone 将自己的仓库 clone 下来。 如果有能力可以自己生成一个新的 jekyll 项目：新建一个空文件夹，在此文件夹中命令行输入jekyll new site-name，将这个文件夹的所有文件复制到 clone 下来的仓库中。 也可以在模板网站下载一个修改配置。我的github page使用的是 Hux 大佬的模板。 这里可以克隆我的模板 git clone https://github.com/SaltFishPr/saltfishpr.github.io.git 在项目文件夹中打开 git bash 输入bundle install，等待安装完成。如果安装出现依赖问题，可以在百度中搜索缺少的包看如何安装，都可以找到解决答案。 项目文件的编辑推荐使用 vscode。用 vscode 打开文件夹后，可以看到文件夹的目录结构: ├── 404.html ├── about.html ├── archive.html ├── CNAME ├── _config.yml ├── css ├── feed.xml ├── fonts ├── Gemfile ├── Gemfile.lock ├── Gruntfile.js ├── img ├── _includes ├── index.html ├── js ├── _layouts ├── less ├── LICENSE ├── offline.html ├── _posts ├── pwa ├── README.md ├── _site └── sw.js 这里主要修改配置文件_config.yml。 ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:2:0","tags":["jekyll"],"title":"Github Pages 个人博客搭建","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"运行本地服务 在项目根目录命令行中输入bundle exec jekyll server --watch，在本地启动 jekyll 服务器，浏览器输入 127.0.0.1:4000 本地查看 blog。 对照_config.yml 修改；善用 ctrl+shift+F 全局搜索。修改出属于自己的 blog 吧！ ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:3:0","tags":["jekyll"],"title":"Github Pages 个人博客搭建","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"写在最后 由于建立 blog 是在一个星期前了，可能有些小步骤记不清楚，有什么问题欢迎下方 issue 指正 O(∩_∩)O。 ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:4:0","tags":["jekyll"],"title":"Github Pages 个人博客搭建","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["technique"],"content":"issues 缺少 ruby.in : sudo apt-get install ruby-dev zlib is missing; necessary for building libxml2 : sudo apt-get install zlib1g zlib1g.dev 参考资料： jekyll 目录结构 github pages 文档 ","date":"2020-03-10","objectID":"/posts/2020-03-10-build-a-github-pages/:5:0","tags":["jekyll"],"title":"Github Pages 个人博客搭建","uri":"/posts/2020-03-10-build-a-github-pages/"},{"categories":["python"],"content":"《Python 数据挖掘入门与实践》学习笔记","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"书上的源码在官网上可以注册账号下载，这里只为记录自己的学习过程。 如果有侵权情况，请给我发邮件通知我删除 526191197@qq.com 此笔记的代码均在 pycharm - python3.8 中运行通过 学习数据挖掘，让数据服务于人类 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:0:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第一章 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:1:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"亲和性分析 亲和性分析根据样本个体（物体）之间的相似度，确定他们的关系亲疏。应用场景有以下几个方面： 向用户投放定向广告 为用户提供推荐（如歌曲推荐，电影推荐等） 名词： 规则：一条规则由前提条件和结论两部分组成 支持度：数据集中规则应验的次数 置信度：规则（结果）出现的次数 / 条件出现的次数（条件相同的规则数量），衡量规则的准确率 # -*- coding: utf-8 -*- import numpy as np from collections import defaultdict from operator import itemgetter if __name__ == '__main__': dataset_filename = \"affinity_dataset.txt\" X = np.loadtxt(dataset_filename) n_samples, n_features = X.shape # 样本数，特征数 features = [\"bread\", \"milk\", \"cheese\", \"apples\", \"bananas\"] # 商品名列表 # 如果xxx，那么xxx 就是一条规则。规则由前提条件和结论两部分组成 # 这里注意'如果买A则他们会买B'和'如果买B则他们会买A'不是一个规则，在下面的循环中体现出来 valid_rules = defaultdict(int) # 规则应验 invalid_rules = defaultdict(int) # 规则无效 num_occurences = defaultdict(int) # 商品购买数量字典 for sample in X: # 对数据集里的每个消费者 for premise in range(n_features): if sample[premise] == 0: # 如果这个商品没有买，继续看下一个商品 continue num_occurences[premise] += 1 # 记录这个商品购买数量 for conclusion in range(n_features): if premise == conclusion: # 跳过此商品 continue if sample[conclusion] == 1: valid_rules[(premise, conclusion)] += 1 # 规则应验 else: invalid_rules[(premise, conclusion)] += 1 # 规则无效 support = valid_rules # 支持度字典，即规则应验次数 confidence = defaultdict(float) # 置信度字典 for premise, conclusion in valid_rules.keys(): # 条件/结论 rule = (premise, conclusion) # 置信度 = 规则发生的次数/条件发生的次数 confidence[rule] = valid_rules[rule] / num_occurences[premise] def print_rule(premise, conclusion, support, confidence, features): premise_name = features[premise] conclusion_name = features[conclusion] print( \"Rule: If a person buys {0}they will also buy {1}\".format( premise_name, conclusion_name)) print( \" - Confidence: {0:.3f}\".format(confidence[(premise, conclusion)])) print(\" - Support: {0}\".format(support[(premise, conclusion)])) print(\"\") # 得到支持度最高的规则，items()返回字典所有元素的列表，itemgetter(1)表示用支持度的值作为键，进行降序排列 sorted_support = sorted(support.items(), key=itemgetter(1), reverse=True) for i in range(5): print(\"Rule #{0}\".format(i + 1)) premise, conclusion = sorted_support[i][0] print_rule(premise, conclusion, support, confidence, features) sorted_confidence = sorted(confidence.items(), key=itemgetter(1), reverse=True) for i in range(5): print(\"Rule #{0}\".format(i + 1)) premise, conclusion = sorted_confidence[i][0] print_rule(premise, conclusion, support, confidence, features) Output： Rule #1 Rule: If a person buys cheese they will also buy bananas - Confidence: 0.659 - Support: 27 Rule #2 Rule: If a person buys bananas they will also buy cheese - Confidence: 0.458 - Support: 27 Rule #3 Rule: If a person buys cheese they will also buy apples - Confidence: 0.610 - Support: 25 Rule #1 Rule: If a person buys apples they will also buy cheese - Confidence: 0.694 - Support: 25 Rule #2 Rule: If a person buys cheese they will also buy bananas - Confidence: 0.659 - Support: 27 Rule #3 Rule: If a person buys bread they will also buy bananas - Confidence: 0.630 - Support: 17 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:1:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"One Rule 算法 OneR(One Rule)算法根据已有的数据中，具有相同特征值的个体最可能属于哪个类别进行分类。One Rule 就是从四个特征中选择分类效果最好的哪个作为分类依据。 假如数据集的某一个特征可以取 0 或 1 两个值。数据集共有三个类别。特征值为 0 的情况下，A 类有 20 个这样的个体，B 类有 60 个，C 类也有 20 个。那么特征值为 0 的个体最可能属于 B 类,当然还有 40 个个体确实是特征值为 0，但是它们不属于 B 类。将特征值为 0 的个体分到 B 类的错误率就是 40%，因为有 40 个这样的个体分别属于 A 类和 C 类。特征值为 1 时，计算方法类似，不再赘述；其他各特征值最可能属于的类别及错误率的计算方法也一样。 # -*- coding: utf-8 -*- import numpy as np from sklearn.datasets import load_iris # Iris植物分类数据集 from collections import defaultdict # 初始化数据字典 from operator import itemgetter # 得到一个列表的制定元素 from sklearn.model_selection import train_test_split # 将一个数据集且分为训练集和测试集 from sklearn.metrics import classification_report # 分析预测结果 # 这里保留函数的文档方便查阅 def train(X, y_true, feature): \"\"\" Computes the predictors and error for a given feature using the OneR algorithm Parameters ---------- X: array [n_samples, n_features] The two dimensional array that holds the dataset. Each row is a sample, each column is a feature. y_true: array [n_samples,] The one dimensional array that holds the class values. Corresponds to X, such that y_true[i] is the class value for sample X[i]. feature: int An integer corresponding to the index of the variable we wish to test. 0 \u003c= variable \u003c n_features Returns ------- predictors: dictionary of tuples: (value, prediction) For each item in the array, if the variable has a given value, make the given prediction. error: float The ratio of training data that this rule incorrectly predicts. \"\"\" # 检查是否为有效数字 n_samples, n_features = X.shape assert 0 \u003c= feature \u003c n_features # X[:, feature]为numpy矩阵的索引用法，第一维：所有数组，第二维：feature，set去重得到value有几个取值 # 这个feature特征值在每个数据中有多少个取值 values = set(X[:, feature]) # Stores the predictors array that is returned predictors = dict() errors = [] # 对每个特征值的每个取值调用train_feature_value函数获得该取值出现最多的类和错误率 for current_value in values: most_frequent_class, error = train_feature_value( X, y_true, feature, current_value) predictors[current_value] = most_frequent_class # 该取值出现最多的类 errors.append(error) # 存储错误率 total_error = sum(errors) # 返回预测方案（即feature的取值分别对应哪个类别）和总错误率 return predictors, total_error def train_feature_value(X, y_true, feature, value): class_counts = defaultdict(int) # Iterate through each sample and count the frequency of each class/value pair # 第feature个特征的值为value的时候，在每个种类中出现的次数，这里的植物有三个种类 # 因此最终class_counts有三个键值对 for sample, y in zip(X, y_true): if sample[feature] == value: class_counts[y] += 1 # 对class_count以value由大到小排列 sorted_class_counts = sorted( class_counts.items(), key=itemgetter(1), reverse=True) most_frequent_class = sorted_class_counts[0][0] # 出现最多次的类 n_samples = X.shape[1] error = sum([class_count for class_value, class_count in class_counts.items( ) if class_value != most_frequent_class]) # error就是除去上面那个类的其它value的和 return most_frequent_class, error # 返回出现次数最多的类和错误率 def predict(X_test, model): variable = model['variable'] # 使用哪个feature作为OneRule进行预测 predictor = model['predictor'] # 一个字典，保存着feature取值对应哪一类 y_predicted = np.array([predictor[int(sample[variable])] for sample in X_test]) return y_predicted # 返回预测结果 if __name__ == '__main__': dataset = load_iris() X = dataset.data y = dataset.target n_samples, n_features = X.shape # 计算每个属性的均值 attribute_means = X.mean(axis=0) assert attribute_means.shape == (n_features,) # 对数据集离散化 X_d = np.array(X \u003e= attribute_means, dtype='int') random_state = 14 X_train, X_test, y_train, y_test = train_test_split( X_d, y, random_state=random_state) # 分割训练集和测试集 print(\"There are {}training samples\".format(y_train.shape)) # 训练集数量 print(\"There are {}testing samples\".format(y_test.shape)) # 测试集数量 # 对每个特征返回预测器和错误率[0：{0: x, 1: x}, sum_error， ...] all_predictors = { variable: train( X_train, y_train, variable) for variable in range( X_train.shape[1])} errors = {variable: error for variable, (mapping, error) in all_predictors.items()} # 把每个预测器的值提取出来 # 找出最好（错误最少）的那个feature构成的预测器 best_variable, best_error = sorted(errors.items(), key=itemgetter(1))[0] print( \"The best model is based on variable {0}and has error {1:.2f}%\".format( best_variable","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:1:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第二章 主要学习数据挖掘通用框架的搭建方法 估计器(Estimator)：用于分类、聚类和回归分析 转换器(Transformer)：用于数据预处理和数据转换 流水线(Pipline)：组合数据挖掘流程，便于再次使用 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:2:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"scikit-learn 估计器 估计器用于分类，主要包含下面两个函数： fit(): 训练算法，设置内部参数。该函数接受训练集和类别两个参数 predict(): 参数为测试集。预测测试集类别，返回一个包含测试集各条数据类别的数组 近邻算法 用途广泛 计算量很大 距离度量 欧氏距离：即真实距离 曼哈顿距离：两个特征在标准坐标系中绝对轴距之和(x1,y1),(x2,y2)即 abs(x1-x2)+abs(y1-y2) 余弦距离：指的是特征向量夹角的余弦值，更适合解决异常值和数据稀疏问题。 电离层(Ionosphere)数据集分析 Input: # -*- coding: utf-8 -*- import numpy as np import csv from matplotlib import pyplot as plt from sklearn.neighbors import KNeighborsClassifier # 导入K近邻分类器 from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score # 导入交叉检验的 # 把每个特征值的值域规范化到0，1之间，最小值用0代替，最大值用1代替 from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline # 流水线 if __name__ == '__main__': # 数据集大小已知有351行，每行35个值前34个为天线采集的数据，最后一个 g/b 表示数据的好坏 X = np.zeros((351, 34), dtype='float') y = np.zeros((351,), dtype='bool') # 打开根目录的数据集文件 with open(\"ionosphere.data\", 'r', encoding='utf-8') as input_file: # 创建csv阅读器对象 reader = csv.reader(input_file) # 使用枚举函数为每行数据创建索引 for i, row in enumerate(reader): # 获取行数据的前34个值，并将其转化为浮点型，保存在X中 data = [float(datum) for datum in row[:-1]] # Set the appropriate row in our dataset X[i] = data # 数据集 # 1 if the class is 'g', 0 otherwise y[i] = row[-1] == 'g' # 类别 # 创建训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=14) print( \"There are {}samples in the training dataset\".format( X_train.shape[0])) print( \"There are {}samples in the testing dataset\".format( X_test.shape[0])) print(\"Each sample has {}features\".format(X_train.shape[1])) Output: There are 263 samples in the training dataset There are 88 samples in the testing dataset Each sample has 34 features Input: # 初始化一个K近邻分类器实例，该算法默认选择5个近邻作为分类依据 estimator = KNeighborsClassifier() # 用训练数据进行训练 estimator.fit(X_train, y_train) # 使用测试集测试算法，评价其表现 y_predicted = estimator.predict(X_test) # 准确性 accuracy = np.mean(y_test == y_predicted) * 100 print(\"The accuracy is {0:.1f}%\".format(accuracy)) # 使用交叉检验的方式获得平均准确性 scores = cross_val_score(estimator, X, y, scoring='accuracy') average_accuracy = np.mean(scores) * 100 print(\"The average accuracy is {0:.1f}%\".format(average_accuracy)) Output: The accuracy is 86.4% The average accuracy is 82.6% Input: # 设置参数 # 参数的选取跟数据集的特征息息相关 avg_scores = [] all_scores = [] parameter_values = list(range(1, 21)) for n_neighbors in parameter_values: estimator = KNeighborsClassifier(n_neighbors=n_neighbors) scores = cross_val_score(estimator, X, y, scoring='accuracy') avg_scores.append(np.mean(scores)) all_scores.append(scores) # 作出n_neighbors不同取值和分类正确率之间的关系的折线图 plt.figure(figsize=(32, 20)) plt.plot(parameter_values, avg_scores, '-o', linewidth=5, markersize=24) plt.show() Output: 经过上面的例子，可以总结数据挖掘最简单基本的流程如下： 载入数据集，数据分类提取到内存中 创建训练集和测试集 选择合适的算法进行训练 使用测试集测试算法，评估其表现 为了保证算法的准确性，可以将大数据集分为几个部分，通过交叉检验的方法测试算法。使用 cross_val_score 函数是一个不错的选择。 在参数的设置上，可以针对不同的参数进行交叉测试，使用图表直观地表示出参数的影响。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:2:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"流水线在预处理中的作用 sckit-learn 的预处理工具叫做转换器Transformer Input: # 模拟脏数据 X_broken = np.array(X) X_broken[:, ::2] /= 10 # 对比两种情况下预测准确率 estimator = KNeighborsClassifier() original_scores = cross_val_score(estimator, X, y, scoring='accuracy') print( \"The original average accuracy for is {0:.1f}%\".format( np.mean(original_scores) * 100)) broken_scores = cross_val_score(estimator, X_broken, y, scoring='accuracy') print( \"The broken average accuracy for is {0:.1f}%\".format( np.mean(broken_scores) * 100)) Output: The original average accuracy for is 82.6% The broken average accuracy for is 73.8% Input: # 组合成为一个工作流 X_transformed = MinMaxScaler.fit_transform(X_broken) # 完成训练和转换 estimator = KNeighborsClassifier() transformed_scores = cross_val_score( estimator, X_transformed, y, scoring='accuracy') print(\"The average accuracy for is {0:.1f}%\".format( np.mean(transformed_scores) * 100)) Output: The average accuracy for is 82.9% 将数据经过规范化后，正确率再次提高 其它的规范化函数举例： 为使每条数据各特征值的和为 1：sklearn.preprocessing.Normalizer 为使各特征值的均值为 0，方差为 1：sklearn.preprocessing.StandardScaler 为将数值型特征二值化：sklearn.preprocessing.Binarizer ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:2:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"流水线 sklearn.pipeline.Pipeline用于创建流水线。流水线的输入为一连串的数据挖掘步骤，最后一步必须是估计器，前几步是转换器。 Input: # 创建流水线 # 流水线的每一步都用('名称',步骤)的元组表示 scaling_pipeline = Pipeline([('scale', MinMaxScaler()), # 规范特征取值 ('predict', KNeighborsClassifier())]) # 预测 # 调用流水线 scores = cross_val_score(scaling_pipeline, X_broken, y, scoring='accuracy') print( \"The pipelin scored an average accuracy for is {0:.1f}%\".format( np.mean(scores) * 100)) Output: The pipelin scored an average accuracy for is 82.9% ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:2:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第三章 决策树也是一种分类算法，它的优点如下： 机器和人都能看懂 能够处理多种不同的特征 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"加载数据集 pandas(Python Data Analysis 的简写) 逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号），其文件以纯文本形式存储表格数据（数字和文本），来源百度百科。 这里使用 pandas 导入.csv 文件，生成一个 dataframe （数据框）的类。导入使用 read_csv() 函数，常用参数如下： sep=',' 以,为数据分隔符 parse_dates='col_name' 将某个特征值读取为日期格式 error_bad_lines=False 当某行数据有问题时，跳过而不报错 skiprows=[\u003cparam\u003e] 跳过列表中所包括的行，参数可以是 0,1,…的数字序列，也可以用切片表达式[0:] usecols=[\u003cparam\u003e] 选择使用哪几个特征值，参数同上 在使用 dataframe.ix[]获取 dataframe 中的某几行数据时，提示错误信息，原因是 pandas 在 0.20.0 版本后就废弃掉了这个函数。在这里我改为使用 iloc 函数。 Input: # -*- coding: utf-8 -*- import numpy as np import pandas as pd from collections import defaultdict from sklearn.tree import DecisionTreeClassifier # 创建决策树的类 from sklearn.model_selection import cross_val_score from sklearn.preprocessing import LabelEncoder # 能将字符串类型的特征转化成整型 from sklearn.preprocessing import OneHotEncoder # 将特征转化为二进制数字 from sklearn.ensemble import RandomForestClassifier # 随机森林 from sklearn.model_selection import GridSearchCV # 网格搜索，找到最佳参数 if __name__ == '__main__': # 清洗数据集 results = pd.read_csv( \"NBA_data.csv\", parse_dates=[\"Date\"], skiprows=[ 0, ], usecols=[ 0, 2, 3, 4, 5, 6, 7, 9]) # 加载数据集 # 修复数据特征名 results.columns = [ \"Date\", \"Visitor Team\", \"VisitorPts\", \"Home Team\", \"HomePts\", \"Score Type\", \"OT?\", \"Notes\"] # results.ix[]已被弃用 print(results.loc[:5]) # 查看数据集前五行 Output: Date Visitor Team VisitorPts ... Score Type OT? Notes 0 2013-10-29 Orlando Magic 87 ... Box Score NaN NaN 1 2013-10-29 Chicago Bulls 95 ... Box Score NaN NaN 2 2013-10-29 Los Angeles Clippers 103 ... Box Score NaN NaN 3 2013-10-30 Brooklyn Nets 94 ... Box Score NaN NaN 4 2013-10-30 Boston Celtics 87 ... Box Score NaN NaN 5 2013-10-30 Miami Heat 110 ... Box Score NaN NaN [6 rows x 8 columns] ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"决策树 创建新的特征列，可以从数据集中导入： dataset[\"New Feature\"] = feature_creator() 也可以一开始为新特征值设置默认的值： dataset[\"My New Feature\"] = 0 这里的 X_previouswins = results[[\"HomeLastWin\", \"VisitorLastWin\"]].values 生成一个数据集，这个数据集有两个特征 DecisionTreeClassifier() 用来创建决策树，常用参数如下： min_samples_split: 指定了创建一个新节点至少需要多少个个体 min_samples_leaf: 指定为了保留节点，每个节点至少应该包含的个体数量 创建决策的标准: 基尼不纯度/信息增益 Input: # 提取新特征，值为这场中主场队伍是否胜利 results[\"HomeWin\"] = results[\"VisitorPts\"] \u003c results[\"HomePts\"] y_true = results[\"HomeWin\"].values # 胜负情况 # 创建两个新feature，初始值都设为0，保存这场比赛的两个队伍上场比赛的情况 results[\"HomeLastWin\"] = False results[\"VisitorLastWin\"] = False won_last = defaultdict(int) for index, row in results.iterrows(): home_team = row[\"Home Team\"] visitor_team = row[\"Visitor Team\"] # 这场比赛之前两个球队上次是否获胜保存在result中 row[\"HomeLastWin\"] = won_last[home_team] row[\"VisitorLastWin\"] = won_last[visitor_team] results.iloc[index] = row # 这场比赛的结果更新won_last中的情况 won_last[home_team] = row[\"HomeWin\"] won_last[visitor_team] = not row[\"HomeWin\"] X_previouswins = results[[\"HomeLastWin\", \"VisitorLastWin\"]].values # 创建决策树生成器实例 clf = DecisionTreeClassifier(random_state=14) # 交叉训练 scores = cross_val_score(clf, X_previouswins, y_true, scoring='accuracy') print(\"Using just the last result from the home and visitor teams\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: Using just the last result from the home and visitor teams Accuracy: 56.4% 这里为了创建一个新的特征导入了上一年的 NBA 排名。 Input: ladder = pd.read_csv(\"NBA_standings.csv\", skiprows=[0, ]) # 创建一个新特征，两个队伍在上个赛季的排名哪个比较高 results[\"HomeTeamRanksHigher\"] = 0 for index, row in results.iterrows(): home_team = row[\"Home Team\"] visitor_team = row[\"Visitor Team\"] # 这个球队改名了 if home_team == \"New Orleans Pelicans\": home_team = \"New Orleans Hornets\" elif visitor_team == \"New Orleans Pelicans\": visitor_team = \"New Orleans Hornets\" # 这里源代码无法运行，少加了一个括号 ladder[(ladder[\"Team\"] == home_team)] 表示根据条件获取这一行的数据 home_row = ladder[(ladder[\"Team\"] == home_team)] visitor_row = ladder[(ladder[\"Team\"] == visitor_team)] home_rank = home_row[\"Rk\"].values[0] visitor_rank = visitor_row[\"Rk\"].values[0] row[\"HomeTeamRanksHigher\"] = int(home_rank \u003e visitor_rank) results.iloc[index] = row X_homehigher = results[[\"HomeLastWin\", \"VisitorLastWin\", \"HomeTeamRanksHigher\"]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_homehigher, y_true, scoring='accuracy') print(\"Using whether the home team is ranked higher\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: Using whether the home team is ranked higher Accuracy: 60.0% Input: # 创建新特征，两个队伍上一次进行比赛时的获胜者 last_match_winner = defaultdict(int) results[\"HomeTeamWonLast\"] = 0 for index, row in results.iterrows(): home_team = row[\"Home Team\"] visitor_team = row[\"Visitor Team\"] # 按照英文字母表排序，不去考虑哪个是主场球队 teams = tuple(sorted([home_team, visitor_team])) # 找到两支球队上次比赛的赢家，更新框中的数据，初始为0 # 这里的HomeTeamWonLast跟主场客场没有什么关系，也可以叫WhichTeamWonLast，这里为了和源码尽量保持一致使用了源码 row[\"HomeTeamWonLast\"] = 1 if last_match_winner[teams] == row[\"Home Team\"] else 0 results.iloc[index] = row winner = row[\"Home Team\"] if row[\"HomeWin\"] else row[\"Visitor Team\"] # 将两个球队上次遇见比赛的情况存到字典中去 last_match_winner[teams] = winner X_home_higher = results[[\"HomeTeamRanksHigher\", \"HomeTeamWonLast\"]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_home_higher, y_true, scoring='accuracy') print(\"Using whether the home team is ranked higher\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: Using whether the home team is ranked higher Accuracy: 59.9% ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"随机森林 LabelEncoder() 用来将一个字符串型的特征转化为整型 OneHotEncoder() 将整数转化成消除差异的二进制数字，即将 1,2,3 转换成 001,010,100 stacking （向量组合），这里 np.vstack() 将两个队伍名向量纵向组合成一个矩阵.T表示将矩阵转置 决策树存在的问题： 创建的多颗决策树在很大程度上是相同的，训练集相同，则生成的决策树也相同。一个解决办法是装袋(bagging) 用于前几个决策节点的特征非常突出，即使采用不同的训练集，创建的决策树相似性依旧很大。解决办法是随机选取部分特征作为决策数据 RandomForestClassifier() 用来调用随机森林算法，因为它调用了 DecisionTreeClassifier 的大量实例，所以他们的参数有很多是一致的。其引入的一部分新参数如下： n_estimators 用来指定创建决策树的数量，值越高，耗时越长，准确率(可能)越高 oob_score 如果设置为真，测试时将不适用训练模型时用过的数据 n_jobs 采用并行算法训练时所用到的内核数量，设置为 -1 则启用全部内核 Input: # 创建一个转化器实例 encoding = LabelEncoder() # 将球队名转化为整型 encoding.fit(results[\"Home Team\"].values) # 抽取所有比赛中主客场球队的球队名，组合起来形成一个矩阵 home_teams = encoding.transform(results[\"Home Team\"].values) visitor_teams = encoding.transform(results[\"Visitor Team\"].values) # 建立训练集，[[\"Home Team Feature\"，\"Visitor Team Feature\"],[\"Home Team Feature\"，\"Visitor Team Feature\"]...] X_teams = np.vstack([home_teams, visitor_teams]).T # 创建转化器实例 onehot = OneHotEncoder() # 生成转化后的特征 X_teams = onehot.fit_transform(X_teams).todense() clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_teams, y_true, scoring='accuracy') print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_teams, y_true, scoring='accuracy') print(\"Using full team labels is ranked higher\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: Accuracy: 60.5% Using full team labels is ranked higher Accuracy: 61.4% 将上面生成的特征整合起来，创建新的决策方案 这里使用 np.hstack()横向拼接两个决策方案矩阵 Input: X_all = np.hstack([X_home_higher, X_teams]) # 将上面计算的特征进行组合 print(X_all.shape) scores = cross_val_score(clf, X_all, y_true, scoring='accuracy') print(\"Using whether the home team is ranked higher\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) Output: (1319, 62) Using whether the home team is ranked higher Accuracy: 61.6% 使用 GridSearchCV （网格搜索）搜索最佳参数 Input: # 设置参数搜索范围 parameter_space = { \"max_features\": [2, 10, 'auto'], \"n_estimators\": [100, ], \"criterion\": [\"gini\", \"entropy\"], \"min_samples_leaf\": [2, 4, 6], } grid = GridSearchCV(clf, parameter_space) grid.fit(X_all, y_true) print(\"Accuracy: {0:.1f}%\".format(grid.best_score_ * 100)) # 输出最佳方案 print(grid.best_estimator_) Output: Accuracy: 65.6% RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=14, verbose=0, warm_start=False) ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"课后练习 拿到了数据，如何创建新的特征，如何在数据中发现其关键点，如何找出数据内部的联系，也是一个需要斟酌的方面 创建下述特征并看一下效果: 球队上次打比赛距今有多长时间？ 两支球队过去五场比赛结果如何？ 球队是不是跟某支特定球队打比赛时发挥更好？ 在这里使用了上面书中的方法，完成了前两个点，第三个点实现起来有点麻烦，现在只有一个思路：建立一个字典，数据形式为 (两支球队建立一个元组:(前一个队伍获胜的次数，后一个队伍获胜的次数)) 在处理 dataset 中的数据项时，对于 pandas 中的 Timestamp 类型没有了解，耗费了太长时间，查阅文档后发现可以用 date() 将其转化为 datetime.date 日期。 使用前两个特征作为决策标准时，效果还算可以，加上书上的所有特征后，准确率反而较上面的结果降低了。（不知道为什么） 这个“课后练习”使我对于标准库了解匮乏的短板显现出来，要抽出时间学习一下 python, numpy 和 pandas 标准库中常用函数及其参数。 Input: #!/usr/bin/env python3 # -*- coding: utf-8 -*- import datetime import numpy as np import pandas as pd from collections import defaultdict from sklearn.model_selection import cross_val_score from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from ch3.nba_test import X_all from sklearn.model_selection import GridSearchCV # 网格搜索，找到最佳参数 if __name__ == '__main__': \"\"\" - 球队上次打比赛距今有多长时间？ - 两支球队过去五场比赛结果如何？ - 球队是不是跟某支特定球队打比赛时发挥更好？ \"\"\" dataset = pd.read_csv( \"NBA_data.csv\", parse_dates=[\"Date\"], skiprows=[ 0, ], usecols=[ 0, 2, 3, 4, 5, 6, 7, 9]) # 加载数据集 dataset.columns = [ \"Date\", \"Visitor Team\", \"VisitorPts\", \"Home Team\", \"HomePts\", \"Score Type\", \"OT?\", \"Notes\"] dataset[\"HomeWin\"] = dataset[\"VisitorPts\"] \u003c dataset[\"HomePts\"] y_true = dataset[\"HomeWin\"].values # 胜负情况 # 保存上次打比赛的时间 last_played_date = defaultdict(datetime.date) # 手动为每个球队初始化 for team in set(dataset[\"Home Team\"]): last_played_date[team] = datetime.date(year=2013, month=10, day=25) # 两支球队过去的比赛结果，每个球队的数据是[True,False,,,]的序列 last_five_games = defaultdict(list) # 存放Home和Visitor前五次比赛的获胜次数 dataset[\"HWinTimes\"] = 0 dataset[\"VWinTimes\"] = 0 # 存放距离上次比赛的时间间隔，用天计数 dataset[\"HLastPlayedSpan\"] = 0 dataset[\"VLastPlayedSpan\"] = 0 for index, row in dataset.iterrows(): home_team = row[\"Home Team\"] visitor_team = row[\"Visitor Team\"] row[\"HWinTimes\"] = sum(last_five_games[home_team][-5:]) row[\"VWinTimes\"] = sum(last_five_games[visitor_team][-5:]) row[\"HLastPlayedSpan\"] = ( row[\"Date\"].date() - last_played_date[home_team]).days row[\"VLastPlayedSpan\"] = ( row[\"Date\"].date() - last_played_date[visitor_team]).days dataset.iloc[index] = row last_played_date[home_team] = row[\"Date\"].date() last_played_date[visitor_team] = row[\"Date\"].date() last_five_games[home_team].append(row[\"HomeWin\"]) last_five_games[visitor_team].append(not row[\"HomeWin\"]) X_1 = dataset[[\"HLastPlayedSpan\", \"VLastPlayedSpan\", \"HWinTimes\", \"VWinTimes\"]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_1, y_true, scoring='accuracy') print(\"DecisionTree: Using time span and win times\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_1, y_true, scoring='accuracy') print(\"RandomForest: Using time span and win times\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) print(\"---------------------------------\") X_all = np.hstack([X_1, X_all]) clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_all, y_true, scoring='accuracy') print(\"DecisionTree: Using time span and win times\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_all, y_true, scoring='accuracy') print(\"RandomForest: Using time span and win times\") print(\"Accuracy: {0:.1f}%\".format(np.mean(scores) * 100)) print(\"---------------------------------\") parameter_space = { \"max_features\": [2, 10, 'auto'], \"n_estimators\": [100, ], \"criterion\": [\"gini\", \"entropy\"], \"min_samples_leaf\": [2, 4, 6], } grid = GridSearchCV(clf, parameter_space) grid.fit(X_all, y_true) print(\"Accuracy: {0:.1f}%\".format(grid.best_score_ * 100)) print(grid.best_estimator_) Output: DecisionTree: Using time span and win times Accuracy: 56.4% RandomForest: Using time span and win times Accuracy: 58.3% --------------------------------- DecisionTree: Using time span and win times Accuracy: 57.2% RandomForest: Using time span and win times Accuracy: 61.0% -","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:3:4","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第四章 本章重点： 亲和性分析 用 Apriori 算法挖掘关联特征 数据稀疏问题 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"亲和性分析 Apriori 算法是经典的亲和性分析算法，它只从数据集中频繁出现的商品中选取出共同出现的商品组成频繁项集，避免了复杂度呈指数级增长的问题。一旦找到频繁项集，生成关联规则就变得容易了。 原理：确保了规则在数据集中有足够的支持度。Apriori 算法一个重要参数就是最小支持度，如果想要生成(A,B,C)的频繁项集，则其子集必须都要满足最小支持度标准。 其它亲和性算法还有 Eclat 和频繁项集挖掘算法(FP-growth)。这些算法比起基础的 Apriori 算法有很多改进，性能也有进一步提升。 第一阶段，为 Apriori 算法指定一个项集要成为频繁项集所需的最小支持度。第二阶段，根据置信度取关联规则，设定最小置信度，返回大于此值的规则。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"电影推荐问题 下载并加载数据集 Input: import sys import pandas as pd from collections import defaultdict from operator import itemgetter if __name__ == '__main__': # header=None 不把第一行当做表头 all_ratings = pd.read_csv( \"ml-100k/u.data\", delimiter=\"\\t\", header=None, names=[ \"UserID\", \"MovieID\", \"Rating\", \"Datetime\"]) # 转化时间戳为datetime all_ratings[\"Datetime\"] = pd.to_datetime(all_ratings[\"Datetime\"], unit='s') # 输出用户-电影-评分稀疏矩阵 print(all_ratings[:5]) print() # 创建Favorite特征，将评分属性二值化为是否喜欢 all_ratings[\"Favorable\"] = all_ratings[\"Rating\"] \u003e 3 # 取用户ID为前200的用户的打分数据 ratings = all_ratings[all_ratings[\"UserID\"].isin(range(200))] favorable_ratings = ratings[ratings[\"Favorable\"]] # 创建用户喜欢哪些电影的字典 favorable_reviews_by_users = dict( (k, frozenset( v.values)) for k, v in favorable_ratings.groupby(\"UserID\")[\"MovieID\"]) # 创建一个数据框，了解每部电影的影迷数量 num_favorable_by_movie = ratings[[ \"MovieID\", \"Favorable\"]].groupby(\"MovieID\").sum() # 查看最受欢迎的五部电影 print(num_favorable_by_movie.sort_values(\"Favorable\", ascending=False)[:5]) Output: UserID MovieID Rating Datetime 0 196 242 3 1997-12-04 15:55:49 1 186 302 3 1998-04-04 19:22:22 2 22 377 1 1997-11-07 07:18:36 3 244 51 2 1997-11-27 05:02:03 4 166 346 1 1998-02-02 05:33:16 Favorable MovieID 50 100.0 100 89.0 258 83.0 181 79.0 174 74.0 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"Apriori 算法的实现 把各项目放到只包含自己的项集中，生成最初的频繁项集。只使用达到最小支持度的项目。 查找现有频繁项集的超集，发现新的频繁项集，并用其生成新的备选项集。 测试新生成的备选项集的频繁程度（与最小支持度比较），如果不够频繁则舍弃。如果没有新的频繁项集，就跳到最后一步。 存储新发现的频繁项集，跳到步骤 2 返回所有的频繁项集 Input: # 字典保存最新发现的频繁项集 frequent_itemsets = {} min_support = 50 # 第一步，每一步电影生成只包含它自己的项集 # frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素 # 普通集合可变，集合中不能有可变的元素，因此普通集合不能被放在集合中；冻结集合不可变，因此可以被放入集合 frequent_itemsets[1] = dict((frozenset((movie_id,)), row[\"Favorable\"]) for movie_id, row in num_favorable_by_movie.iterrows() if row[\"Favorable\"] \u003e min_support) # 会有重复，导致喜欢电影1,50的人分别为50,100但是 {1,50} 的集合有100个 # 两个原因，第一在current_superset时项集有时候会突然调换位置 def find_frequent_itemsets( favorable_reviews_by_users, k_1_itemsets, min_support): counts = defaultdict(int) # 遍历每一个用户，获取其喜欢的电影 for user, reviews in favorable_reviews_by_users.items(): # 遍历每个项集 for itemset in k_1_itemsets: if itemset.issubset(reviews): # 判断itemset是否是用户喜欢的电影的子集 # 对用户喜欢的电影中除了这个子集的电影进行遍历 for other_reviewed_movie in reviews - itemset: # 将该电影并入项集中 current_superset = itemset | frozenset( {other_reviewed_movie}) counts[current_superset] += 1 # 这个项集的支持度+1 # 返回元素数目+1的项集和数量 res = dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency \u003e= min_support]) return res for k in range(2, 20): cur_frequent_itemsets = find_frequent_itemsets( favorable_reviews_by_users, frequent_itemsets[k - 1], min_support) frequent_itemsets[k] = cur_frequent_itemsets if len(cur_frequent_itemsets) == 0: print(\"Did not find any frequent itemsets of length {}\".format(k)) sys.stdout.flush() # 将缓冲区内容输出到终端，不宜多用，输出操作带来的计算开销会拖慢程序运行速度 break else: print( \"I found {}frequent itemsets of length {}\".format( len(cur_frequent_itemsets), k)) sys.stdout.flush() # 除去只包含一个元素的初始集合 del frequent_itemsets[1] Output: I found 93 frequent itemsets of length 2 I found 295 frequent itemsets of length 3 I found 593 frequent itemsets of length 4 I found 785 frequent itemsets of length 5 I found 677 frequent itemsets of length 6 I found 373 frequent itemsets of length 7 I found 126 frequent itemsets of length 8 I found 24 frequent itemsets of length 9 I found 2 frequent itemsets of length 10 Did not find any frequent itemsets of length 11 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"抽取关联规则 对每个频繁项集，选出其中的一个元素当结论，剩下的元素都作为条件，生成规则。 Input: # 规则形式：如果用户喜欢前提中的所有电影，那么他们也会喜欢结论中的电影 candidate_rules = [] for itemset_length, itemset_counts in frequent_itemsets.items(): for itemset in itemset_counts.keys(): for conclusion in itemset: premise = itemset - {conclusion} candidate_rules.append((premise, conclusion)) print(candidate_rules[:5]) Output: [(frozenset({7}), 1), (frozenset({1}), 7), (frozenset({50}), 1), (frozenset({1}), 50), (frozenset({1}), 56)] 置信度计算，方法与第一章类似。 # 计算置信度 correct_counts = defaultdict(int) incorrect_counts = defaultdict(int) # 遍历每一个用户，获取其喜欢的电影 for user, reviews in favorable_reviews_by_users.items(): # 遍历每个规则 for candidate_rule in candidate_rules: # 获取规则的条件和结论 premise, conclusion = candidate_rule # 如果条件是喜欢电影的子集（条件成立） if premise.issubset(reviews): # 如果用户也喜欢结论的电影 if conclusion in reviews: correct_counts[candidate_rule] += 1 else: incorrect_counts[candidate_rule] += 1 # 计算置信度，结论发生的次数除以条件发生的次数 rule_confidence = { candidate_rule: correct_counts[candidate_rule] / float( correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in candidate_rules} # 给置信度排序 sorted_confidence = sorted( rule_confidence.items(), key=itemgetter(1), reverse=True) for index in range(5): print(\"Rule #{}\".format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] print( \"Rule: If a person recommends {}they will also recommand {}\".format( premise, conclusion)) print( \"- Confidence: {0:.3f}\".format(rule_confidence[(premise, conclusion)])) print(\"--------------------\") Output: Rule #1 Rule: If a person recommends frozenset({98, 181}) they will also recommand 50 - Confidence: 1.000 -------------------- Rule #2 Rule: If a person recommends frozenset({172, 79}) they will also recommand 174 - Confidence: 1.000 -------------------- Rule #3 Rule: If a person recommends frozenset({258, 172}) they will also recommand 174 - Confidence: 1.000 -------------------- Rule #4 Rule: If a person recommends frozenset({1, 181, 7}) they will also recommand 50 - Confidence: 1.000 -------------------- Rule #5 Rule: If a person recommends frozenset({1, 172, 7}) they will also recommand 174 - Confidence: 1.000 -------------------- 调整输出，加上电影名 Input: movie_name_data = pd.read_csv( \"ml-100k/u.item\", delimiter='|', header=None, encoding=\"mac-roman\") movie_name_data.columns = [ 'MovieID', 'Title', 'Release Date', 'Video Release', 'IMDB', '\u003cUNK\u003e', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'] for index in range(5): print('Rule #{0}'.format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] premise_names = ', '.join(get_movie_name(idx) for idx in premise) conclusion_name = get_movie_name(conclusion) print( 'Rule: if a person recommends {0}they will also recommend {1}'.format( premise_names, conclusion_name)) print( ' - Confidence: {0:.3f}'.format(rule_confidence[(premise, conclusion)])) print(\"--------------------\") Output: Rule #1 Rule: if a person recommends Silence of the Lambs, The (1991), Return of the Jedi (1983) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #2 Rule: if a person recommends Empire Strikes Back, The (1980), Fugitive, The (1993) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #3 Rule: if a person recommends Contact (1997), Empire Strikes Back, The (1980) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #4 Rule: if a person recommends Toy Story (1995), Return of the Jedi (1983), Twelve Monkeys (1995) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #5 Rule: if a person recommends Toy Story (1995), Empire Strikes Back, The (1980), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:4","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"评估测试 使用剩下的数据集计算规则的置信度，也是查看每条规则表现的一个方法。 Input: # 评估测试 test_dataset = all_ratings[~all_ratings['UserID'].isin(range(200))] test_favorable = test_dataset[test_dataset[\"Favorable\"]] test_favorable_by_users = dict((k, frozenset(v.values)) for k, v in test_favorable.groupby(\"UserID\")[\"MovieID\"]) correct_counts = defaultdict(int) incorrect_counts = defaultdict(int) for user, reviews in test_favorable_by_users.items(): for candidate_rule in candidate_rules: premise, conclusion = candidate_rule if premise.issubset(reviews): if conclusion in reviews: correct_counts[candidate_rule] += 1 else: incorrect_counts[candidate_rule] += 1 test_confidence = { candidate_rule: correct_counts[candidate_rule] / float( correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in rule_confidence} for index in range(5): print(\"Rule #{0}\".format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] premise_names = \", \".join(get_movie_name(idx) for idx in premise) conclusion_name = get_movie_name(conclusion) print( 'Rule: if a person recommends {0}they will also recommend {1}'.format( premise_names, conclusion_name)) print( ' - Confidence: {0:.3f}'.format(rule_confidence[(premise, conclusion)])) print(\"--------------------\") Output: Rule #1 Rule: if a person recommends Silence of the Lambs, The (1991), Return of the Jedi (1983) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #2 Rule: if a person recommends Empire Strikes Back, The (1980), Fugitive, The (1993) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #3 Rule: if a person recommends Contact (1997), Empire Strikes Back, The (1980) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #4 Rule: if a person recommends Toy Story (1995), Return of the Jedi (1983), Twelve Monkeys (1995) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #5 Rule: if a person recommends Toy Story (1995), Empire Strikes Back, The (1980), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- 这一章用电影进行亲和度分析，由于元素的数量变多了，时间复杂度呈指数级增长，遍历的笨方法已经不适用。需要寻找更加巧妙地解决方案。 在用集合计算电影的项集时，{1, 2} 与 {2, 1} 是同一个事件，但在遍历的时候会被多次计算，可能这是一个错误的点。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:4:5","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第五章 本章讨论如何从数据集中抽取数值和类别型特征，并选出最佳特征。还会介绍特征抽取的常用模式和技巧。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"特征抽取 把实体用特征表示出来，通过特征建模，再通过机器挖掘算法能够理解的近似方式来表示现实。 特征可以是数值型或类别型。数值特征可以离散化生成类别特征。 Input: import numpy as np import pandas as pd if __name__ == '__main__': adult = pd.read_csv(\"adult.data\", header=None, names=[\"Age\", \"Work-Class\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-gain\", \"Capital-loss\", \"Hours-per-week\", \"Native-Country\", \"Earnings-Raw\"]) # 去除空值 adult.dropna(how='all', inplace=True) # 输出详细描述 print(adult[\"Hours-per-week\"].describe()) # 输出中位数 print(adult[\"Education-Num\"].median()) # 输出工作的种类 print(adult[\"Work-Class\"].unique()) # 将工作时长二值化为是否超过40h adult[\"LongHours\"] = adult[\"Hours-per-week\"] \u003e 40 Output: count 32561.000000 mean 40.437456 std 12.347429 min 1.000000 25% 40.000000 50% 40.000000 75% 45.000000 max 99.000000 Name: Hours-per-week, dtype: float64 10.0 [' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov' ' ?' ' Self-emp-inc' ' Without-pay' ' Never-worked'] ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"特征选择 实物的特征有很多，我们只选择其中一小部分。 降低复杂度，提高算法运行速度 减低噪音，增加无关的特征会干扰算法的工作 增加模型可读性，特征较少，人们易于理解 拿到数据后，先进行简单直接的分析，了解数据的特点。 sklearn.feature_selection.VarianceThreshold 转换器可以用来删除特征值的方差达不到最低标准的特征。 Input: # 构造测试数据集 X = np.arange(30).reshape((10, 3)) X[:, 1] = 1 print(X) print(\"----------------\") vt = VarianceThreshold() Xt = vt.fit_transform(X) # 第二列消失了，因为第二列都是1，方差为0，不包括具有区别意义的信息 print(Xt) print(\"----------------\") print(vt.variances_) Output: [[ 0 1 2] [ 3 1 5] [ 6 1 8] [ 9 1 11] [12 1 14] [15 1 17] [18 1 20] [21 1 23] [24 1 26] [27 1 29]] ---------------- [[ 0 2] [ 3 5] [ 6 8] [ 9 11] [12 14] [15 17] [18 20] [21 23] [24 26] [27 29]] ---------------- [27. 0. 27.] 选择最佳特征 随着特征数量的增加，寻找最佳特征组合的任务复杂度呈指数级增长。分类任务通常的做法是寻找表现好的单个特征，依据是他们能达到的精确度。 scikit-learn 提供了几个用于选择单变量特征的转换器。 SelectKBest 返回 k 个最佳特征 SelectPercentile 返回表现最佳的 r%个特征 这两个转换器都提供计算特征表现的一系列方法。 单个特征和某一类别之间的相关性计算方法有卡方检验(x²)、互信息和信息熵等。 Input: # 构造数据集 X = adult[[\"Age\", \"Education-Num\", \"Capital-gain\", \"Capital-loss\", \"Hours-per-week\"]] y = (adult[\"Earnings-Raw\"] == ' \u003e50K').values # 使用SelectKBest转换器，用卡方打分 transformer = SelectKBest(score_func=chi2, k=3) # 调用fit_transform方法对相同的数据集进行预处理和转换 Xt_chi2 = transformer.fit_transform(X, y) # 输出每个特征的得分 print(transformer.scores_) print(\"----------------\") # 用皮尔逊相关系数计算相关性,创建包装函数 def mutivariate_pearsonr(X, y): scores, pvalues = [], [] for column in range(X.shape[1]): cur_score, cur_p = pearsonr(X[:, column], y) scores.append(abs(cur_score)) pvalues.append(cur_p) return np.array(scores), np.array(pvalues) transformer = SelectKBest(score_func=mutivariate_pearsonr, k=3) Xt_pearson = transformer.fit_transform(X, y) print(transformer.scores_) print(\"----------------\") clf = DecisionTreeClassifier(random_state=14) scores_chi2 = cross_val_score(clf, Xt_chi2, y, scoring='accuracy') scores_pearson = cross_val_score(clf, Xt_pearson, y, scoring='accuracy') print('卡方: {}'.format(np.mean(scores_chi2))) print(\"----------------\") print(\"pearson: {}\".format(np.mean(scores_pearson))) Output: [8.60061182e+03 2.40142178e+03 8.21924671e+07 1.37214589e+06 6.47640900e+03] ---------------- [0.2340371 0.33515395 0.22332882 0.15052631 0.22968907] ---------------- 卡方: 0.8291514400795839 ---------------- pearson: 0.7721507467016449 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"创建特征 特征之间相关性很强，或者特征冗余，会增加算法处理难度。 这里在加载 ad 数据集之前先创建了一个转换器，用于在加载时转换数据集中的值。 源码运行会产生报错，第一个原因是，用函数初始化转换器并没有把函数名传入，因此将 defaultdict 中每一个索引都进行了初始化。第二个原因是，PCA 转换器无法对 NaN 数据进行处理，于是我在处生成数据集之前将所有含有 NaN 的行删掉。 Input: # -*- coding: utf-8 -*- import numpy as np import pandas as pd from collections import defaultdict from sklearn.decomposition import PCA from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score from matplotlib import pyplot as plt # 创建转换函数 def convert_number(x): try: res = float(x) return res except ValueError: return np.nan if __name__ == '__main__': # 创建数据加载的转换器 converters = defaultdict(convert_number, {i: convert_number for i in range(1588)}) converters[1558] = lambda x: 1 if x.strip() == \"ad.\" else 0 # 使用转换器读取数据集 temp = pd.read_csv(\"ad.data\", header=None, converters=converters) # 删除所有含有nan的行,axis=0是数据索引(index)，axis=1是列标签(column) ads = temp.dropna(axis=0, how='any') print(ads[10:15]) Output: 0 1 2 3 4 5 ... 1553 1554 1555 1556 1557 1558 11 90.0 52.0 0.5777 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 12 90.0 60.0 0.6666 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 13 90.0 60.0 0.6666 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 14 33.0 230.0 6.9696 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 15 60.0 468.0 7.8000 1.0 0.0 0.0 ... 0.0 1.0 1.0 0.0 0.0 1 [5 rows x 1559 columns] 主成分分析(PCA) 目的是找到能用较少信息描述数据集的特征组合。主成分的方差跟整体方差没有多大差距。经过分析主成分，第一个特征的方差对数据集方差的贡献率为 85.4%，第二个为 14.5%，后面越来越少。 Input: X = ads.drop(1558, axis=1).values y = ads[1558] # 参数为主成分数量 pca = PCA(n_components=5) Xd = pca.fit_transform(X) # 设置输出选项 # 第一个参数为输出精度位数，第二个参数是使用定点表示法打印浮点数 np.set_printoptions(precision=3, suppress=True) print(pca.explained_variance_ratio_) Output: [0.854 0.145 0.001 0. 0. ] 使用随机森林验证模型正确率，并将 pca 转换结果绘制出来。 Input: clf = DecisionTreeClassifier(random_state=14) scores_reduced = cross_val_score(clf, Xd, y, scoring='accuracy') print(np.mean(scores_reduced)) # 获取数据集类别的所有取值 classes = set(y) # 指定在图形中用什么颜色表示这两个类别 colors = ['red', 'green'] # 同时遍历这两个容器 for cur_class, color in zip(classes, colors): # 为属于当前类别的所有个体创建遮罩层 mask = (y == cur_class).values plt.scatter(Xd[mask, 0], Xd[mask, 1], marker='o', color=color, label=int(cur_class)) plt.legend() plt.show() Output: 0.936405592140775 输出结果pca \" 输出结果 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"创建自己的转换器 转换器有两个关键函数 fit() 接收训练数据，设置内部参数 transform() 转换过程。接收训练数据集或相同格式的新数据集 接口要与 scikit-learn 接口一致，便于在流水线中使用。 Input: # -*- coding: utf-8 -*- import numpy as np from sklearn.base import TransformerMixin from sklearn.utils import as_float_array from numpy.testing import assert_array_equal class MeanDiscrete(TransformerMixin): def fit(self, X): # 尝试对X进行转换，数据转换成float类型 X = as_float_array(X) # 计算数据集的均值 self.mean = X.mean(axis=0) # 返回它本身，进行链式调用transformer.fit(X).transform(X) return self def transform(self, X): X = as_float_array(X) # 检查输入是否合法 assert X.shape[1] == self.mean.shape[0] # 返回X中大于均值的数据 return X \u003e self.mean def test_meandiscrete(): X_test = np.array([[0, 2], [3, 5], [6, 8], [9, 11], [12, 14], [15, 17], [18, 20], [21, 23], [24, 26], [27, 29]]) mean_discrete = MeanDiscrete() mean_discrete.fit(X_test) # 与正确的计算结果进行比较，检查内部参数是否正确设置 assert_array_equal(mean_discrete.mean, np.array([13.5, 15.5])) # 转换后的X X_transfromed = mean_discrete.transform(X_test) # 验证数据 X_expected = np.array([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]) assert_array_equal(X_transfromed, X_expected) if __name__ == '__main__': test_meandiscrete() Output: # 没有输出，说明测试通过 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:5:4","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第六章 本章介绍如何从文本数据中提取特征。通过强大却简单的朴素贝叶斯算法消除社会媒体用语的歧义。 朴素贝叶斯算法在计算用于分类的概率时，为了简化计算，假定各特征间是相互独立的，因此名字中含有朴素二字。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"消歧 由于无法申请到 Twitter app 暂时搁置。。。%\u003e_\u003c% ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"文本转换器 词袋：一种最简单却非常有效的模型就是只统计数据集中每个单词的出现次数。模型主要分为以下三种 使用词语实际出现的次数作为词频。缺点是当文章长度明显差异时，词频差距会非常大。 使用归一化后的词频，每篇文章中所有词语的词频之和为 1 直接使用二值特征来表示，单词在文档中出现值为 1，不出现值为 0 还有一种更通用的规范化方法叫做词频-逆文档频率法，该加权方法用词频来代替词的出现次数，然后再用词频除以包含该词的文档的数量。 Input: # -*- coding: utf-8 -*- from collections import Counter if __name__ == '__main__': s = \"\"\"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in halls of stone, Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne In the Land of Mordor where the Shadows lie. One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them. In the Land of Mordor where the Shadows lie\"\"\".lower() words = s.split() c = Counter(words) # 输出出现次数最多的前5个词 print(c.most_common(5)) Output: [('the', 9), ('for', 4), ('in', 4), ('to', 4), ('one', 4)] N 元语法是指由几个连续的词组成的子序列。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"朴素贝叶斯 我们用 C 表示某种类别，用 D 表示数据集中一篇文档，来计算贝叶斯公式所要用到的各种统计量，对于不好计算，出朴素假设，简化计算。朴素贝叶斯分类算法使用贝叶斯定理计算个体从属于某一类别的概率。 P(C) 为某一类别的概率，可以从训练集中计算得到（方法跟上文检测垃圾邮件例子所用到的一致）。统计训练集所有文档从属于给定类别的百分比。 P(D) 为某一文档的概率，它牵扯到各种特征，计算起来很困难，但是在计算文档属于哪个类别时，对于所有类别来说，P(D)相同，因此根本就不用计算它。稍后我们来看下怎么处理。 P(D|C) 为文档 D 属于 C 类的概率。由于 D 包含多个特征，计算起来可能很困难，这时朴素贝叶斯算法就派上用场了。我们朴素地假定各个特征之间是相互独立的，分别计算每个特征（D1、D2、D3 等）在给定类别出现的概率，再求它们的积。 P(D|C) = P(D1|C) x P(D2|C) ... x P(Dn|C) 举例说明下计算过程，假如数据集中有以下一条用二值特征表示的数据：[1, 0, 0, 1] 训练集中有 75% 的数据属于类别 0， 25% 属于类别 1，且每个特征属于每个类别的似然度如下。 类别 0：[0.3, 0.4, 0.4, 0.7] 类别 1：[0.7, 0.3, 0.4, 0.9] 拿类别 0 中特征 1 的似然度举例子，上面这两行数据可以这样理解：类别 0 中有 30%的数据，特征 1 的值为 1。 我们来计算一下这条数据属于类别 0 的概率。类别为 0 时，P(C=0) = 0.75。 朴素贝叶斯算法用不到 P(D)，因此我们不用计算它。 P(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0) = 0.3 x 0.6 x 0.6 x 0.7 = 0.0756 我们就可以计算该条数据从属于每个类别的概率。我们没有计算 P(D)，因此，计算结果不是实际的概率。由于两次都不计算 P(D)，结果具有可比较性，能够区分出大小就足够了。来看下计算结果。 P(C=0|D) = P(C=0) P(D|C=0) = 0.75 * 0.0756 = 0.0567 P(D|C=1) = P(D1|C=1) x P(D2|C=1) x P(D3|C=1) x P(D4|C=1) = 0.7 x 0.7 x 0.6 x 0.9 = 0.2646 P(C=1|D) = P(C=1)P(D|C=1) = 0.25 * 0.2646 = 0.06615 因此这条数据属于类别 1 的概率大于属于类别 2 的概率 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"应用 创建流水线，接收一条消息，仅根据消息内容，确定它与编程语言 Python 是否相关。 用 NLTK 的 word_tokenize 函数，将原始文档转换为由单词及其是否出现组成的字典。 用 scikit-learn 中的 DictVectorizer 转换器将字典转换为向量矩阵，这样朴素贝叶斯分类器就能使用第一步中抽取的特征。 正如前几章做过的那样，训练朴素贝叶斯分类器。 还需要新建一个笔记本文件 ch6_classify_twitter（本章最后一个），用于分类。 F1 值来评估算法 F1 值是以每个类别为基础进行定义的，包括两大概念：准确率（precision）和召回率（recall）。准确率是指预测结果属于某一类的个体，实际属于该类的比例。召回率是指被正确预测为某个类别的个体数量与数据集中该类别个体总量的比例 Input: # -*- coding: utf-8 -*- import json import numpy as np from sklearn.base import TransformerMixin from nltk import word_tokenize from sklearn.feature_extraction import DictVectorizer # 接受元素为字典的列表，将其转换为矩阵 from sklearn.model_selection import cross_val_score from sklearn.naive_bayes import BernoulliNB # 用于二值特征分类的 BernoulliNB 分类器， from sklearn.pipeline import Pipeline # 创建转换器类 class NLTKBOW(TransformerMixin): def fit(self, X, y=None): return self def transform(self, X): return [{word: True for word in word_tokenize(document)} for document in X] if __name__ == '__main__': tweets = [] input_filename = \"\" classes_filename = \"\" with open(input_filename) as inf: for line in inf: if len(line.strip()) == 0: continue tweets.append(json.loads(line)['text']) with open(classes_filename, 'r') as inf: labels = json.load(inf) # 组装流水线 pipline = Pipeline([('bag-of-words', NLTKBOW()), ('vectorizer', DictVectorizer()), ('naive-bayes', BernoulliNB())]) # 用F1值来评估 scores = cross_val_score(pipline, tweets, labels, scoring='f1') print(\"Score: {:.3f}\".format(np.mean(scores))) model = pipline.fit(tweets, labels) nb = model.named_steps['naive-bayes'] feature_probabilities = nb.feature_log_prob_ top_features = np.argsort(-feature_probabilities[1])[:50] dv = model.named_steps['vectorizer'] for i, feature_index in enumerate(top_features): print(i, dv.feature_names_[feature_index], np.exp(feature_probabilities[1][feature_index])) Output: 暂时没有数据集 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:6:4","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第七章 本章介绍的算法引入聚类分析概念–根据相似度，把大数据集划分为几个子集。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:7:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"加载数据集 由于申请不到 Twitter 开发者账号，我想办法爬了一些 b 站用户关注数据，做成了本次试验相仿的形式 Input: # -*- coding: utf-8 -*- import json import pandas as pd import networkx as nx from matplotlib import pyplot as plt import numpy as np from scipy.optimize import minimize from sklearn.metrics import silhouette_score if __name__ == '__main__': with open('bili.txt', mode='r') as fin: temp = json.load(fin) users = pd.DataFrame(temp) users.columns = ['Id', 'Friends'] print(users[:5]) Output: Id Friends 0 214582845 [4370617, 259345180, 186334806, 546195, 477132... 1 4370617 [74507, 883968, 122879, 585267] 2 259345180 [] 3 186334806 [] 4 546195 [] 将每个记录的用户左右 main_users，把他们关注的人作为边，生成有向图 由于对 matplotlib 库和 networkx 库了解太少，在作图时遇到了许多困难（根基不牢，地动山摇。(\u003e_\u003c)） Input: G = nx.DiGraph() main_users = list(users['Id'].values) for u in main_users: G.add_node(u, label=u) for user in users.values: friends = user[1] for friend in friends: if friend in main_users: G.add_edge(user[0], int(friend)) print('graph finished') plt.figure(3, figsize=(100, 100)) nx.draw(G, alpha=0.1, edge_color='b', with_labels=True, font_size=16, node_size=30, node_color='r') plt.savefig('fix1.png') Output: 创建用户相似度图 由于每个用户关注的人数可能相差很大，因此使用杰卡德相似系数（两个用户关注的集合的交集除以并集），该系数在 0 到 1 之间，代表两者重合的比例。 规范化是数据挖掘的一个重要方法，要坚持使用（除非有充足的理由不这样做） 访问http://networkx.lanl.gov/reference/drawing/html了解 networkx 的布局方法 Input: friends = {user: set(friends) for user, friends in users.values} def compute_similarity(friends1, friends2): return len(friends1 \u0026 friends2) / len(friends1 | friends2) def create_graph(followers, threshold=0.0): G = nx.Graph() for user1 in friends.keys(): if len(friends[user1]) == 0: continue for user2 in friends.keys(): if len(friends[user2]) == 0: continue if user1 == user2: continue weight = compute_similarity(friends[user1], friends[user2]) if weight \u003e= threshold: G.add_node(user1, lable=user1) G.add_node(user2, lable=user2) G.add_edge(user1, user2, weight=weight) return G G = create_graph(friends) plt.figure(3, figsize=(100, 100)) pos = nx.spring_layout(G) nx.draw_networkx_nodes(G, pos, node_size=30) edgewidth = [d['weight'] for (u, v, d) in G.edges(data=True)] nx.draw_networkx_edges(G, pos, width=edgewidth) plt.savefig('fix2.png') Output: ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:7:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"寻找子图 networkx 的 connected_component_subgraphs() 函数在 2.1 版本中被移除了（代码过时的比较多，并且使用 Twitter 作为演示数据集让我这两章做的很头疼），我查看官方文档后发现可以使用 connected_components() 替代，但是此函数返回的是一个生成器，一次生成一组连通顶点，可以配合 G.subgraph(nodes) 使用获得连通分支 Input: # 生成新图，指定最低阈值为0.1 G = create_graph(friends, 0.1) sub_graphs = nx.connected_components(G) for i, sub_graphs in enumerate(sub_graphs): n_nodes = len(sub_graphs) print(\"Subgraph{}has {}nodes\".format(i, n_nodes)) print('---------------------') G = create_graph(friends, 0.15) sub_graphs = nx.connected_components(G) for i, sub_graphs in enumerate(sub_graphs): n_nodes = len(sub_graphs) print(\"Subgraph{}has {}nodes\".format(i, n_nodes)) sub_graphs = [c for c in sorted(nx.connected_components(G), key=len, reverse=True)] n_subgraphs = nx.number_connected_components(G) fig = plt.figure(figsize=(20, (n_subgraphs*3))) for i, sub_graph in enumerate(sub_graphs): # sub_graph是一个连通分支顶点的集合 ax = fig.add_subplot(int(n_subgraphs / 3) + 1, 3, i + 1) # 将坐标轴标签关掉 ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) pos = nx.spring_layout(G) nx.draw(G=G.subgraph(sub_graph), alpha=0.1, edge_color='b', with_labels=True, font_size=16, node_size=30, node_color='r', ax=ax) plt.show() Output: 轮廓系数定义： s = (b - a) / max(a, b) 其中 a 为簇内距离，表示与簇内其它个体之间的平均距离。b 为簇间距离，也就是最近簇内各个个体之间的平均距离 Input: def compute_silhouette(threshold, friends): G = create_graph(friends, threshold=threshold)\\ # 图是否至少有两个顶点 if len(G.nodes()) \u003c 2: # 返回-99表示问题无效 return -99 # 抽取连通分支 sub_graphs = nx.connected_components(G) # 至少有两个连通分支 if not (2 \u003c= nx.number_connected_components(G) \u003c len(G.nodes()) - 1): return -99 label_dict = {} for i, sub_graph in enumerate(sub_graphs): for node in sub_graph: # 给不同连通分支的顶点分配不同的标签 label_dict[node] = i labels = np.array([label_dict[node] for node in G.nodes()]) X = nx.to_scipy_sparse_matrix(G).todense() # 这里要将相似度转换为距离，所以用最大相似度减去现有相似度，把相似度转化为距离 X = 1 - X # 这里将距离矩阵的对角线处理为0，因为自己到自己的距离为0 np.fill_diagonal(X, 0) return silhouette_score(X, labels, metric='precomputed') def inverted_silhouette(threshold, friends): # 对轮廓系数取反，将打分函数转化成损失函数 res = compute_silhouette(threshold, friends=friends) return - res # minimize函数是一个损失函数，值越小越好 # 参数：inverted_silhouette要寻找的函数；0.1开始时猜测的阈值；options={'maxiter': 10} 只进行10轮迭代，增加迭代次数，效果可能更好，但运行时间会增加，method='nelder-mead'使用\"下山单纯形法\"优化方法 result = minimize(inverted_silhouette, 0.1, args=(friends,), options={'maxiter': 10}) print(result.x) Output: [0.10005086] 本章探讨了社交网络和图以及如何对其进行聚类分析。目标是推荐用户，使用聚类分析方法能够找到不同的用户簇，主要步骤有根据相似度创建加权图，从图中寻找连通分支。创建图时用到了 NetworkX 库。 还比较了几对意义相反的概念。对于两者之间的相似度这个概念，值越大，表明两者之间更相像。相反，对于距离而言，值越小，两者更相像。另外一对是损失函数和打分函数。对于损失函数，值越小，效果越好（也就是损失越少）。而对于打分函数，值越大，效果越好。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:7:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第八章 本章使用神经网络分析自己生成的验证码图像 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"人工神经网络 神经网络算法最初是根据人类大脑的工作机制设计的。神经网络由一系列相互连接的神经元组成。每个神经元都是一个简单的函数，接收一定输入，给出相应输出。 神经元可以使用任何标准函数来处理数据，比如线性函数，这些函数统称为激活函数（activation function）。一般来说，神经网络学习算法要能正常工作，激活函数应当是可导（derivable）和光滑的。常用的激活函数有逻辑斯谛函数，函数表达式如下（x 为神经元的输入，k、L 通常为 1，这时函数达到最大值）。 $$ f(x) = \\frac{L}{1+e^{-k(x-x_{0})}} $$ 每个神经元接收几个输入，根据这几个输入，计算输出。这样的一个个神经元连接在一起组成了神经网络，对数据挖掘应用来说，它非常强大。这些神经元紧密连接，密切配合，能够通过学习得到一个模型，使得神经网络成为机器学习领域最强大的概念之一。 数据挖掘应用的神经网络，神经元按照层级进行排列，至少有三层 第一层：输入层。用来接收数据集的输入。第一层中的每个神经元对输入进行计算，把得到的结果传给第二层的神经元。这种叫作前向神经网络 隐含层：数据表现方式令人难以理解，一层或多层 最后一层：输出层。输出结果表示的是神经网络分类器给出的分类结果 神经元激活函数通常使用逻辑斯谛函数，每层神经元之间为全连接，创建和训练神经网络还需要用到其他几个参数。 创建过程，指定神经网络的规模需要用到两个参数：神经网络共有多少层，隐含层每层有多少个神经元（输入层和输出层神经元数量通常由数据集来定）。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"创建数据集 使用长度为 4 个字母的英文单词作为验证码 Input: # -*- coding: utf-8 -*- import numpy as np from PIL import Image, ImageDraw, ImageFont from skimage import transform as tf from skimage.transform import resize from matplotlib import pyplot as plt from skimage.measure import label, regionprops # 用于图像分割 from sklearn.utils import check_random_state from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from pybrain.datasets.supervised import SupervisedDataSet # 神经网络数据集 from pybrain.tools.shortcuts import buildNetwork # 构建神经网络 from pybrain.supervised.trainers.backprop import BackpropTrainer # 反向传播算法 from sklearn.metrics import f1_score from nltk.corpus import words # 导入语料库 用于生成单词 from sklearn.metrics import confusion_matrix # 混淆矩阵 from nltk.metrics import edit_distance # 编辑距离 from operator import itemgetter # 用于生成验证码，接收一个单词和错切值，返回用numpy数组格式表示的图像 def create_captcha(text, shear=0.0, size=(100, 26)): im = Image.new(\"L\", size, \"black\") draw = ImageDraw.Draw(im) # 验证码文字所用字体，该开源字体可在github下载 font = ImageFont.truetype(\"FiraCode-Medium.otf\", 22) draw.text((0, 0), text, fill=1, font=font) # 将PIL图像转换为numpy数组，以便用scikit-image库为图像添加错切变化效果 image = np.array(im) # 应用错切变化效果 affine_tf = tf.AffineTransform(shear=shear) image = tf.warp(image, affine_tf) # 对图像进行归一化处理，确保特征值落在0到1之间 return image / image.max() if __name__ == '__main__': image = create_captcha('GENE', shear=0.5) plt.imshow(image, cmap='Greys') plt.show() Output: 将图像切分为单个的字母 Input: def segment_image(image): \"\"\" 接收图像，返回小图像列表 :param image: :return: \"\"\" # 找出像素值相同又连接在一起的像素块，类似上一章的连通分支 labeled_image = label(image \u003e 0) subimages = [] for region in regionprops(labeled_image): # 获取当前位置的起始和结束坐标 start_x, start_y, end_x, end_y = region.bbox subimages.append(image[start_x:end_x, start_y:end_y]) # 如果没有找到小图像，则将原图像作为子图返回 if len(subimages) == 0: return [image, ] return subimages subimages = segment_image(image) f, axes = plt.subplots(1, len(subimages), figsize=(10, 3)) for i in range(len(subimages)): axes[i].imshow(subimages[i], cmap='gray') plt.show() Output: 创建训练集 Input: # 指定随机状态值 random_state = check_random_state(14) letters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\") shear_values = np.arange(0, 0.5, 0.05) # 用来生成一条训练数据 def generate_sample(random_state=None): random_state = check_random_state(random_state) letter = random_state.choice(letters) shear = random_state.choice(shear_values) return create_captcha(letter, shear=shear, size=(25, 25)), letters.index(letter) image, target = generate_sample(random_state) plt.imshow(image, cmap='Greys') print(\"The target for this image is {}\".format(target)) plt.show() # 调用3000次此函数，生成训练数据传到numpy的数组里 dataset, targets = zip(*(generate_sample(random_state) for i in range(3000))) dataset = np.array(dataset, dtype=float) targets = np.array(targets) # 对26个字母类别进行编码 onehot = OneHotEncoder() y = onehot.fit_transform(targets.reshape(targets.shape[0], 1)) # 将稀疏矩阵转换为密集矩阵 y = y.todense() # 调整图像大小 dataset = np.array([resize(segment_image(sample)[0], (20, 20)) for sample in dataset]) # 将最后三维的dataset的后二维扁平化 X = dataset.reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2])) X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9) Output: The target for this image is 11 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"训练和分类 反向传播算法（back propagation，backprop）的工作机制为对预测错误的神经元施以惩罚。从输出层开始，向上层层查找预测错误的神经元，微调这些神经元输入值的权重，以达到修复输出错误的目的。 神经元之所以给出错误的预测，原因在于它前面为其提供输入的神经元，更确切来说是由这两个神经元之间边的权重及输入值决定的。我们可以尝试对权重进行微调。每次调整的幅度取决于以下两个方面 神经元各边权重的误差函数的偏导数 一个叫作学习速率的参数（通常使用很小的值） 计算出函数误差的梯度，再乘以学习速率，用总权重减去得到的值。梯度的符号由误差决定，每次对权重的修正都是朝着给出正确的预测值努力。有时候，修正结果为局部最优（local optima），比起其他权重组合要好，但所得到的各权重还不是最优组合。 反向传播算法从输出层开始，层层向上回溯到输入层。到达输入层后，所有边的权重更新完毕。 这里在导入 SupervisedDataSet 时发生了错误，使用 pip install pybrain 安装的包会有找不到方法的现象，因此我从 github-pybrain 下载了源码包，在解压后的文件夹中输入 python setup.py install 进行安装，解决了这个问题。还有一个问题是原文使用 from pybrain.datasets import SupervisedDataSet 来导入 SupervisedDataSet 但是我在导入时发现并没有这个类，于是看了项目结构后使用 from pybrain.datasets.supervised import SupervisedDataSet 进行导入。还有几处相同的问题均是这样解决的。 这里在使用 f1_score 进行评估时也出现了错误，原因见代码注释。 Input: # 为pybrain库创建格式适配的数据集 training = SupervisedDataSet(X.shape[1], y.shape[1]) for i in range(X_train.shape[0]): training.addSample(X_train[i], y_train[i]) testing = SupervisedDataSet(X.shape[1], y.shape[1]) for i in range(X_test.shape[0]): testing.addSample(X_test[i], y_test[i]) # 指定维度，创建神经网络，第一个参数为输入层神经元数量，第二个参数隐含层神经元数量，第三个参数为输出层神经元数量 # bias在每一层使用一个一直处于激活状态的偏置神经元 net = buildNetwork(X.shape[1], 100, y.shape[1], bias=True) # 使用反向传播算法调整权重 trainer = BackpropTrainer(net, training, learningrate=0.01, weightdecay=0.01) # 设定代码的运行步数 trainer.trainEpochs(epochs=20) # 预测值 predictions = trainer.testOnClassData(dataset=testing) # f1_score的average默认值为'binary'，如果不指定average则会发生ValueError print(\"F-score:{0:.2f}\".format(f1_score(y_test.argmax(axis=1), predictions, average='weighted'))) print(\"F-score:{0:.2f}\".format(f1_score(y_test.argmax(axis=1), predictions, average='micro'))) print(\"F-score:{0:.2f}\".format(f1_score(y_test.argmax(axis=1), predictions, average='macro'))) Output: F-score:1.00 F-score:1.00 F-score:1.00 预测单词 Input: # 接收验证码，用神经网络进行训练，返回单词预测结果 def predict_captcha(captcha_image, neural_network): subimages = segment_image(captcha_image) predicted_word = \"\" # 遍历四张小图像 for subimage in subimages: # 调整每张小图像的大小为20*20像素 subimage = resize(subimage, (20,20)) # 把小图像数据传入神经网络的输入层，激活神经网络。这些数据将在神经网络中进行传播，返回输出结果 outputs = net.activate(subimage.flatten()) # 神经网络输出26个值，每个值都有索引号，分别对应letters列表中有着相同索引的字母，每个值的大小表示与对应字母的相似度。为了获得实际的预测值，我们取到最大值的索引，再通过letters列表找到对应的字母 prediction = np.argmax(outputs) # 把上面得到的字母添加到正在预测的单词中 predicted_word += letters[prediction] return predicted_word word = \"GENE\" captcha = create_captcha(word, shear=0.2) print(predict_captcha(captcha, net)) Output: GENE nltk 下载语料库时可能会很慢，需要的可以在这里下载。如何离线安装 nltk 语料库自行百度。 Input: def test_prediction(word, net, shear=0.2): captcha = create_captcha(word, shear=shear) prediction = predict_captcha(captcha, net) prediction = prediction[:4] # 返回预测结果是否正确，验证码中的单词和预测结果的前四个字符 return word == prediction, word, prediction # 语料库中字长为4的单词列表 valid_words = [word.upper() for word in words.words() if len(word) == 4] num_correct = 0 num_incorrect = 0 for word in valid_words: correct, word, prediction = test_prediction(word, net, shear=0.2) if correct: num_correct += 1 else: num_incorrect += 1 print(\"Number correct is {}\".format(num_correct)) print(\"Number incorrect is {}\".format(num_incorrect)) # 二维混淆矩阵， 每行每列均为一个类别 cm = confusion_matrix(np.argmax(y_test,axis=1), predictions) # 混淆矩阵作图 plt.figure(figsize=(20, 20)) plt.imshow(cm) tick_marks = np.arange(len(letters)) plt.xticks(tick_marks, letters) plt.yticks(tick_marks, letters) plt.ylabel('Actual') plt.xlabel('Predicted') plt.show() Output: Number correct is 3738 Number incorrect is 1775 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"用词典提升准确率 假设验证码全部都是英语单词 列文斯坦编辑距离（Levenshtein edit distance）是一种通过比较两个短字符串，确定它们相似度的方法。它不太适合扩展，字符串很长时通常不用这种方法。编辑距离需要计算从一个单词变为另一个单词所需要的步骤数。以下操作都算一步 在单词的任意位置插入一个新字母 从单词中删除任意一个字母 把一个字母替换为另外一个字母 Input: # 获得两个单词的编辑距离 steps = edit_distance(\"STEP\", \"STOP\") print(\"The num of steps needed is: {}\".format(steps)) # 用词长4减去同等位置上相同的字母数量，得到的值越小表示两个词相似度越高 def compute_distance(prediction, word): return len(prediction) - sum(prediction[i] == word[i] for i in range(len(prediction))) # 改进预测函数 def improved_prediction(word, net, dictionary, shear=0.2): captcha = create_captcha(word, shear=shear) prediction = predict_captcha(captcha, net) prediction = prediction[:4] # 如果单词不在词典中则比较取词典中距离最小的单词 if prediction not in dictionary: distance = sorted([(w, compute_distance(prediction, w)) for w in dictionary], key=itemgetter(1)) best_word = distance[0] prediction = best_word[0] return word == prediction, word, prediction num_correct = 0 num_incorrect = 0 for word in valid_words: correct, word, prediction = improved_prediction(word, net, valid_words,shear=0.2) if correct: num_correct += 1 else: num_incorrect += 1 print(\"Number correct is {}\".format(num_correct)) print(\"Number incorrect is {}\".format(num_incorrect)) Output: The num of steps needed is: 1 Number correct is 3785 Number incorrect is 1728 正确率稍有提升 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:8:4","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第九章 昨天跑去搞 wordpress 搭建网站了 (๑•́ ₃•̀๑) （摸鱼真舒服 本章主要介绍如下内容 特征工程和如何根据应用选择特征 带着新问题，重新回顾词袋模型 特征类型和字符 N 元语法模型 支持向量机 数据集清洗 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:9:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"为作品找到作者 作者归属可以看作是一种分类问题，已知一部分作者，数据集为多个作者的作品（训练集），目标是确定一组作者不详的作品（测试集）是谁写的。如果作者恰好是已知的作者里面的，这种问题叫作封闭问题 如果作者可能不在里面，这种问题就叫作开放问题 获取数据，书中的链接有很多已经失效，我参考网上的取得了下载方式。 Input: # -*- coding: utf-8 -*- # get_data.py import requests import os import time from collections import defaultdict titles = {'burton': [4657, 2400, 5760, 6036, 7111, 8821, 18506, 4658, 5761, 6886, 7113], 'dickens': [24022, 1392, 1414, 1467, 2324, 580, 786, 888, 963, 27924, 1394, 1415, 15618, 25985, 588, 807, 914, 967, 30127, 1400, 1421, 16023, 28198, 644, 809, 917, 968, 1023, 1406, 1422, 17879, 30368, 675, 810, 924, 98, 1289, 1413, 1423, 17880, 32241, 699, 821, 927], 'doyle': [2349, 11656, 1644, 22357, 2347, 290, 34627, 5148, 8394, 26153, 12555, 1661, 23059, 2348, 294, 355, 5260, 8727, 10446, 126, 17398, 2343, 2350, 3070, 356, 5317, 903, 10581, 13152, 2038, 2344, 244, 32536, 423, 537, 108, 139, 2097, 2345, 24951, 32777, 4295, 7964, 11413, 1638, 21768, 2346, 2845, 3289, 439, 834], 'gaboriau': [1748, 1651, 2736, 3336, 4604, 4002, 2451, 305, 3802, 547], 'nesbit': [34219, 23661, 28804, 4378, 778, 20404, 28725, 33028, 4513, 794], 'tarkington': [1098, 15855, 1983, 297, 402, 5798, 8740, 980, 1158, 1611, 2326, 30092, 483, 5949, 8867, 13275, 18259, 2595, 3428, 5756, 6401, 9659], 'twain': [1044, 1213, 245, 30092, 3176, 3179, 3183, 3189, 74, 86, 1086, 142, 2572, 3173, 3177, 3180, 3186, 3192, 76, 91, 119, 1837, 2895, 3174, 3178, 3181, 3187, 3432, 8525]} assert len(titles) == 7 assert len(titles['tarkington']) == 22 assert len(titles['dickens']) == 44 assert len(titles['nesbit']) == 10 assert len(titles['doyle']) == 51 assert len(titles['twain']) == 29 assert len(titles['burton']) == 11 assert len(titles['gaboriau']) == 10 url_base = 'http://www.gutenberg.org/files/' url_format = '{url_base}{id}/{id}-0.txt' # 修复URL url_fix_format = 'http://www.gutenberg.org/cache/epub/{id}/pg{id}.txt' fiexes = defaultdict(list) # fixes = {} # fixes[4657] = 'http://www.gutenberg.org/cache/epub/4657/pg4657.txt' # make parent folder if not exists # data_folder = os.path.join(os.path.expanduser('~'),'Data','books') # # 这是在用户user目录中存储 data_folder = os.path.dirname(os.path.abspath(__file__)) if __name__ == '__main__': if not os.path.exists(data_folder): os.makedirs(data_folder) print(data_folder) for author in titles: print('Downloading titles from', author) # make author's folder if not exists author_folder = os.path.join(data_folder, author) if not os.path.exists(author_folder): os.makedirs(author_folder) # download each title to this folder for bookid in titles[author]: # if bookid in fixes: # print(' - Applying fix to book with id', bookid) # url = fixes[bookid] # else: # print(' - Getting book with id', bookid) # url = url_format.format(url_base=url_base, id=bookid) url = url_format.format(url_base=url_base, id=bookid) print(' - ', url) filename = os.path.join(author_folder, '%s.txt' % bookid) if os.path.exists(filename): print(' - File already exists, skipping') else: r = requests.get(url) if r.status_code == 404: print('url 404:', author, bookid, 'add to fixes list') fiexes[author].append(bookid) else: txt = r.text with open(filename, 'w', encoding='utf-8') as f: f.write(txt) time.sleep(1) print('Download complete') print('开始下载修复列表') for author in fiexes: print('开始下载\u003c%s\u003e的作品' % author) author_folder = os.path.join(data_folder, author) if not os.path.exists(author_folder): os.makedirs(author_folder) for bookid in fiexes[author]: filename = os.path.join(author_folder, '%s.txt' % bookid) if os.path.exists(filename): print('文件已经下载，跳过') else: url_fix = url_fix_format.format(id=bookid) print(' - ', url_fix) r = requests.get(url_fix) if r.status_code == 404: print('又出错了！', author, bookid) else: with open(filename, 'w', encoding='utf-8') as f: f.write(r.text) time.sleep(1) print('修复列表下载完毕') 最后下载完成有 177 本书 支持向量机是一种二类分类器，扩展后可用来对多个类别进行分类 9（对于多种类别的分类问题，我们创建多个 SVM 分类器——每个还是二类分类器） C 参数对于训练 SVM 来说很重要，C 参数与分类器正确分类比例相关，但可能带来过拟合的风险。C 值越高，间隔越小，表示要尽可能把所有数据正确分类。C 值越小，间隔越大——有些数据将无法正确分类。C 值低，过拟合训练数据的可能性就低，但是分类效果可能会相对较差 SV","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:9:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"N 元语法 N 元语法由一系列的 N 个为一组的对象组成，N 为每组对象的个数 Input: # 用N元语法分类 pipeline = Pipeline([('feature_extraction', CountVectorizer(analyzer='char', ngram_range=(3, 3))), # 长度为3的N元语法 ('classifier', grid) ]) scores = cross_val_score(pipeline, documents, classes, scoring='f1_macro') print(\"Score: {:.3f}\".format(np.mean(scores))) Output: Score: 0.813 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:9:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"安然邮件数据集 读取数据集 清洗数据 组装流水线 使用 F 值评估 # -*- coding: utf-8 -*- import os from email.parser import Parser # 邮件解析器 from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.utils import check_random_state # 随机状态实例 from sklearn.metrics import confusion_matrix from matplotlib import pyplot as plt import numpy as np import quotequail enron_data_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"maildir\") if __name__ == '__main__': p = Parser() def get_enron_corpus(num_authors=10, data_folder=enron_data_folder, min_docs_author=10, max_docs_author=100, random_state=None): random_state = check_random_state(random_state) # 随机对得到的邮箱列表进行排序 # os.listdir函数每次返回结果不一定相同，在使用该函数前先排序，从而保持返回结果的一致性 email_addresses = sorted(os.listdir(data_folder)) random_state.shuffle(email_addresses) documents = [] classes = [] author_num = 0 authors = {} # 遍历邮箱文件夹，查找它下面名字中含有“sent”的表示发件箱的子文件夹 for user in email_addresses: users_email_folder = os.path.join(data_folder, user) mail_folders = [os.path.join(users_email_folder, subfolder) for subfolder in os.listdir(users_email_folder) if \"sent\" in subfolder] try: # 获取子文件夹中的每一封邮件，跳过其中的子文件夹 authored_emails = [open(os.path.join(mail_folder, email_filename), encoding='cp1252').read() for mail_folder in mail_folders for email_filename in os.listdir(mail_folder)] except IsADirectoryError: continue # 获得至少十封邮件 if len(authored_emails) \u003c min_docs_author: continue # 最多获取前100封邮件 if len(authored_emails) \u003e max_docs_author: authored_emails = authored_emails[:max_docs_author] # 解析邮件，获取邮件内容 contents = [p.parsestr(email)._payload for email in authored_emails] documents.extend(contents) # 将发件人添加到类列表中，每封邮件添加一次 classes.extend([author_num] * len(authored_emails)) # 记录收件人编号，再把编号+1 authors[user] = author_num author_num += 1 # 收件人数量达到设置的值跳出循环 if author_num \u003e= num_authors or author_num \u003e= len(email_addresses): break # 返回邮件数据集以及收件人字典 return documents, np.array(classes), authors documents, classes, authors = get_enron_corpus(data_folder=enron_data_folder, random_state=14) # 移除邮件的回复信息 def remove_replies(email_contents): r = quotequail.unwrap(email_contents) if r is None: return email_contents if 'text_top' in r: return r['text_top'] # 字典r中存在text_top，返回它的值 elif 'text' in r: return r['text'] return email_contents documents = [remove_replies(document) for document in documents] parameters = {'kernel': ('linear', 'rbf'), 'C': [1, 10]} svr = SVC() grid = GridSearchCV(svr, parameters) pipeline = Pipeline([('feature_extraction', CountVectorizer(analyzer='char', ngram_range=(3, 3))), ('classifier', grid) ]) scores = cross_val_score(pipeline, documents, classes, scoring='f1_macro') print(\"Score: {:.3f}\".format(np.mean(scores))) Output: Score: 0.664 从流水线中获得最好的参数组合 Input: training_documents, test_documents, y_train, y_test = train_test_split(documents, classes, random_state=14) pipeline.fit(training_documents, y_train) y_pred = pipeline.predict(test_documents) print(pipeline.named_steps['classifier'].best_params_) Output: {'C': 10, 'kernel': 'rbf'} 绘制混淆矩阵查看分类情况 Input: cm = confusion_matrix(y_test, y_pred) cm = cm / cm.astype(np.float).sum(axis=1) sorted_authors = sorted(authors.keys(), key=lambda x: authors[x]) plt.figure(figsize=(20, 20)) plt.imshow(cm, cmap='Blues') tick_marks = np.arange(len(sorted_authors)) plt.xticks(tick_marks, sorted_authors) plt.yticks(tick_marks, sorted_authors) plt.ylabel('Actual') plt.xlabel('Predicted') plt.show() Output: ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:9:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第十章 这两天在鼓捣 jupyterlab，一开始在服务器上建了一个 lab 环境，可每次连接都要登上几分钟，不知道是服务器 CPU 不行还是网络不行。然后又在本地鼓捣，在 debian 装 nodejs 和 npm 的时候把系统依赖搞崩了，于是狠下心来重装了电脑。。。发生的事情太多，心累。。 昨天重装了 Ubuntu，搞了下美化，安装了必须的软件（别说 Ubuntu 还挺好用，真香） 我保证这是最后一句吐槽了，一定 本章介绍如何对新闻语料进行聚类，以发现其中的趋势和主题。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"获取新闻文章 这一章的数据集是从 reddit 获得的网页链接，reddit 的 app 审核机制不是很严格(?)因此我终于拿到了墙外的 api，使用 requests 下载又费了一番功夫，使用书上源码的 url 下载总是 403 错误，研究了好半天 reddit 的 api，发现 reddit 的 url 改成了(new, top, …)，修改之后总算完成了链接的索引 Input: # get_links.py # -*- coding: utf-8 -*- import json import os import requests import getpass import time # 需要的一些凭证 CLIENT_ID = \"xxxxxxxxxxx\" CLIENT_SECRET = \"xxxxxxxxxxx\" USER_AGENT = \"python:xxxxxxxxx (by /u/xxxxxxxxx)\" USERNAME = \"xxxxxxxx\" PASSWORD = \"xxxxxxxxxxxxxx\" # requests使用代理 proxies = {\"http\": \"socks5://xxxxxx\", \"https\": \"socks5://xxxxxx\"} def login(username, password): if password is None: password = getpass.getpass( \"Enter reddit password for user {}: \".format(username) ) headers = {\"User-Agent\": USER_AGENT} # 使用凭据设置身份验证对象 client_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET) post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password} response = requests.post( \"https://www.reddit.com/api/v1/access_token\", proxies=proxies, auth=client_auth, data=post_data, headers=headers, ) return response.json() if __name__ == \"__main__\": # 调用login获取token # token = login(USERNAME, PASSWORD) # print(token) token = { \"access_token\": \"xxxxxxxxxxxxxxxxxxxxxxxx\", \"token_type\": \"xxxxx\", \"expires_in\": 3600, \"scope\": \"*\", } def get_links(subreddit, token, n_pages=5): # 存放链接信息 stories = [] after = None for page_number in range(n_pages): # 进行调用之前等待，以避免超过API限制 print(\"等待2s...\") time.sleep(2) # 设置标头进行调用 headers = { \"Authorization\": \"bearer {}\".format(token[\"access_token\"]), \"User-Agent\": USER_AGENT, } # top为最热链接，这里也可以换成new url = \"https://oauth.reddit.com/r/{}/top?limit=100\".format(subreddit) if after: url += \"\u0026after={}\".format(after) while True: try: response = requests.get( url, proxies=proxies, headers=headers, timeout=10 ) result = response.json() # 获取下一个循环的cursor after = result[\"data\"][\"after\"] except: print(\"requests出错等待...\") time.sleep(2) else: break # 将所有新闻项添加到story列表中 for story in result[\"data\"][\"children\"]: stories.append( ( story[\"data\"][\"title\"], story[\"data\"][\"url\"], story[\"data\"][\"score\"], ) ) return stories stories = get_links(\"worldnews\", token) base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \"raw\") # 这里我将所有的链接都存在了文件里，因为获取这些网站的内容要很久 with open(os.path.join(base_folder, \"stories2.txt\"), \"w\") as f: for link in stories: f.write(json.dumps(list(link))) f.write(\"\\n\") ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"从网站抽取文本 api/top 总共有 500 个网站，我又获取了 api/new 的 490 个，总共下载了半个小时，失败了 300。。。 最后成功下载的网站数为 365 # get_data.py # -*- coding: utf-8 -*- import hashlib import os import requests import json proxies = {\"http\": \"socks5://xxxxxxxxxxxx\", \"https\": \"socks5://xxxxxxxxxxxxx\"} if __name__ == \"__main__\": base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \"raw\") # 读取链接数据 with open(os.path.join(base_folder, \"stories1.txt\"), \"r\") as f: temp = f.readlines() stories = [] for l in temp: stories.append(json.loads(l)) # 获取网页内容 number_errors = 0 for title, url, score in stories: print(url) output_filename = hashlib.md5(url.encode()).hexdigest() fullpath = os.path.join(data_folder, output_filename + \".txt\") try: response = requests.get(url, proxies=proxies, timeout=10) data = response.text with open(fullpath, \"w\") as outf: outf.write(data) except Exception as e: number_errors += 1 # 输出出错数量 print(\"出错：{}\".format(number_errors)) 下载下来的网页全是 html 文件，要从中提取出有用的信息，这里使用较为通用的 lxml 库，其它处理 html 的库还有 BeautifulSoup 等。 # get_content.py # -*- coding: utf-8 -*- import os from lxml import html, etree if __name__ == \"__main__\": base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \"raw\") # 输出变成纯文本文件的路径 text_output_folder = os.path.join(base_folder, \"textonly\") filenames = [ os.path.join(data_folder, filename) for filename in os.listdir(data_folder) ] # 存放不可能包含新闻内容的节点 skip_node_types = [\"script\", \"head\", \"style\", etree.Comment] # 把html文件解析成lxml对象 def get_text_from_file(filename): with open(filename, \"r\") as inf: html_tree = html.parse(inf) return get_text_from_node(html_tree.getroot()) # 抽取子节点中的文本内容，最后返回拼接在一起的所有子节点的文本 def get_text_from_node(node): if len(node) == 0: # 没有子节点，直接返回内容 if node.text: return node.text else: return \"\" else: # 有子节点，递归调用得到内容 results = ( get_text_from_node(child) for child in node if child.tag not in skip_node_types ) result = str.join(\"\\n\", (r for r in results if len(r) \u003e 1)) # 检查文本长度 if len(result) \u003e= 100: return result else: return \"\" for filename in os.listdir(data_folder): text = get_text_from_file(os.path.join(data_folder, filename)) with open(os.path.join(text_output_folder, filename), \"w\") as outf: outf.write(text) ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"新闻语料聚类 k-means 算法 k-means 聚类算法迭代寻找最能够代表数据的聚类质心点。算法开始时使用从训练数据中随机选取的几个数据点作为质心点。k-means 中的 k 表示寻找多少个质心点，同时也是算法将会找到的簇的数量。步骤： 为每一个数据点分配簇标签 为每个个体设置一个标签，将它和最近的质心点联系起来，标签相同的个体属于同一个簇 更新各簇的质心点 每次更新质心点时，所有质心点将会小范围移动，这会轻微改变每个数据点在簇内的位置，从而引发下一次迭代时质心点的变动 # -*- coding: utf-8 -*- import os from sklearn.cluster import KMeans # TfidfVectorizer向量化工具，根据词语出现在多少篇文章中，对词语计数进行加权 # 出现在较多文档中的词语权重较低（用文档集数量除以词语出现在的文档的数量，然后取对数） from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.pipeline import Pipeline from collections import Counter from scipy.sparse import csr_matrix # 稀疏矩阵 import numpy as np from scipy.sparse.csgraph import minimum_spanning_tree # 计算最小生成树MST from scipy.sparse.csgraph import connected_components # 连通分支 from sklearn.base import BaseEstimator, ClusterMixin from sklearn.cluster import MiniBatchKMeans from sklearn.feature_extraction.text import HashingVectorizer base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \"raw\") text_output_folder = os.path.join(base_folder, \"textonly\") if __name__ == \"__main__\": # 分簇的数量 n_clusters = 10 pipeline = Pipeline( [ (\"feature_extraction\", TfidfVectorizer(max_df=0.4)), # 特征抽取，忽略出现在40%文档中的词语（删除功能词） (\"clusterer\", KMeans(n_clusters=n_clusters)), # 调用k-means算法 ] ) documents = [ open(os.path.join(text_output_folder, filename)).read() for filename in os.listdir(text_output_folder) ] # 不为fit函数指定目标类别，进行训练 pipeline.fit(documents) # 使用训练过的算法预测 # labels包含每个数据点的簇标签，标签相同的数据点属于同一个簇，标签本身没有含义 labels = pipeline.predict(documents) # 使用Counter类查看每个簇的数据点数量 c = Counter(labels) for cluster_number in range(n_clusters): print( \"Cluster {}contains {}samples\".format(cluster_number, c[cluster_number]) ) Output: Cluster 0 contains 2 samples Cluster 1 contains 4 samples Cluster 2 contains 1 samples Cluster 3 contains 2 samples Cluster 4 contains 329 samples Cluster 5 contains 7 samples Cluster 6 contains 2 samples Cluster 7 contains 13 samples Cluster 8 contains 3 samples Cluster 9 contains 2 samples 聚类分析主要是探索性分析，因此很难有效地评估结果的好坏，如果有测试集，可以对其分析来评价效果 对于 k-means 算法，寻找新质心点的标准是，最小化每个数据点到最近质心点的距离。这叫作算法的惯性权重（inertia），任何经过训练的 KMeans 实例都有该属性 下面将 n_clusters 依次取 2 到 20 之间的值，每取一个值，k-means 算法运行 10 次。每次运行算法都记录惯性权重。 Input: # 惯性权重，这个值没有意义，但是可以用来确定n_clusters print(pipeline.named_steps[\"clusterer\"].inertia_) print() inertia_scores = [] n_clusters_values = list(range(2, 20)) for n_clusters in n_clusters_values: # 当前的惯性权重组 cur_inertia_scores = [] X = TfidfVectorizer(max_df=0.4).fit_transform(documents) for i in range(10): km = KMeans(n_clusters=n_clusters).fit(X) cur_inertia_scores.append(km.inertia_) inertia_scores.append(cur_inertia_scores) print(\"{}: {}\".format(n_clusters, np.mean(cur_inertia_scores))) Output: 291.45747555507467 2 : 310.72961350285766 3 : 305.7904332223444 4 : 302.18859768191396 5 : 300.28785590112705 6 : 297.48005120447067 7 : 294.226862724111 8 : 292.340968109182 9 : 291.18707107605024 10 : 289.46981977256536 11 : 287.9333326469133 12 : 285.0561596766078 13 : 284.33745019948356 14 : 282.71178879028537 15 : 280.94991762471807 16 : 279.9555799316599 17 : 278.3825941905214 18 : 274.94616060558434 19 : 275.0297854253871 将上表作图 Input: import plotly data = plotly.graph_objs.Scatter( x=list(range(18)), y=[ 310.73, 305.79, 302.18, 300.28, 297.48, 294.22, 292.34, 291.18, 289.46, 287.93, 285.05, 284.33, 282.71, 280.94, 279.95, 278.38, 274.94, 275.02, ], ) fig = plotly.graph_objs.Figure(data) fig.show() Output: 根据上图可以发现在 n_clusters=9 和 15 时拐点比较明显，这里为了方便计算，我们按照书上选择 6 Input: # 设置n_clusters值为6， 重新运行算法 n_clusters = 6 pipeline = Pipeline( [ (\"feature_extraction\", TfidfVectorizer(max_df=0.4)), (\"clusterer\", KMeans(n_clusters=n_clusters)), ] ) pipeline.fit(documents) labels = pipeline.predict(documents) # 获取特征的所对应的词 terms = pipeline.named_steps[\"feature_extraction\"].get_feature_names() # 统计6个簇中每个簇的元素个数 c = Counter(labels) for cluster_number in range(n_clusters): print( \"Cluster {}contains {}samples\".format(cluster_number, c[cluster_number]) ) print(\" Most important terms\") centroid = pipeli","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"聚类融合 聚类算法也可以进行融合，这样做的主要原因是，融合后得到的算法能够平滑算法多次运行所得到的不同结果。多次运行 k-means 算法得到的结果因最初选择的质心点不同而不同。多次运行算法，综合考虑所得到的多个结果，可以减少波动。聚类融合方法还可以降低参数选择对最终结果的影响。大多数聚类算法对参数选择很敏感,参数稍有不同将带来不同的聚类结果 最基本的融合方法是对数据进行多次聚类，每次都记录各个数据点的簇标签。然后计算每两个数据点被分到同一个簇的次数。这就是证据累积算法（Evidence Accumulation Clustering，EAC）的精髓 第一步，使用 k-means 等低水平的聚类算法对数据集进行多次聚类，记录每一次迭代两个数据点出现在同一簇的频率，将结果保存到共协矩阵（coassociation）中 第二步，使用另外一种聚类算法——分级聚类对第一步得到的共协矩阵进行聚类分析。分级聚类一个比较有趣的特性是，它等价于寻找一棵把所有节点连接到一起的树，并把权重低的边去掉。 Input: # 遍历所有标签，记录具有相同标签的两个数据点的位置，创建共协矩阵 def create_coassociation_matrix(labels): rows = [] cols = [] # labels种类 unique_labels = set(labels) for label in unique_labels: # 找出label值相同的数据点 indices = np.where(labels == label)[0] # 记录他们的位置：如1、3点的数据均为1，即1和1相同，1和3相同，3和1相同，3和3相同 # 行和列均增加了4个indices*indices个数字 for index1 in indices: for index2 in indices: rows.append(index1) cols.append(index2) # 返回给定shape和type的值全为1的矩阵 data = np.ones((len(rows),)) # 创建稀疏矩阵满足：a[rows[k], cols[k]] = data[k] return csr_matrix((data, (rows, cols)), dtype=\"float\") # 使用标签生成共协矩阵 C = create_coassociation_matrix(labels) # 这里书上说多输入几次C看看结果，我没有用notebook，但是使用print输出是一样的，因此没有搞懂书上的含义 print(C) print((365 ** 2 - create_coassociation_matrix(labels).nnz) / 365 ** 2) mst = minimum_spanning_tree(C) mst = minimum_spanning_tree(-C) pipeline.fit(documents) labels2 = pipeline.predict(documents) C2 = create_coassociation_matrix(labels2) C_sum = (C + C2) / 2 mst = minimum_spanning_tree(-C_sum) # 删除低于阈值的边 mst.data[mst.data \u003e -1] = 0 number_of_clusters, labels = connected_components(mst) Output: (0, 0) 1.0 (0, 1) 1.0 (0, 2) 1.0 (0, 3) 1.0 (0, 4) 1.0 (0, 5) 1.0 (0, 6) 1.0 (0, 7) 1.0 (0, 8) 1.0 (0, 9) 1.0 (0, 10) 1.0 (0, 11) 1.0 (0, 12) 1.0 (0, 13) 1.0 : : (364, 350) 1.0 (364, 351) 1.0 (364, 352) 1.0 (364, 353) 1.0 (364, 354) 1.0 (364, 355) 1.0 (364, 356) 1.0 (364, 357) 1.0 (364, 358) 1.0 (364, 359) 1.0 (364, 360) 1.0 (364, 361) 1.0 (364, 362) 1.0 (364, 363) 1.0 (364, 364) 1.0 0.11092512666541565 从图的理论角度看，生成树为所有节点都连接到一起的图。最小生成树（Minimum Spanning Tree，MST）即总权重最低的生成树。结合我们的应用来讲，图中的节点对应数据集中的个体，边的权重对应两个顶点被分到同一簇的次数——也就是共协矩阵所记录的值。 矩阵 C 中，值越高表示一组数据点被分到同一簇的次数越多——这个值表示相似度。相反，minimum_spanning_tree 函数的输入为距离，高的值反而表示相似度越小。这里又用到了一次取反 Input: mst = minimum_spanning_tree(C) # 对C取反再计算最小生成树 mst = minimum_spanning_tree(-C) # 创建额外的标签 pipeline.fit(documents) labels2 = pipeline.predict(documents) C2 = create_coassociation_matrix(labels2) C_sum = (C + C2) / 2 # 生成阈值不全为1和0的最小生成树 mst = minimum_spanning_tree(-C_sum) # 删除低于阈值的边 mst.data[mst.data \u003e -1] = 0 number_of_clusters, labels = connected_components(mst) print(number_of_clusters) print(labels) Output: 2 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0, 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0, 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] k-means 算法不考虑特征的权重，它寻找的是圆形簇（circular clusters） 证据累积算法的工作原理为重新把特征映射到新空间，每次运行 k-means 算法都相当于使用转换器对特征进行一次转换。 证据累积算法只关心数据点之间的距离而不是它们在原来特征空间的位置。对于没有规范化过的特征，仍然存在问题。因此，特征规范很重要，无论如何都要做（我们用 tf-idf 规范特征值，从而使特征具有相同的值域） Input: # 创建证据累积算法类 class EAC(BaseEstimator, ClusterMixin): def __init__( self, n_clusterings=10, cut_threshold=0.5, n_clusters_range=(3, 10) ): self.n_clusterings = n_clusterings # k-means算法运行次数 self.cut_threshold = cut_threshold # 用来删除边的阈值 self.n_clusters_range = n_clusters_range # 每次运行k-means算法要找到的簇的数量 def fit(self, X, y=None): # 进行指定次数的共协矩阵累加 C = sum( ( create_coassociation_matrix(self._single_clustering(X)) for _ in range(self.n_clusterings) ) ) mst = minimum_spanning_tree(-C) mst.data[mst.data \u003e -self.cut_threshold] = 0 self.n_components, self.labels_ = connected_components(mst) return self # 进行一次集群 def _single_clustering(self, X): # 在给定范围中随机选择一个集群数 n_clusters = np.random.randint(*self.n_clusters_range) km = KMeans(n_clusters=n_clusters) # 返回由k-means计算得到的簇标签 return km.fit_predict(X) pipeline = Pipeline( [(\"feature_extraction\", TfidfVectorizer(max_df=0.4)), (\"clusterer\", EAC())] ) pipeline.fit(documents) number_of_clusters, labels = ( pipeline[\"clusterer\"].n_components, pipeline[\"clusterer\"].labels_, ) print(numb","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:4","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"线上学习 线上学习是指用新数据增量地改进模型。支持线上学习的算法可以先用一条或少量数据进行训练，随着更多新数据的添加，更新模型。 线上学习与流式学习（streaming-based learning）有关，但有几个重要的不同点。线上学习能够重新评估先前创建模型时所用到的数据，而对于后者，所有数据都只使用一次。 scikit-learn 提供了 MiniBatchKMeans 算法，可以用它来实现线上学习功能。这个类实现了 partial_fit 函数，接收一组数据，更新模型。调用fit()将会删除之前的训练结果，重新根据新数据进行训练。 Input: # 使用TfIDFVectorizer从数据集中抽取特征，创建矩阵X n_clusters = 6 vec = TfidfVectorizer(max_df=0.4) X = vec.fit_transform(documents) mbkm = MiniBatchKMeans(random_state=14, n_clusters=3) batch_size = 10 # 随机从X矩阵中选择数据，模拟来自外部的新数据 for iteration in range(int(X.shape[0] / batch_size)): start = batch_size * iteration end = batch_size * (iteration + 1) mbkm.partial_fit(X[start:end]) # 获取数据集聚类结果 labels = mbkm.predict(X) c = Counter(labels) for cluster_number in range(n_clusters): print( \"Cluster {}contains {}samples\".format(cluster_number, c[cluster_number]) ) Output: Cluster 0 contains 2 samples Cluster 1 contains 362 samples Cluster 2 contains 1 samples Cluster 3 contains 0 samples Cluster 4 contains 0 samples Cluster 5 contains 0 samples 由于 TfIDFVectorizer 不是在线算法，因此无法在流水线中使用 为了解决这个问题，我们使用 HashingVectorizer 类，它巧妙地使用散列算法极大地降低了计算词袋模型所需的内存开销，将数据的内容转换成散列值 Input: class PartialFitPipeline(Pipeline): def partial_fit(self, X, y=None): Xt = X # 经过最后一步之前的所有步转换 for name, transform in self.steps[:-1]: Xt = transform.transform(Xt) #　调用MiniBatchKMeans的partial_fit函数 return self.steps[-1][1].partial_fit(Xt, y=y) pipeline = PartialFitPipeline( [ (\"feature_extraction\", HashingVectorizer()), (\"clusterer\", MiniBatchKMeans(random_state=14, n_clusters=3)), ] ) batch_size = 10 for iteration in range(int(len(documents) / batch_size)): start = batch_size * iteration end = batch_size * (iteration + 1) pipeline.partial_fit(documents[start:end]) labels = pipeline.predict(documents) c = Counter(labels) for cluster_number in range(n_clusters): print( \"Cluster {}contains {}samples\".format(cluster_number, c[cluster_number]) ) Output: Cluster 0 contains 4 samples Cluster 1 contains 76 samples Cluster 2 contains 285 samples Cluster 3 contains 0 samples Cluster 4 contains 0 samples Cluster 5 contains 0 samples 这一章的内容比较多，也学了挺久，虽然中间结果跟书上的差的有点多。。可能是因为最近新冠肺炎吧(￣_,￣ ) ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:10:5","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第十一章 本章介绍如何使用深度神经网络识别图像中的物体 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:11:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"深度神经网络 深度神经网络和第 8 章中的基本神经网络的差别在于规模大小。至少包含两层隐含层的神经网络被称为深度神经网络。神经网络的核心其实就是一系列矩阵运算，两个网络之间连接的权重可以用矩阵来表示。其中行表示前一层神经元，列表示后一层神经元，一个神经网络就可以用一组这样的矩阵来表示。除了神经元外，每层增加一个偏置项，它是一个特殊的神经元，永远处于激活状态，并且跟下一层的每一个神经元都有连接。 神经网络使用卷积层（一般来说，仅卷积神经网络包含该层）和池化层（pooling layer），池化层接收某个区域最大输出值，可以降低图像中的微小变动带来的噪音，减少（down-sample，降采样）信息量，这样后续各层所需工作量也会相应减少。 使用 Iris 数据集进行对比实验 Input: import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from keras.layers import Dense from keras.models import Sequential from matplotlib import pyplot as plt iris = load_iris() X = iris.data.astype(np.float32) y_true = iris.target.astype(np.int32) # 预处理数据集 y_onehot = OneHotEncoder().fit_transform(y_true.reshape(-1, 1)) y_onehot = y_onehot.astype(np.int64).todense() X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, random_state=14) input_layer_size, hidden_layer_size, output_layer_size = 4, 6, 3 # 隐含层 hidden_layer = Dense(output_dim=hidden_layer_size, input_dim=input_layer_size, activation='relu') # 输出层 output_layer = Dense(output_layer_size, activation='sigmoid') # 创建顺序模型 model = Sequential(layers=[hidden_layer, output_layer]) # 为训练神经网络配置模型 # 损失函数设置为均方误差，优化器设置为adam(亚当)即遵循原始文件中的默认参数，指定精度衡量标准 model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy']) # 当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一次epoch # 为模型训练固定的epoch（数据集上的迭代） # 输出模式。0不输出，1每个epoch一个进度条，2一行每个epoch。 history = model.fit(X_train, y_train, nb_epoch=100, verbose=2) # 记录了连续几个epoch的训练损失值和度量值，以及验证损失值和验证度量值(如果适用的话) history.history # 作图，绘制出epoch和loss关系图 plt.figure(figsize=(10, 10)) plt.plot(history.epoch, history.history['loss']) plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() # 为输入样本生成输出预测，计算是分批进行的 # 返回的是数值[0.9356668, 0.20588416, 0.00021186471],代表样本属于每个类别的概率 y_pred = model.predict(X_test) # 返回一串预测结果，样本属于哪一个类别 y_pred = model.predict_classes(X_test) y_pred = model.predict_classes(X_test) print(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred)) Output: precision recall f1-score support 0 1.00 1.00 1.00 17 1 1.00 0.08 0.14 13 2 0.40 1.00 0.57 8 accuracy 0.68 38 macro avg 0.80 0.69 0.57 38 weighted avg 0.87 0.68 0.62 38 重复上面的操作，这次运行 1000 步，对比实验结果 Input: hidden_layer = Dense(output_dim=hidden_layer_size, input_dim=input_layer_size, activation='relu') output_layer = Dense(output_layer_size, activation='sigmoid') model = Sequential(layers=[hidden_layer, output_layer]) model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy']) history = model.fit(X_train, y_train, nb_epoch=1000, verbose=False) plt.figure(figsize=(12, 8)) plt.plot(history.epoch, history.history['loss']) plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show(\"keras_on_iris_2.png\") y_pred = model.predict_classes(X_test) print(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred)) Output: precision recall f1-score support 0 1.00 1.00 1.00 17 1 1.00 1.00 1.00 13 2 1.00 1.00 1.00 8 accuracy 1.00 38 macro avg 1.00 1.00 1.00 38 weighted avg 1.00 1.00 1.00 38 从结果可以看出，经过 100 步训练的神经网络正确率达到了 68%，经过 1000 步训练后正确率达到了 100% 验证码识别实验 Input: import numpy as np from PIL import Image, ImageDraw, ImageFont from skimage import transform as tf from matplotlib import pyplot as plt from sklearn.utils import check_random_state from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from keras.layers import Dense from keras.models import Sequential from skimage.measure import label, regionprops def create_captcha(text, shear=0, size=(100, 30), scale=1): im = Image.new(\"L\", size, \"black\") draw = ImageDraw.Draw(im) font = ImageFont.truetype( \"/home/saltfish/Programming/Python/data_mining/ch11/FiraCode-Medium.otf\", 22 ) draw.text((0, 0), text, fill=1, font=font) image = np.array(im) affine_tf = tf.AffineTransform(shear=shear) image = tf.warp(image, affine_tf) image = image / image.max() shape = image.shape # Apply scale shapex, shapey = (","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:11:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"使用 GPU 优化 为了让我的 GPU 能跑程序，可费了我好大功夫，结果我这 960M 的 2G 内存还跑不了太大的程序/(ㄒ o ㄒ)/~~ 第 101 次想念我的台式机，可恶的病毒 配置的过程跟 TensorFlow 官网给的方法没啥区别，在这就不多说了（官网给出的 NVIDIA 显卡驱动版本是 430，我这里是 440，CUDA 版本是 10.2，依然能运行程序，可能只需要 development and runtime libraries 正确安装就行？） 使用 tensorflow 在执行 modle.compile() 的时候需要较长的时间，运行时的速度还是很快的 初次接触神经网络，不了解的东西太多了，还是先多做几个训练再说吧。。 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:11:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"应用 书上使用 CIFAR 图像数据集的代码太老了（原谅我太菜了解决不了依赖问题），因此我跟着 Tensorflow 官网的代码做完了这个实验 服装识别 Input: # -*- coding: utf-8 -*- from __future__ import absolute_import, division, print_function, unicode_literals import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt if __name__ == \"__main__\": # --------加载、了解、预处理数据集-------- fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = [ \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\", ] # 查看数据集 print( train_images.shape, # (60000，28，28) len(train_labels), # 60000 train_labels, # [9 0 0 ... 3 0 5] test_images.shape, # (10000, 28, 28) len(test_labels), # 10000 ) Output: (60000, 28, 28) 60000 [9 0 0 ... 3 0 5] (10000, 28, 28) 10000 Input: # 查看图像 plt.figure() plt.imshow(train_images[0]) plt.colorbar() plt.grid(False) plt.show() # 预处理标准化 train_images = train_images / 255.0 test_images = test_images / 255.0 # 查看数据集 plt.figure(figsize=(10, 10)) for i in range(25): plt.subplot(5, 5, i + 1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]]) plt.show() Output: Input: # --------建立模型-------- # 建立神经网络所需要模型的各层 # tf.keras.layers.Flatten将图像的格式从二维数组(28 * 28)转换为一维数组(28 * 28 = 784) # 可以将这个图层看作是图像中取消堆叠的像素行，并将它们排列起来 # 这个层没有参数需要学习; 它只是重新格式化数据。 # # 然后是两个稠密层（完全连接的层），中间一层有128个节点， # 最后一层返回长度为10的对数数组。每个神经元包含一个得分，指示当前图像对这一类的评分 model = keras.Sequential( [ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=\"relu\"), keras.layers.Dense(10), ] ) # --------编译模型-------- # 损失函数：这可以衡量训练期间模型的准确程度，希望最小化这个函数，以便将模型“引导”到正确的方向 # 优化器：如何基于它看到的数据和它的损失函数更新模型 # 指标：用于检测训练和测试步骤。下面的例子使用精确度，即正确分类的图像的分数 model.compile( optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"], ) # --------训练模型-------- model.fit(train_images, train_labels, epochs=10) # --------评估表现-------- test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print(\"\\nTest accuracy:\", test_acc) probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()]) # prediction是由10个数字组成的数组。它们表示模型对图像对应于10种不同衣服各自的置信度 predictions = probability_model.predict(test_images) print(predictions[0]) Output: pciBusID: 0000:02:00.0 name: GeForce GTX 960M computeCapability: 5.0 coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s Epoch 1/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4919 - accuracy: 0.8271 Epoch 2/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.3758 - accuracy: 0.8648 Epoch 3/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3346 - accuracy: 0.8770 Epoch 4/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3099 - accuracy: 0.8860 Epoch 5/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2927 - accuracy: 0.8927 Epoch 6/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2807 - accuracy: 0.8962 Epoch 7/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2655 - accuracy: 0.9010 Epoch 8/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2548 - accuracy: 0.9044 Epoch 9/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2440 - accuracy: 0.9095 Epoch 10/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2373 - accuracy: 0.9113 313/313 - 0s - loss: 0.3479 - accuracy: 0.8798 Test accuracy: 0.879800021648407 [1.3496768e-07 1.5826453e-10 1.7375668e-09 2.1999605e-10 5.5648923e-07 1.9829762e-03 1.9957926e-07 1.8424643e-04 9.3086570e-09 9.9783188e-01] 从输出可以看出 loss 函数正在逐渐减小，训练的准确率在不断的增加，这正是我们所要的 在训练集中的准确率为 91.1%， 而在测试集中只有 88%，这是出现了过拟合(overfitting)，关于过拟合的证明和避免过拟合的方法，等过几天单独写一个 post 学习一下 定义两个函数用来绘图，更直观地看出预测结果 Input: # --------验证模型-------- # 制作图表来观察十个类别预测的完整集合 def plot_image(i, predictions_array, tru","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:11:3","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"第十二章 本章主要介绍了 python 使用 MapReduce 来进行大数据处理 ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:12:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"MapReduce 例子 MapReduce 主要分为两步：映射（Map）和规约（Reduce） Input: # -*- coding: utf-8 -*- from collections import defaultdict from sklearn.datasets import fetch_20newsgroups from joblib import Parallel, delayed import timeit # 计算documents中的单词出现词素 def map_word_count(document_id, document): counts = defaultdict(int) for word in document.split(): counts[word] += 1 for word in counts: yield word, counts[word] # 将map得到的结果，即每篇文章中单词出现的次数整合起来 # 如文章1中单词\"apple\"出现了2次，文章2中单词\"apple\"出现了5次，则返回结果为[\"apple\":[2,5],...] def shuffle_words(results_generators): records = defaultdict(list) # 遍历每一篇文章 for results in results_generators: # 遍历每个单词 for word, count in results: records[word].append(count) # 每次生成一个单词 for word in records: yield word, records[word] # 将单词所对应的列表叠加起来得到单词出现次数 def reduce_counts(word, list_of_counts): return word, sum(list_of_counts) if __name__ == \"__main__\": dataset = fetch_20newsgroups(subset=\"train\") documents = dataset.data start = timeit.default_timer() # 生成器，输出(单词，出现次数的键值对) map_results = map(map_word_count, range(len(documents)), documents) shuffle_results = shuffle_words(map_results) reduce_results = [ reduce_counts(word, list_of_counts) for word, list_of_counts in shuffle_results ] end = timeit.default_timer() print(reduce_results[:5]) print(len(reduce_results)) print(\"----------\", str(end - start)) Output: pydev debugger: process 7540 is connecting [('From:', 11536), ('lerxst@wam.umd.edu', 2), (\"(where's\", 3), ('my', 7679), ('thing)', 9)] 280308 ---------- 4.087287616999674 接下来导入 joblib 库，将 map 工作分配出去，使用 4 个进程进行计算 Input: def map_word_count(document_id, document): counts = defaultdict(int) for word in document.split(): counts[word] += 1 return list(counts.items()) start = timeit.default_timer() map_results = Parallel(n_jobs=4)( delayed(map_word_count)(i, document) for i, document in enumerate(documents) ) shuffle_results = shuffle_words(map_results) reduce_results = [ reduce_counts(word, list_of_counts) for word, list_of_counts in shuffle_results ] end = timeit.default_timer() print(reduce_results[:5]) print(len(reduce_results)) print(\"----------\", str(end - start)) Output: pydev debugger: process 7566 is connecting pydev debugger: process 7556 is connecting pydev debugger: process 7552 is connecting pydev debugger: process 7561 is connecting [('From:', 11536), ('lerxst@wam.umd.edu', 2), (\"(where's\", 3), ('my', 7679), ('thing)', 9)] 280308 ---------- 3.5958340090001 可以看到运行时间确实减少了（数据集太少了效果不怎么样） ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:12:1","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"MapReduce 应用 书中使用 blogs 的数据集，有 19320 个人的 blog 信息 手头上的电脑配置有点不行了，跑的属实费劲，就放在这了（其实是迫不及待想去做做 tensorflow 的练习了嘿嘿） ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:12:2","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"接下来的方向 书中根据每一章的内容，都有更进一步的实践，我会选几个单独做一下练习 Done ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:13:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"参考链接 python-3.8.2-doc pandas-doc numpy-doc scikit-learn tensorflow ","date":"2020-03-09","objectID":"/posts/2020-03-09-data-mining/:14:0","tags":["data science"],"title":"数据挖掘入门与实践","uri":"/posts/2020-03-09-data-mining/"},{"categories":["python"],"content":"Python中的字符串和编码问题","date":"2020-03-06","objectID":"/posts/2020-03-06-python-encode/","tags":null,"title":"Python 字符串和编码","uri":"/posts/2020-03-06-python-encode/"},{"categories":["python"],"content":"编码 编码是数据从一种格式变为另一种格式的过程。通过编码，我们可以把数据以不同的格式保存和转移。 Unicode 把所有语言都统一到一套编码里，这样就不会再有乱码问题了。 ","date":"2020-03-06","objectID":"/posts/2020-03-06-python-encode/:1:0","tags":null,"title":"Python 字符串和编码","uri":"/posts/2020-03-06-python-encode/"},{"categories":["python"],"content":"Python3 字符串类型 Unicode –encode(“utf-8”)-\u003e UTF-8 编码的二进制数据 UTF-8 编码的二进制数据 –decode(“utf-8”)-\u003e Unicode 示例 1： s1 = \"中文\" s2 = s1.encode(\"utf-8\") print(type(s1), s1) print(type(s2), s2) output 1: \u003cclass 'str'\u003e 中文 \u003cclass 'bytes'\u003e b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 文件中存储的都是 byte 这样一个一个二进制数，所以在将 str 存入文件或从文件读取内容时需要指明编码类型。 ","date":"2020-03-06","objectID":"/posts/2020-03-06-python-encode/:2:0","tags":null,"title":"Python 字符串和编码","uri":"/posts/2020-03-06-python-encode/"},{"categories":["python"],"content":"经验总结 写 python 程序的时候，把编码和解码操作放在界面的最外围来做，程序的核心部分使用 Unicode 字符类型(Python3 中的 str)。 ——Effective+Python 在写文件时注明编码类型 with open(file_path, 'w',encode='utf-8') as f: f.write(data) 在使用 requests 库获取响应信息时，指明编码类型 def send_req(url): headers = {} req = requests.get(url, headers) req.encoding = \"utf-8\" if req.ok: print(req.text) return req.text else: return Exception(\"访问失败\") base64 编码 Base64 是网络上最常见的用于传输 8Bit 字节码的编码方式之一，可用于在 HTTP 环境下传递较长的标识信息。采用 Base64 编码具有不可读性，需要解码后才能阅读。 将文件转换成 base64 编码形式进行传输。 import base64 with open(file_path, 'rb') as f: res = base64.b64encode(f.read()) python3 中 str 和 bytes 互换函数 def to_str(bytes_or_str): if isinstance(bytes_or_str, bytes): value = bytes_or_str.decode('utf-8') else: value = bytes_or_str return value def to_bytes(bytes_or_str): if isinstance(bytes_or_str, str): value = bytes_or_str.encode('utf-8') else: value = bytes_or_str return value 参考资料： base64 编码 Effective+Python ","date":"2020-03-06","objectID":"/posts/2020-03-06-python-encode/:3:0","tags":null,"title":"Python 字符串和编码","uri":"/posts/2020-03-06-python-encode/"},{"categories":["technique"],"content":"分割线 --- ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:1:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"标题 # 一级标题 ## 二级标题 ### 三级标题 ###### 六级标题 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:2:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"斜体文字 _斜体_ 斜体 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:3:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"加粗文字 **加粗** 加粗 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:4:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"删除线 ~~删除内容~~ 删除内容 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:5:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"段落和换行 新一段落需要空一行 新的一段 或者在最后加上两个空格 换行 新一段落需要空一行 新的一段 或者在最后加上两个空格 换行 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:6:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"列表 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"基本用法 - 列表 - 列表 - 列表 1. 列表 2. 列表 3. 列表 - [ ] 列表 - [ ] 列表 - [ ] 列表 列表 列表 列表 列表 列表 列表 列表 列表 列表 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:1","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"多级列表 - 一级列表 - 二级列表 - 一级列表 一级列表 二级列表 一级列表 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:2","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"列表中分段 - 项目一，段落一 项目一，段落二 项目一，段落一 项目一，段落二 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:3","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"列表中换行 - 项目二，第一行 项目二，第二行 项目二，第一行 项目二，第二行 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:7:4","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"引用 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"基本用法 \u003e 我亦飘零久，十年来，深恩负尽，死生师友。 ——顾贞观 我亦飘零久，十年来，深恩负尽，死生师友。 ——顾贞观 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:1","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"多级引用 \u003e 引用 1 \u003e \u003e \u003e 引用 2 引用 1 引用 2 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:2","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"引用中分段 \u003e 引用 \u003e \u003e 引用 引用 引用 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:3","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"引用中换行 \u003e 引用 \u003e 引用 引用 引用 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:8:4","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"链接 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:9:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"文内链接 这是一个文内链接的[例子](http://example.com/ \"鼠标悬浮此处显示的标题\")。 [这个](http://example.net/)链接在鼠标悬浮时没有标题。 [这个](/about/)链接是本地资源。 这是一个文内链接的例子。 这个链接在鼠标悬浮时没有标题。 这个链接是本地资源。 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:9:1","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"引用链接 这是一个引用链接的[例子][id]。 [id]: http://example.com/ \"鼠标悬浮标题\" 这是一个引用链接的例子。 注意，这里的 id 没有大小写区分，如果省略 id，则前面方括号的内容会被用作 id。 我常用的网站包括[Google][1]，[Yahoo][]和[MSN][]。 [1]: http://google.com/ \"Google\" [yahoo]: http://search.yahoo.com/ \"Yahoo Search\" [msn]: http://search.msn.com/ \"MSN Search\" 我常用的网站包括Google，Yahoo和MSN。 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:9:2","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"图片 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:10:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"基本用法 ![地铁](/img/post-bg-2015.jpg \"图片下方描述\") 图片下方描述地铁 \" 图片下方描述 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:10:1","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"base64 图片数据 ![地铁][base64str] [base64str]: data:image/jpg;base64,xxxxxxxxxxxxxxx... \"图片下方描述\" 图片下方描述地铁 \" 图片下方描述 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:10:2","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"代码块 ```python print('Hello,World') ``` print('Hello,World') ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:11:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"链接 \u003chttps://www.baidu.com/\u003e https://www.baidu.com/ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:12:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"转义 在不希望符号被当成 markdown 标识符时，用\\转义 \\_不是斜体\\_ _不是斜体_ ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:13:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"Karmdowm 扩展 表格 | 左对齐 | 中间对齐 | 右对齐 | | :----- | :------: | -----: | | 左 1 | 中 1 | 右 1 | | 左 2 | 中 2 | 右 3 | 左对齐 中间对齐 右对齐 左 1 中 1 右 1 左 2 中 2 右 3 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:14:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"脚注 请参阅脚注 1. [^1] [^1]: 脚注 1 内容。 # 出现在文末 请参阅脚注 2. [^2] [^2]: 脚注 2 内容。 # 出现在文末 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:15:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"HTML 扩展 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"下划线 \u003cu\u003e下划内容\u003c/u\u003e 下划内容 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:1","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"上标 S = πr\u003csup\u003e2\u003c/sup\u003e S = πr2 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:2","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"下标 Water: H\u003csub\u003e2\u003c/sub\u003eO Water: H2O ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:3","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"首行缩进 \u0026emsp;\u0026emsp;开始内容  开始内容 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:4","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"内部跳转 点此[标签](#j1)跳转。 \u003ca name=\"锚点\" id=\"j1\" href=\"https://saltfishpr.github.io/\"\u003e\u003c/a\u003e 点此标签跳转。 id 要匹配， name 和 href 都不是必须的 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:5","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"插入视频 \u003cdiv\u003e \u003ca href=\"//player.bilibili.com/player.html?aid=93178052\u0026cid=159088790\u0026page=1\" target=\"_blank\"\u003e\u003cimg src=\"封面图片路径\" alt=\"没有图片时显示此文字\" width=\"100%\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" /\u003e\u003c/a\u003e \u003cscript\u003e var em = document.getElementById(\"video-iframe\"); console.log(em.clientWidth); em.height = em.clientWidth * 0.75 \u003c/script\u003e \u003c/div\u003e var em = document.getElementById(\"video-iframe\"); console.log(em.clientWidth); em.height = em.clientWidth * 0.75 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:6","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"注释 [^_^]: # (注释，不会在浏览器中显示。) \u003cdiv style='display: none'\u003e 注释内容 \u003c/div\u003e 注释内容 \u003c!-- 多段 注释， 不会在浏览器中显示。 --\u003e ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:16:7","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":["technique"],"content":"参考资料 https://www.jianshu.com/p/d7d6da4b7c60#fnref1 https://guides.github.com/features/mastering-markdown/ emoji 目录 ","date":"2020-03-05","objectID":"/posts/2020-03-05-markdown-cheat-sheet/:17:0","tags":["markdown"],"title":"Markdown 常用写法大全","uri":"/posts/2020-03-05-markdown-cheat-sheet/"},{"categories":null,"content":"Hello，世界!","date":"2020-03-03","objectID":"/posts/2020-03-03-first-blog/","tags":null,"title":"第一篇博客","uri":"/posts/2020-03-03-first-blog/"},{"categories":null,"content":"博客，斯达托！ ","date":"2020-03-03","objectID":"/posts/2020-03-03-first-blog/:0:0","tags":null,"title":"第一篇博客","uri":"/posts/2020-03-03-first-blog/"},{"categories":null,"content":"👋 你好，我是硕。 😎 技能：主要是 Go, 会一些 Flutter, 很久没写 Python。 🌱 专注于 Go 开发。 对游戏制作、数据挖掘、AI、IOT 等新事物也感兴趣。 📫 联系方式 526191197@qq.com / saltfishpr@gmail.com ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"}]