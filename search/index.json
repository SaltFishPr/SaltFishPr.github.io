[{"content":"Ultimate Go Go è¯­è¨€åŸºç¡€å­¦ä¹ \nå˜é‡ ç»“æ„ä½“çš„å£°æ˜ä¸ä½¿ç”¨ ä½¿ç”¨ type \u0026lt;struct_name\u0026gt; struct {} å£°æ˜ç»“æ„ä½“\ninput:\ntype student struct { name string grade int } func main() { var s student = student{name: \u0026#34;SaltFish\u0026#34;, grade: 4} fmt.Printf(\u0026#34;%v\\n\u0026#34;, s) } output:\n{SaltFish 4}  %T å€¼ç±»å‹çš„ Go è¯­æ³•è¡¨ç¤º main.student %v å€¼ {SaltFish 4} %+v å€¼ï¼Œæ·»åŠ å­—æ®µå {name:SaltFish grade:4} %#v å€¼ï¼ŒGo è¯­æ³•è¡¨ç¤º main.student{name:\u0026quot;SaltFish\u0026quot;, grade:4}  go è¯­è¨€æ ¸å¿ƒç¼–ç¨‹ç¬”è®°  \u0026amp;^ è¿ç®—ç¬¦\na \u0026amp;^ b\n å¦‚æœå³ä¾§æ˜¯ 0ï¼Œåˆ™å·¦ä¾§æ•°ä¿æŒä¸å˜ å¦‚æœå³ä¾§æ˜¯ 1ï¼Œåˆ™å·¦ä¾§æ•°ä¸€å®šæ¸…é›¶   const ( a = 1 // iota = 0, unused \tb = iota // iota = 1 \tc = 3 // iota = 2, unused \td // iota = 3 ) const ( e = iota // iota = 0, æ¯ä¸€ä¸ªconstï¼Œiotaä»0å¼€å§‹ ) func main() { fmt.Printf(\u0026#34;%d %d %d %d %d\\n\u0026#34;, a, b, c, d, e) } 0 1 3 3 0  int æ˜¯å¤§å°è‡³å°‘ä¸º 32 ä½çš„æœ‰ç¬¦å·æ•´æ•°ç±»å‹ã€‚ä½†æ˜¯ï¼Œå®ƒæ˜¯ä¸€ä¸ªä¸åŒçš„ç±»å‹ï¼Œè€Œä¸æ˜¯ int32 çš„åˆ«åã€‚ type int int\nbyte æ˜¯ uint8 çš„åˆ«åï¼Œåœ¨æ‰€æœ‰æ–¹é¢éƒ½ä¸ uint8 ç­‰æ•ˆã€‚æŒ‰ç…§æƒ¯ä¾‹ï¼Œå®ƒç”¨äºåŒºåˆ†å­—èŠ‚å€¼å’Œ 8 ä½æ— ç¬¦å·æ•´æ•°å€¼ã€‚ type byte = uint8\nrune æ˜¯ int32 çš„åˆ«åï¼Œåœ¨æ‰€æœ‰æ–¹é¢éƒ½ä¸ int32 ç­‰æ•ˆã€‚æŒ‰æƒ¯ä¾‹ï¼Œå®ƒç”¨äºåŒºåˆ†å­—ç¬¦å€¼å’Œæ•´æ•°å€¼ã€‚ type rune = int32\n// int8 is the set of all signed 8-bit integers. // Range: -128 through 127. type int8 int8 // int16 is the set of all signed 16-bit integers. // Range: -32768 through 32767. type int16 int16 // int32 is the set of all signed 32-bit integers. // Range: -2147483648 through 2147483647. type int32 int32 // int64 is the set of all signed 64-bit integers. // Range: -9223372036854775808 through 9223372036854775807. type int64 int64 // float32 is the set of all IEEE-754 32-bit floating-point numbers. type float32 float32 // float64 is the set of all IEEE-754 64-bit floating-point numbers. type float64 float64 // complex64 is the set of all complex numbers with float32 real and // imaginary parts. type complex64 complex64 // complex128 is the set of all complex numbers with float64 real and // imaginary parts. type complex128 complex128  float çš„æ¯”è¾ƒ\nè®¡ç®—æœºå¾ˆéš¾è¿›è¡Œæµ®ç‚¹æ•°çš„ç²¾ç¡®è¡¨ç¤ºå’Œå­˜å‚¨ï¼Œå› æ­¤ä¸¤ä¸ªæµ®ç‚¹æ•°ä¹‹é—´ä¸åº”è¯¥ä½¿ç”¨==æˆ–!=è¿›è¡Œæ¯”è¾ƒæ“ä½œï¼Œé«˜ç²¾åº¦ç§‘å­¦è®¡ç®—åº”è¯¥ä½¿ç”¨ math æ ‡å‡†åº“ã€‚\nfunc main() { var v = complex(2.1, 3) // æ„é€ ä¸€ä¸ªå¤æ•° \ta := real(v) // 2.1, å®éƒ¨ \tb := imag(v) // 3 , è™šéƒ¨ \tfmt.Printf(\u0026#34;%v + %vi\\n\u0026#34;, a, b) }  å­—ç¬¦ä¸²æ˜¯å¸¸é‡ï¼Œä¸èƒ½ä¿®æ”¹æŸä¸ªå­—èŠ‚çš„å€¼\nvar s = \u0026#34;SaltFish\u0026#34; s[2] = \u0026#39;I\u0026#39; // cannot assign to s[2] (value of type byte) string æ˜¯ 8 ä½å­—èŠ‚çš„æ‰€æœ‰å­—ç¬¦ä¸²çš„é›†åˆï¼Œé€šå¸¸ä½†ä¸ä¸€å®šè¡¨ç¤º UTF-8 ç¼–ç çš„æ–‡æœ¬ã€‚å­—ç¬¦ä¸²å¯ä»¥ä¸ºç©ºï¼Œä½†ä¸èƒ½ä¸º nilã€‚å­—ç¬¦ä¸²ç±»å‹çš„å€¼æ˜¯ä¸å¯å˜çš„ã€‚ type string string\nå­—ç¬¦ä¸²åº•å±‚æ˜¯ä¸€ä¸ªäºŒå…ƒçš„æ•°æ®ç»“æ„ï¼Œä¸€ä¸ªæ˜¯æŒ‡é’ˆ(æŒ‡å‘å­—èŠ‚æ•°ç»„çš„èµ·ç‚¹)ï¼Œå¦ä¸€ä¸ªæ˜¯é•¿åº¦\nå¯¹å­—ç¬¦ä¸²åˆ‡ç‰‡è¿”å› string è€Œä¸æ˜¯ sliceï¼Œå¹¶ä¸”å­—ä¸²æŒ‡å‘ç›¸åŒçš„åº•å±‚å­—ç¬¦æ•°ç»„\nvar s string = \u0026#34;SaltFish\u0026#34; a := s[0:4] // var a string a[2] = \u0026#34;I\u0026#34; // cannot assign to a[2] (value of type byte) å­—ç¬¦ä¸²è½¬æ¢ä¸º å­—èŠ‚æ•°ç»„/Unicode å­—ç¬¦æ•°ç»„\nvar s string = \u0026#34;SaltFish, ä½ å¥½\u0026#34; a := []byte(s) // [83 97 108 116 70 105 115 104 44 32 228 189 160 229 165 189] b := []rune(s) // [83 97 108 116 70 105 115 104 44 32 20320 22909]  å…è®¸å‡½æ•°è¿”å›å±€éƒ¨å˜é‡çš„åœ°å€ï¼Œè¯¥å±€éƒ¨å˜é‡çš„ç©ºé—´è¢«åˆ†é…åœ¨å †ä¸Š(\u0026ldquo;æ ˆé€ƒé€¸\u0026quot;æœºåˆ¶)\nfunc sum(a, b int) *int { res := a + b return \u0026amp;res } func main() { x, y := 1, 3 z := sum(x, y) // 4 \tfmt.Println(*z) }  æ•°ç»„åœ¨å£°æ˜æ—¶éœ€è¦æŒ‡å®šé•¿åº¦ï¼Œåˆå§‹åŒ–æ—¶å¯ä»¥ä¸æŒ‡å®šé•¿åº¦ï¼Œç”±\nvar a [2]int // [0, 0], å£°æ˜æ—¶éœ€è¦æŒ‡å®šé•¿åº¦ b := [3]int{1, 2} // [1, 2, 0], åˆå§‹åŒ–æ—¶æŒ‡å®šé•¿åº¦ c := [...]int{4, 5, 6} // [4, 5, 6], åˆå§‹åŒ–æ—¶ä¸æŒ‡å®šé•¿åº¦ï¼Œç”±å…ƒç´ ä¸ªæ•°å†³å®š d := [3]int{1: 1, 2: 3} // [0, 1, 3], åˆå§‹åŒ–æ—¶æŒ‡å®šé•¿åº¦ï¼Œä½¿ç”¨ç´¢å¼•å€¼èµ‹å€¼ e := [...]int{1: 1, 3: 3} // [0, 1, 0, 3], åˆå§‹åŒ–æ—¶ä¸æŒ‡å®šé•¿åº¦ï¼Œç”±æœ€åä¸€ä¸ªç´¢å¼•å€¼å†³å®š  æ•°ç»„åˆ›å»ºåé•¿åº¦å›ºå®š æ•°ç»„æ˜¯å€¼ç±»å‹çš„ï¼Œèµ‹å€¼æˆ–ä½œä¸ºå‡½æ•°å‚æ•°éƒ½æ˜¯å€¼æ‹·è´ æ•°ç»„é•¿åº¦æ˜¯æ•°ç»„ç±»å‹çš„ç»„æˆéƒ¨åˆ†, [1O]int å’Œ [20]int è¡¨ç¤ºä¸åŒçš„ç±»å‹ å¯ä»¥æ ¹æ®æ•°ç»„åˆ›å»ºåˆ‡ç‰‡   åˆ‡ç‰‡æ˜¯å¼•ç”¨ç±»å‹ï¼Œå…¶æ•°æ®ç»“æ„ä¸­æœ‰æŒ‡å‘æ•°ç»„çš„æŒ‡é’ˆ\ntype slice struct { array unsafe.Pointer // æŒ‡å‘åº•å±‚æ•°ç»„çš„æŒ‡é’ˆ \tlen int // åˆ‡ç‰‡é•¿åº¦ \tcap int // åˆ‡ç‰‡åº•å±‚æ•°ç»„å®¹é‡ } åˆ‡ç‰‡å¤åˆ¶æ—¶çš„é™åˆ¶\nvar a [7]int = [7]int{0, 1, 2, 3, 4, 5, 6} b := a[0:3] // [0 1 2] \tc := make([]int, 2) copy(c, b) // [0 1], ä¸¤ä¸ªåˆ‡ç‰‡ä¸­é•¿åº¦æœ€å°çš„é™åˆ¶å¤åˆ¶é•¿åº¦  åˆ›å»ºä¸€ä¸ª map\nm1 := map[string]int{\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 2} //map[a:1 b:2] m2 := make(map[int]string) //map[]  var m3 map[int]int // å®šä¹‰ä¸€ä¸ªmapï¼Œä½†æŒ‡é’ˆæŒ‡å‘nilï¼Œéœ€è¦åˆå§‹åŒ–ï¼Œæ­£ç¡®åšæ³• var m3 map[int]int = make(map[int]int) m3[1] = 10 // assignment to entry in nil map map æ˜¯æ— åºçš„\nfunc main() { m1 := make(map[int]string) m1[1] = \u0026#34;tom\u0026#34; m1[1] = \u0026#34;jerry\u0026#34; // è¦†ç›– \tm1[2] = \u0026#34;piony\u0026#34; m1[3] = \u0026#34;july\u0026#34; delete(m1, 3) // åˆ é™¤é”®3å…ƒç´   fmt.Println(m1[1]) // jerry \tfmt.Println(len(m1)) // 2, é”®å€¼å¯¹æ•°é‡ \tfor k, v := range m1 { fmt.Println(\u0026#34;key=\u0026#34;, k, \u0026#34; value=\u0026#34;, v) // è¾“å‡ºé¡ºåºå¯èƒ½æ”¹å˜ \t} } æ— æ³•åœ¨ map ä¸­æ›´æ”¹å…ƒç´ çš„å€¼ï¼Œ è‹¥æƒ³ä¿®æ”¹ï¼Œåªèƒ½æ•´ä½“èµ‹å€¼\nm := make(map[int]student) s1 := student{name: \u0026#34;saltfish\u0026#34;, age: 18} m[1] = s1 fmt.Printf(\u0026#34;%+v\\n\u0026#34;, m[1]) //{name:saltfish age:18} //m[1].age = 22 // err: cannot assign to struct field m[1].age in map m[1] = student{name: \u0026#34;saltfish\u0026#34;, age: 22} fmt.Printf(\u0026#34;%+v\\n\u0026#34;, m[1]) //{name:saltfish age:22}   struct ä¸­çš„ç±»å‹å¯ä»¥ä¸ºä»»æ„ç±»å‹ struct çš„å­˜å‚¨ç©ºé—´æ˜¯è¿ç»­çš„ï¼Œå­—æ®µæŒ‰ç…§å£°æ˜æ—¶çš„é¡ºåºå­˜æ”¾ï¼Œä½†å­—æ®µä¹‹é—´æœ‰å¯¹é½è¦æ±‚  a := person{\u0026#34;saltfish\u0026#34;, 18} // ä¸æ¨èä½¿ç”¨è¿™ç§åˆå§‹åŒ–æ–¹å¼ï¼Œä¸€æ—¦structå®šä¹‰å‘ç”Ÿå˜åŒ–ï¼Œè¿™ä¸ªè¯­å¥å°±ä¼šæŠ¥é”™ p := \u0026amp;person{name: \u0026#34;saltfish\u0026#34;, age: 18} // å…¶å®ƒå­—æ®µä¼šè¢«åˆå§‹åŒ–ä¸ºç±»å‹çš„é›¶å€¼ s := student{person: p, grade: 4}   goto è¯­å¥åªèƒ½åœ¨å‡½æ•°å†…éƒ¨è·³è½¬ goto è¯­å¥ä¸èƒ½è·³è¿‡å†…éƒ¨å˜é‡å£°æ˜è¯­å¥ goto è¯­å¥åªèƒ½è·³åˆ°åŒçº§ä½œç”¨åŸŸæˆ–ä¸Šå±‚ä½œç”¨åŸŸå†…ï¼Œä¸èƒ½è·³åˆ°å†…éƒ¨ä½œç”¨åŸŸå†…  break ä¸æ ‡ç­¾ä½¿ç”¨å¯ä»¥è·³å‡ºæ ‡ç­¾æ‰€æ ‡è¯†çš„ for, switch, select è¯­å¥çš„æ‰§è¡Œ\nfunc main() { L1: for i := 0; ; i++ { for j := 0; ; j++ { if i \u0026gt;= 5 { break L1 // è·³å‡ºL1æ ‡ç­¾æ‰€åœ¨çš„for, breakå’Œæ ‡ç­¾è¦åœ¨åŒä¸€å‡½æ•°å†… \t} if j \u0026gt; 10 { break } } } }  go å‡½æ•°å®å‚åˆ°å½¢å‚ä¼ é€’æ°¸è¿œæ˜¯å€¼æ‹·è´ï¼Œä¼ é€’æŒ‡é’ˆæ—¶ä¼ é€’çš„æ˜¯æŒ‡é’ˆçš„å‰¯æœ¬ï¼ŒæŒ‡å‘åŒä¸€åœ°å€ï¼Œæœ¬è´¨ä¸Šä¹Ÿæ˜¯å€¼æ‹·è´\n defer è°ƒç”¨ä»¥å…ˆè¿›åå‡ºçš„é¡ºåºåœ¨å‡½æ•°è¿”å›å‰è¢«æ‰§è¡Œ defer å‚æ•°çš„å®å‚åœ¨æ³¨å†Œæ—¶é€šè¿‡å€¼æ‹·è´ä¼ é€’è¿›å» ä¸»åŠ¨è°ƒç”¨ os.Exit(int) é€€å‡ºè¿›ç¨‹æ—¶ï¼Œdefer å°†ä¸å†è¢«æ‰§è¡Œï¼Œå³ä½¿å·²ç»æ³¨å†Œ   é—­åŒ… = å‡½æ•° + å¼•ç”¨ç¯å¢ƒ\n é—­åŒ…å¯¹é—­åŒ…å¤–çš„ç¯å¢ƒå¼•å…¥æ˜¯ç›´æ¥å¼•ç”¨ è¿”å›çš„é—­åŒ…æœ‰è‡ªå·±çš„ç‹¬ç«‹ç©ºé—´ï¼Œå¦‚æœé—­åŒ…å¼•ç”¨ä¿®æ”¹äº†å¤–éƒ¨å˜é‡ï¼Œåˆ™æ¯æ¬¡è°ƒç”¨éƒ½ä¼šå½±å“å¤–éƒ¨å˜é‡  func fa(a int) func(i int) int { // ä¼ å…¥çš„å‚æ•° a å¯¹äºé—­åŒ…å†…æ˜¯å¤–éƒ¨å˜é‡ \treturn func(i int) int { println(\u0026amp;a, a) a = a + i return a } } func main() { f := fa(1) g := fa(1) println(f(1)) // å½¢å‚åœ°å€ï¼š0xc000012018ï¼Œå½¢å‚å€¼ï¼š1ï¼Œè¿”å›å€¼ï¼š2 \tprintln(f(1)) // å½¢å‚åœ°å€ï¼š0xc000012018ï¼Œå½¢å‚å€¼ï¼š2ï¼Œè¿”å›å€¼ï¼š3  println(g(1)) // å½¢å‚åœ°å€ï¼š0xc000012030ï¼Œå½¢å‚å€¼ï¼š1ï¼Œè¿”å›å€¼ï¼š2 \tprintln(g(1)) // å½¢å‚åœ°å€ï¼š0xc000012030ï¼Œå½¢å‚å€¼ï¼š2ï¼Œè¿”å›å€¼ï¼š3 } å¯¹è±¡æ˜¯é™„æœ‰è¡Œä¸ºçš„æ•°æ®ï¼Œè€Œé—­åŒ…æ˜¯é™„æœ‰æ•°æ®çš„è¡Œä¸º\n å‡½æ•°å¹¶ä¸èƒ½æ•è·å†…éƒ¨æ–°å¯åŠ¨çš„ goroutine æ‰€æŠ›å‡ºçš„ panic\nfunc do() { // è¿™é‡Œçš„recoveræ— æ³•æ•è·ä¸‹é¢goroutineæŠ›å‡ºçš„panic \tdefer func() { if err := recover(); err != nil { fmt.Println(err) } }() go da() // æŠ›å‡ºpanic \tgo db() time.Sleep(3 * time.Second) } func da() { panic(\u0026#34;panic da\u0026#34;) // ä¸‹é¢çš„è¯­å¥ä¸ä¼šè¢«æ‰§è¡Œ \tfor i := 0; i \u0026lt; 10; i++ { fmt.Println(i) } } func db() { for i := 0; i \u0026lt; 10; i++ { fmt.Println(i) } }  var b T2 = a a å¯ä»¥èµ‹å€¼ç»™å˜é‡ b å¿…é¡»è¦æ»¡è¶³å¦‚ä¸‹æ¡ä»¶ä¸­çš„ä¸€ä¸ª\n T1 å’Œ T2 çš„ç±»å‹ç›¸åŒ T1 å’Œ T2 å…·æœ‰ç›¸åŒçš„åº•å±‚ç±»å‹ï¼Œå¹¶ä¸” T1 å’Œ T2 ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæ˜¯æœªå‘½åç±»å‹ T2 æ˜¯æ¥å£ç±»å‹ï¼ŒT1 æ˜¯å…·ä½“ç±»å‹ï¼Œä¸” T1 çš„æ–¹æ³•é›†æ˜¯ T2 æ–¹æ³•é›†çš„è¶…é›† T1 å’Œ T2 éƒ½æ˜¯é€šé“ç±»å‹ï¼Œä»–ä»¬æ‹¥æœ‰ç›¸åŒçš„å…ƒç´ ç±»å‹ï¼Œå¹¶ä¸” T1 å’Œ T2 ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæ˜¯æœªå‘½åç±»å‹ a æ˜¯ nil, T2 æ˜¯ pointer, function, slice, map, channel, interface ç±»å‹ä¸­çš„ä¸€ä¸ª a æ˜¯ä¸€ä¸ªå­—é¢å¸¸é‡å€¼ï¼Œå¯ä»¥ç”¨æ¥è¡¨ç¤ºç±»å‹ T çš„å€¼  éå¸¸é‡ç±»å‹çš„å˜é‡ x å¯ä»¥å¼ºåˆ¶è½¬åŒ–å¹¶ä¼ é€’ç»™ç±»å‹ Tï¼Œéœ€è¦æ»¡è¶³å¦‚ä¸‹ä»»ä¸€æ¡ä»¶ï¼š\n x å¯ä»¥ç›´æ¥èµ‹å€¼ç»™ T ç±»å‹çš„å˜é‡ x çš„ç±»å‹å’Œ T å…·æœ‰ç›¸åŒçš„åº•å±‚ç±»å‹ x çš„ç±»å‹å’Œ T éƒ½æ˜¯æœªå‘½åçš„æŒ‡é’ˆç±»å‹ï¼Œå¹¶ä¸”æŒ‡é’ˆæŒ‡å‘çš„ç±»å‹å…·æœ‰ç›¸åŒçš„åº•å±‚ç±»å‹ x çš„ç±»å‹å’Œ T éƒ½æ˜¯æ•´å‹æˆ–éƒ½æ˜¯æµ®ç‚¹å‹ x çš„ç±»å‹å’Œ T éƒ½æ˜¯å¤æ•°ç±»å‹ x æ˜¯æ•´æ•°å€¼æˆ–[]byte ç±»å‹çš„å€¼ï¼ŒT æ˜¯ string ç±»å‹ x æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒT æ˜¯[]byte æˆ–[]rune   ä½¿ç”¨æ— ç¼“å†²çš„é€šé“æ¥å®ç° goroutines ä¹‹é—´çš„åŒæ­¥ç­‰å¾…\npackage main func main() { c := make(chan struct{}) // åˆ›å»ºåŒæ­¥é€šé“ \tci := make(chan int, 100) go func(i chan struct{}, j chan int) { for i := 0; i \u0026lt; 10; i++ { ci \u0026lt;- i } close(ci) c \u0026lt;- struct{}{} // goroutine å®Œæˆ \t}(c, ci) println(\u0026#34;NumGoroutine= \u0026#34;, runtime.NumGoroutine()) \u0026lt;-c // é˜»å¡ç­‰å¾… \tprintln(\u0026#34;NumGoroutine= \u0026#34;, runtime.NumGoroutine()) // ä»å·²ç»å…³é—­çš„cié€šé“ä¸­è¯»å– \tfor v := range ci { println(v) } } WaitGroup ç”¨æ¥ç­‰å¾…å¤šä¸ª goroutine å®Œæˆ\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; ) var wg sync.WaitGroup var urls = []string{\u0026#34;http://www.baidu.com/\u0026#34;, \u0026#34;http://www.qq.com/\u0026#34;, \u0026#34;https://golang.google.cn/\u0026#34;, \u0026#34;https://www.google.com\u0026#34;} func main() { for _, url := range urls { // æ¯ä¸ªURLå¯åŠ¨ä¸€ä¸ªgoroutineï¼ŒåŒæ—¶ç»™wgåŠ 1 \twg.Add(1) // å¯¹æ¯ä¸ªurlå¯åŠ¨ä¸€ä¸ªgoroutine \tgo func(url string) { // å½“goroutineç»“æŸåç»™wgè®¡æ•°å‡1ï¼Œwg.Done() ç­‰ä»·äº wg.Add(-1) \tdefer wg.Done() // å‘é€HTTP getè¯·æ±‚å¹¶æ‰“å°è¿”å›ç  \tresp, err := http.Get(url) if err == nil { println(resp.Status) } }(url) } wg.Wait() // ç­‰å¾…æ‰€æœ‰è¯·æ±‚ç»“æŸ } select ç”¨äºç›‘å¬å¤šä¸ªé€šé“ï¼Œåªè¦ç›‘å¬ä¸­çš„ä¿¡é“æœ‰ä¸€ä¸ªæ˜¯å¯è¯»æˆ–è€…å¯å†™çš„ï¼Œselect å°±ä¸ä¼šé˜»å¡ï¼Œè‹¥æœ‰å¤šä¸ªå¯è¯»æˆ–å¯å†™ï¼Œåˆ™ select éšæœºé€‰å–ä¸€ä¸ªå¤„ç†\npackage main func main() { ch := make(chan int, 1) go func(chan int) { for { select { case ch \u0026lt;- 0: case ch \u0026lt;- 1: } } }(ch) for i := 0; i \u0026lt; 10; i++ { println(\u0026lt;-ch) } } é€šçŸ¥é€€å‡ºæœºåˆ¶\nè¯»å–å·±ç»å…³é—­çš„é€šé“ä¸ä¼šå¼•èµ·é˜»å¡ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´ panic ï¼Œè€Œæ˜¯ç«‹å³è¿”å›è¯¥é€šé“å­˜å‚¨ç±»å‹çš„é›¶å€¼ã€‚å…³é—­ select ç›‘å¬çš„æŸä¸ªé€šé“èƒ½ä½¿ select ç«‹å³æ„ŸçŸ¥è¿™ç§é€šçŸ¥ï¼Œç„¶åè¿›è¡Œç›¸åº”çš„å¤„ç†\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;runtime\u0026#34; ) func generateIntA(done chan struct{}) chan int { ch := make(chan int) go func() { LABEL1: for { select { case ch \u0026lt;- rand.Int(): case \u0026lt;-done: break LABEL1 } } close(ch) }() return ch } func main() { done := make(chan struct{}) ch := generateIntA(done) fmt.Println(\u0026lt;-ch) fmt.Println(\u0026lt;-ch) // å‘é€é€šçŸ¥ï¼Œå‘ŠçŸ¥ç”Ÿäº§è€…åœæ­¢ç”Ÿäº§ \tclose(done) fmt.Println(\u0026lt;-ch) fmt.Println(\u0026lt;-ch) println(\u0026#34;NumGoroutine =\u0026#34;, runtime.NumGoroutine()) // ç”Ÿäº§è€…å·²é€€å‡ºï¼ŒNumGoroutine = 1 }  ç®¡é“\npackage main import \u0026#34;fmt\u0026#34; func chain(in chan int) chan int { out := make(chan int) go func() { for v := range in { out \u0026lt;- 1 + v } close(out) }() return out } func main() { in := make(chan int) // åˆå§‹åŒ–ä¼ å…¥å‚æ•° \tgo func() { for i := 0; i \u0026lt; 10; i++ { in \u0026lt;- i } close(in) }() out := chain(chain(chain(in))) for v := range out { fmt.Println(v) } } ","date":"2020-09-22T00:00:00Z","image":"https://saltfishpr.github.io/p/ultimate-go/cover_hu8ef7b16b59d4782c3267f143f43514ec_63441_120x120_fill_box_smart1_3.png","permalink":"https://saltfishpr.github.io/p/ultimate-go/","title":"Ultimate Go"},{"content":"Kaggle Pandas å…¥é—¨ è¯»å–æ•°æ®é›†\nimport pandas as pd reviews = pd.read_csv(\u0026#34;~/Programming/datasets/winemag-data-130k-v2.csv\u0026#34;) åˆ†ç»„å’Œæ’åº å¯¹åˆ†æ•°åˆ†ç»„æ±‚å’Œ\nreviews.groupby(\u0026#34;points\u0026#34;).points.count() Output:\npoints 80 397 81 692 82 1836 ... 98 77 99 33 100 19 Name: points, dtype: int64 è·å¾—ç›¸åŒè¯„åˆ†è‘¡è„é…’çš„æœ€ä½ä»·æ ¼\nreviews.groupby(\u0026#34;points\u0026#34;).price.min() Output:\npoints 80 5.0 81 5.0 82 4.0 ... 98 50.0 99 44.0 100 80.0 Name: price, dtype: float64 ä»æ¯ä¸ªé…’åº„ä¸­é€‰æ‹©ç¬¬ä¸€ç“¶è‘¡è„é…’çš„åç§°\nreviews.groupby(\u0026#39;winery\u0026#39;).apply(lambda df: df.title.iloc[0]) Output:\nwinery 1+1=3 1+1=3 NV RosÃ© Sparkling (Cava) 10 Knots 10 Knots 2010 Viognier (Paso Robles) 100 Percent Wine 100 Percent Wine 2015 Moscato (California) 1000 Stories 1000 Stories 2013 Bourbon Barrel Aged Zinfande... 1070 Green 1070 Green 2011 Sauvignon Blanc (Rutherford) ... Ã“rale Ã“rale 2011 Cabronita Red (Santa Ynez Valley) Ã–ko Ã–ko 2013 Made With Organically Grown Grapes Ma... Ã–konomierat Rebholz Ã–konomierat Rebholz 2007 Von Rotliegenden SpÃ¤t... Ã Maurice Ã Maurice 2013 Fred Estate Syrah (Walla Walla V... Å toka Å toka 2009 Izbrani Teran (Kras) Length: 16757, dtype: object æ ¹æ®å›½å®¶å’Œçœä»½æŒ‘é€‰æœ€å¥½çš„è‘¡è„é…’\nreviews.groupby([\u0026#34;country\u0026#34;, \u0026#34;province\u0026#34;]).apply(lambda df:df.loc[df.points.idxmax()]) Output:\n Unnamed: 0 ... winery country province ... Argentina Mendoza Province 82754 ... Bodega Catena Zapata Other 78303 ... ColomÃ© Armenia Armenia 66146 ... Van Ardi Australia Australia Other 37882 ... Marquis Philips New South Wales 85337 ... De Bortoli ... ... ... Uruguay Juanico 9133 ... Familia Deicas Montevideo 15750 ... Bouza Progreso 93103 ... Pisano San Jose 39898 ... Castillo Viejo Uruguay 39361 ... Narbona [425 rows x 14 columns] agg() æ–¹æ³•å¯ä»¥åœ¨ä¸€ä¸ª dataframe ä¸Šè¿è¡Œä¸€äº›ä¸€ç»´çš„å‡½æ•°\nreviews.groupby([\u0026#39;country\u0026#39;]).price.agg([len, min, max]) Output:\n len min max country Argentina 3800.0 4.0 230.0 Armenia 2.0 14.0 15.0 Australia 2329.0 5.0 850.0 ... US 54504.0 4.0 2013.0 Ukraine 14.0 6.0 13.0 Uruguay 109.0 10.0 130.0 å¤šç´¢å¼•ï¼š\nä½¿ç”¨ groupby() å¯èƒ½ä¼šäº§ç”Ÿå¤šç´¢å¼•\ncountries_reviewed = reviews.groupby([\u0026#39;country\u0026#39;, \u0026#39;province\u0026#39;]).description.agg([len]) countries_reviewed Output:\n len country province Argentina Mendoza Province 3264 Other 536 Armenia Armenia 2 Australia Australia Other 245 New South Wales 85 ... Uruguay Juanico 12 Montevideo 11 Progreso 11 San Jose 3 Uruguay 24 [425 rows x 1 columns] å¤šç´¢å¼•è½¬å›å¸¸è§„ç´¢å¼•\ncountries_reviewed.reset_index() Output:\n country province len 0 Argentina Mendoza Province 3264 1 Argentina Other 536 2 Armenia Armenia 2 3 Australia Australia Other 245 4 Australia New South Wales 85 .. ... ... ... 420 Uruguay Juanico 12 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 423 Uruguay San Jose 3 424 Uruguay Uruguay 24 [425 rows x 3 columns] ä½¿ç”¨ sort_values() å¯¹æ•°æ®æ’åºï¼ˆé»˜è®¤ä¸ºå‡åºæ’åºï¼‰\ncountries_reviewed = countries_reviewed.reset_index() countries_reviewed.sort_values(by=\u0026#39;len\u0026#39;) Output:\n country province len 179 Greece Muscat of Kefallonian 1 192 Greece Sterea Ellada 1 194 Greece Thraki 1 354 South Africa Paardeberg 1 40 Brazil Serra do Sudeste 1 .. ... ... ... 409 US Oregon 5373 227 Italy Tuscany 5897 118 France Bordeaux 5941 415 US Washington 8639 392 US California 36247 [425 rows x 3 columns] é™åºæ’åº\ncountries_reviewed.sort_values(by=\u0026#39;len\u0026#39;, ascending=False) Output:\n country province len 392 US California 36247 415 US Washington 8639 118 France Bordeaux 5941 227 Italy Tuscany 5897 409 US Oregon 5373 .. ... ... ... 101 Croatia Krk 1 247 New Zealand Gladstone 1 357 South Africa Piekenierskloof 1 63 Chile Coelemu 1 149 Greece Beotia 1 [425 rows x 3 columns] é‡æ–°æŒ‰ç…§ç´¢å¼•æ’åº\ncountries_reviewed.sort_index() Output:\n country province len 0 Argentina Mendoza Province 3264 1 Argentina Other 536 2 Armenia Armenia 2 3 Australia Australia Other 245 4 Australia New South Wales 85 .. ... ... ... 420 Uruguay Juanico 12 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 423 Uruguay San Jose 3 424 Uruguay Uruguay 24 [425 rows x 3 columns] åŒæ—¶ä½¿ç”¨å¤šä¸ªåˆ—æ’åº\ncountries_reviewed.sort_values(by=[\u0026#39;country\u0026#39;, \u0026#39;len\u0026#39;]) Output:\n country province len 1 Argentina Other 536 0 Argentina Mendoza Province 3264 2 Armenia Armenia 2 6 Australia Tasmania 42 4 Australia New South Wales 85 .. ... ... ... 421 Uruguay Montevideo 11 422 Uruguay Progreso 11 420 Uruguay Juanico 12 424 Uruguay Uruguay 24 419 Uruguay Canelones 43 [425 rows x 3 columns] æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼ è·å– price åˆ—çš„æ•°æ®ç±»å‹\nreviews.price.dtype Output:\ndtype('float64') æŸ¥çœ‹ dataframe ä¸­æ¯ä¸€åˆ—çš„æ•°æ®ç±»å‹\nreviews.dtypes Output:\nUnnamed: 0 int64 country object description object designation object points int64 price float64 province object region_1 object region_2 object taster_name object taster_twitter_handle object title object variety object winery object dtype: object ä½¿ç”¨ astype() è½¬æ¢æ•°æ®ç±»å‹\nreviews.points.astype(\u0026#39;float64\u0026#39;) Output:\n0 87.0 1 87.0 2 87.0 3 87.0 4 87.0 ... 129966 90.0 129967 90.0 129968 90.0 129969 90.0 129970 90.0 Name: points, Length: 129971, dtype: float64 ç¼ºå°‘å€¼çš„æ¡ç›®è¢«èµ‹äºˆå€¼ NaNï¼Œç¼©å†™ä¸º \u0026ldquo;Not a Number\u0026rdquo;\næŸ¥çœ‹ country ä¸º NaN çš„è¡Œ\nreviews[pd.isnull(reviews.country)] Output:\n Unnamed: 0 country ... variety winery 913 913 NaN ... Chinuri Gotsa Family Wines 3131 3131 NaN ... Red Blend Barton \u0026amp; Guestier 4243 4243 NaN ... Ojaleshi Kakhetia Traditional Winemaking 9509 9509 NaN ... White Blend Tsililis 9750 9750 NaN ... Chardonnay Ross-idi ... ... ... ... ... 124176 124176 NaN ... Red Blend Les FrÃ¨res Dutruy 129407 129407 NaN ... Cabernet Sauvignon El Capricho 129408 129408 NaN ... Tempranillo El Capricho 129590 129590 NaN ... Red Blend BÃ¼yÃ¼lÃ¼baÄŸ 129900 129900 NaN ... Merlot Psagot [63 rows x 14 columns] ä½¿ç”¨ fillna() å¡«å……ç¼ºå¤±çš„è¡Œ\nreviews.region_2.fillna(\u0026#34;Unknown) Output:\n0 Unknown 1 Unknown 2 Willamette Valley 3 Unknown 4 Willamette Valley ... 129966 Unknown 129967 Oregon Other 129968 Unknown 129969 Unknown 129970 Unknown Name: region_2, Length: 129971, dtype: object ä½¿ç”¨ replace(old_val, new_val) æ–¹æ³•æ›¿æ¢éç©ºå€¼\nreviews.taster_twitter_handle.replace(\u0026#34;@kerinokeefe\u0026#34;, \u0026#34;@kerino\u0026#34;) Output:\n0 @kerino 1 @vossroger 2 @paulgwineÂ 3 NaN 4 @paulgwineÂ ... 129966 NaN 129967 @paulgwineÂ 129968 @vossroger 129969 @vossroger 129970 @vossroger Name: taster_twitter_handle, Length: 129971, dtype: object é‡å‘½åå’Œåˆå¹¶ é‡å‘½åä¸€åˆ—\nreviews.rename(columns={\u0026#39;points\u0026#39;: \u0026#39;score\u0026#39;}) Output:\n Unnamed: 0 ... winery 0 0 ... Nicosia 1 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben MÃ¼ller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] é‡å‘½åå¯ä»¥é€‰æ‹© index æˆ–è€… colum å‚æ•°\nreviews.rename(index={0: \u0026#39;firstEntry\u0026#39;, 1: \u0026#39;secondEntry\u0026#39;}) Output:\n Unnamed: 0 ... winery firstEntry 0 ... Nicosia secondEntry 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben MÃ¼ller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] å› ä¸ºå¾ˆå°‘é‡å‘½åç´¢å¼•å€¼ï¼Œé€šå¸¸æƒ…å†µä¸‹ set_index() æ›´å¥½ç”¨\nç»™è¡Œç´¢å¼•å’Œåˆ—ç´¢å¼•æ·»åŠ åç§°å±æ€§\nreviews.rename_axis(\u0026#34;wines\u0026#34;, axis=\u0026#39;rows\u0026#39;).rename_axis(\u0026#34;fields\u0026#34;, axis=\u0026#39;columns\u0026#39;) Output:\nfields Unnamed: 0 ... winery wines ... 0 0 ... Nicosia 1 1 ... Quinta dos Avidagos 2 2 ... Rainstorm 3 3 ... St. Julian 4 4 ... Sweet Cheeks ... ... ... 129966 129966 ... Dr. H. Thanisch (Erben MÃ¼ller-Burggraef) 129967 129967 ... Citation 129968 129968 ... Domaine Gresser 129969 129969 ... Domaine Marcel Deiss 129970 129970 ... Domaine Schoffit [129971 rows x 14 columns] ç»„åˆæœ‰ä¸‰ä¸ªæ ¸å¿ƒæ–¹æ³•concat(), join() å’Œ merge()\nconcat(): ç»™å®šä¸€ä¸ªå…ƒç´ åˆ—è¡¨ï¼Œæ­¤å‡½æ•°å°†æ²¿ç€ä¸€ä¸ª axis å°†è¿™äº›å…ƒç´ æ··åˆåœ¨ä¸€èµ·\ncanadian_youtube british_youtube pd.concat([canadian_youtube, british_youtube]) Output:\n video_id ... description 0 n1WpP7iowLc ... Eminem's new track Walk on Water ft. BeyoncÃ© i... 1 0dBIkQ4Mz1M ... STill got a lot of packages. Probably will las... 2 5qpjK5DgCt4 ... WATCH MY PREVIOUS VIDEO â–¶ \\n\\nSUBSCRIBE â–º http... 3 d380meD0W0M ... I know it's been a while since we did this sho... 4 2Vv-BfVoq4g ... ğŸ§: https://ad.gt/yt-perfect\\nğŸ’°: https://atlant... ... ... ... 40876 sGolxsMSGfQ ... ğŸš¨ NEW MERCH! http://amzn.to/annoyingorange ğŸš¨â¤ ... 40877 8HNuRNi8t70 ... â–º Retrouvez vos programmes prÃ©fÃ©rÃ©s : https://... 40878 GWlKEM3m2EE ... Find out more about Kingdom Hearts 3: https://... 40879 lbMKLzQ4cNQ ... Peter Navarro isnâ€™t talking so tough now. Ana ... 40880 POTgw38-m58 ... è—äººï¼šæå¦ç‘¾ã€ç‰å…”ã€ç­å‚‘ã€LaLaã€å°å„ªã€å°‘å°‘å°ˆå®¶ï¼šé™³ç­±å±(å¾‹å¸«)ã€Wendy(å¿ƒç†å¸«)ã€ç¾…... [40881 rows x 16 columns] video_id ... description 0 Jw1Y-zhQURU ... Click here to continue the story and make your... 1 3s1rvMFUweQ ... Musical guest Taylor Swift performs â€¦Ready for... 2 n1WpP7iowLc ... Eminem's new track Walk on Water ft. BeyoncÃ© i... 3 PUTEiSjKwJU ... Salford drew 4-4 against the Class of 92 and F... 4 rHwDegptbI4 ... Dashcam captures truck's near miss with child ... ... ... ... 38911 l884wKofd54 ... NEW SONG - MOVE TO MIAMI feat. Pitbull (Click ... 38912 IP8k2xkhOdI ... THE OFFICIAL UP WITH IT MUSIC VIDEO!Get my new... 38913 Il-an3K9pjg ... Get 2002 by Anne-Marie HERE â–¶ http://ad.gt/200... 38914 -DRsfNObKIQ ... Eleni Foureira represented Cyprus at the first... 38915 4YFo4bdMO8Q ... Debut album 'Light of Mine' out now: http://ky... [38916 rows x 16 columns] video_id ... description 0 n1WpP7iowLc ... Eminem's new track Walk on Water ft. BeyoncÃ© i... 1 0dBIkQ4Mz1M ... STill got a lot of packages. Probably will las... 2 5qpjK5DgCt4 ... WATCH MY PREVIOUS VIDEO â–¶ \\n\\nSUBSCRIBE â–º http... 3 d380meD0W0M ... I know it's been a while since we did this sho... 4 2Vv-BfVoq4g ... ğŸ§: https://ad.gt/yt-perfect\\nğŸ’°: https://atlant... ... ... ... 38911 l884wKofd54 ... NEW SONG - MOVE TO MIAMI feat. Pitbull (Click ... 38912 IP8k2xkhOdI ... THE OFFICIAL UP WITH IT MUSIC VIDEO!Get my new... 38913 Il-an3K9pjg ... Get 2002 by Anne-Marie HERE â–¶ http://ad.gt/200... 38914 -DRsfNObKIQ ... Eleni Foureira represented Cyprus at the first... 38915 4YFo4bdMO8Q ... Debut album 'Light of Mine' out now: http://ky... [79797 rows x 16 columns] join(): å¯ä»¥ç»„åˆå…·æœ‰å…±åŒç´¢å¼•çš„ä¸åŒ dataframe å¯¹è±¡\nleft = canadian_youtube.set_index([\u0026#39;title\u0026#39;, \u0026#39;trending_date\u0026#39;]) right = british_youtube.set_index([\u0026#39;title\u0026#39;, \u0026#39;trending_date\u0026#39;]) left.join(right, lsuffix=\u0026#39;_CAN\u0026#39;, rsuffix=\u0026#39;_UK\u0026#39;) Output:\n video_id_CAN ... description_UK title trending_date ... !! THIS VIDEO IS NOTHING BUT PAIN !! | Getting ... 18.04.01 PNn8sECd7io ... NaN #1 Fortnite World Rank - 2,323 Solo Wins! 18.09.03 DvPW66IFhMI ... NaN #1 Fortnite World Rank - 2,330 Solo Wins! 18.10.03 EXEaMjFeiEk ... NaN #1 MOST ANTICIPATED VIDEO (Timber Frame House R... 17.20.12 bYvQmusLaxw ... NaN 17.21.12 bYvQmusLaxw ... NaN ... ... ... ğŸ˜²She Is So Nervous But BLOWS The ROOF After Tak... 18.02.05 WttN1Z0XF4k ... NaN 18.29.04 WttN1Z0XF4k ... NaN 18.30.04 WttN1Z0XF4k ... NaN ğŸš¨ BREAKING NEWS ğŸ”´ Raja Live all Slot Channels W... 18.07.05 Wt9Gkpmbt44 ... NaN ğŸš¨Active Shooter at YouTube Headquarters - LIVE ... 18.04.04 Az72jrKbANA ... NaN [40900 rows x 28 columns] å…¶ä»– æ˜¾ç¤ºæ‰€æœ‰åˆ—\npd.set_option(\u0026#34;display.max_columns\u0026#34;, None) ","date":"2020-05-25T00:00:00Z","permalink":"https://saltfishpr.github.io/p/kaggle-pandas/","title":"Kaggle Pandas"},{"content":"conda å¸¸ç”¨ æ¢æº åˆ›å»º .condarc é…ç½®æ–‡ä»¶\nconda config --set show_channel_urls yes ç”¨æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€ ~/.condarc å¡«å…¥ä»¥ä¸‹å†…å®¹\nchannels: - defaults show_channel_urls: true channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud å‘½ä»¤ åˆ›å»ºæœ‰æœ€æ–°ç‰ˆæœ¬çš„ python çš„ç¯å¢ƒ conda create -n \u0026lt;env-name\u0026gt; python=3\nåˆ é™¤ç¯å¢ƒ conda remove -n \u0026lt;env-name\u0026gt; --all\næ¸…é™¤ç¼“å­˜ conda clean -a\npip å¸¸ç”¨ æ¢æº åœ¨ ~/.config/pip/pip.conf ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹\n[global] index-url = https://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com åˆ é™¤ç¼“å­˜ æ‰¾åˆ° ~/.cache/pip æ–‡ä»¶å¤¹ï¼Œåˆ é™¤å³å¯\nåœ¨å®‰è£…æ—¶ä½¿ç”¨\npip install \u0026lt;package-name\u0026gt; --no-cache-dir å¯¼å…¥å¯¼å‡º python ç¯å¢ƒ ä»Šå¤©åœ¨å­¦ä¹ æ•°æ®æŒ–æ˜çš„æ—¶å€™ï¼Œnolearn å’Œ lasagne ä¸¤ä¸ªåº“çš„æ—¶å€™ç»™æˆ‘çš„ jypyterlab ç¯å¢ƒæå´©äº†ï¼Œåªå¥½ remove \u0026ndash;all é‡æ–°é…èµ·ï¼ŒçœŸåæ‚”æ²¡æœ‰å…ˆæä¸ªç¯å¢ƒå¤‡ä»½( Â´â€¢ï¸µâ€¢` )\nconda conda æ˜¯ä¸ªå¥½ä¸œè¥¿ï¼Œå¯ä»¥è‡ªå·±å¤„ç†ç¯å¢ƒä¾èµ–ï¼Œç¼ºç‚¹å°±æ˜¯ã€‚ã€‚åŒ…æœ‰ç‚¹è€ï¼Œæœ‰äº›åŒ…è¿˜æ‰¾ä¸åˆ°\nå¯¼å…¥ç¯å¢ƒ conda create --name \u0026lt;your env name\u0026gt; --file \u0026lt;this file\u0026gt; --yes å¯¼å‡ºç¯å¢ƒ conda list -e \u0026gt; requirements.txt pip pip ä¹Ÿæœ‰å¾ˆå¤šä¼˜ç‚¹ï¼Œæˆ‘ä¸€èˆ¬æ˜¯åœ¨ conda æ‰¾ä¸åˆ°æ¨¡å—çš„æ—¶å€™ä½¿ç”¨ pip\nå¯¼å…¥ç¯å¢ƒ pip install -r requirements.txt å¯¼å‡ºç¯å¢ƒ pip freeze \u0026gt; requirements.txt  æ€»ä¹‹ï¼Œç”¨ conda åˆ›é€ ç‹¬ç«‹çš„ç¯å¢ƒï¼Œç”¨ pip å®‰è£…æ–°é²œçš„æ¨¡å—ï¼Œä¸¤ä¸ªå·¥å…·ç»“åˆèµ·æ¥ç”¨ç”šå¥½\n","date":"2020-03-26T00:00:00Z","permalink":"https://saltfishpr.github.io/p/python-%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86/","title":"Python ç¯å¢ƒç®¡ç†"},{"content":"ã€ŠEffective+Pythonã€‹è¯»ä¹¦ç¬”è®° ä¸ªäººå­¦ä¹ ç¬”è®°\nå¦‚æœæœ‰ä¾µæƒæƒ…å†µï¼Œè¯·ç»™æˆ‘å‘é‚®ä»¶é€šçŸ¥æˆ‘åˆ é™¤ 526191197@qq.com\nç”¨ zip å‡½æ•°åŒæ—¶éå†å¤šä¸ªè¿­ä»£å™¨ ç”¨ zip å¯ä»¥æŠŠä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Šçš„è¿­ä»£å™¨å°è£…æˆç”Ÿæˆå™¨ï¼Œä»¥ä¾¿ç¨åæ±‚å€¼\nnames = [\u0026#39;aaaa\u0026#39;, \u0026#39;bb\u0026#39;, \u0026#39;ccc\u0026#39;] letters = [len(n) for n in names] for name, count in zip(names, letters): if count \u0026gt; max_letters: longest_name = name max_letters = count å½“ zip å°è£…çš„ä»»æ„ä¸€ä¸ªè¿­ä»£å™¨è€—å°½ï¼Œzip å°±ä¸å†äº§ç”Ÿå…ƒç»„\nitertools å†…ç½®æ¨¡å—ä¸­çš„ zip_longest å‡½æ•°å¯ä»¥å¹³è¡Œåœ°éå†å¤šä¸ªè¿­ä»£å™¨ï¼Œä¸ç”¨è€ƒè™‘é•¿åº¦æ˜¯å¦ç›¸ç­‰ã€‚\nfor/else çš„å«ä¹‰ for i in range(3): print(\u0026#39;Loop %d\u0026#39; % i) if i == 1: break else: print(\u0026#39;Else block!\u0026#39;) è¿™é‡Œåœ¨ for å¾ªç¯åé¢ç´§è·Ÿ else è¯­å¥ï¼Œåœ¨ for æ­£å¸¸æ‰§è¡Œå®Œæ—¶ä¼šæ‰§è¡Œ else è¯­å¥ï¼›è€Œå¦‚æœç»è¿‡ break è·³å‡ºï¼Œä¼šå¯¼è‡´ç¨‹åºä¸æ‰§è¡Œ else å—\nå°½é‡ä¸è¦è¿™æ ·å»å†™ï¼Œå¾ˆä¸ç›´è§‚\nåˆç†ä½¿ç”¨ try/except/else/finally try ç”¨æ¥æ‰§è¡Œå¯èƒ½å‘ç”Ÿå¼‚å¸¸çš„ä»£ç å—ï¼Œelse ç”¨æ¥æ‰§è¡Œ try æ­£å¸¸è¿è¡Œåçš„ä»£ç å—ï¼Œexcept ç”¨æ¥æ•è· try ä¸­å‘ç”Ÿçš„å¼‚å¸¸ï¼Œfinally æ˜¯æœ€ç»ˆéƒ½è¦æ‰§è¡Œçš„ä»£ç å—ï¼Œåšæ¸…ç†å·¥ä½œ\nç”¨å¼‚å¸¸è¡¨ç¤ºç‰¹æ®Šæƒ…å†µï¼Œè€Œä¸è¦è¿”å› None å¾ˆå¤šæ—¶å€™æˆ‘ä¼šè¿™æ ·å†™ï¼š\ndef divide(a, b): try: return a / b except ZeroDivisionError: return None result = divide(x, y) if result is None: print(\u0026#39;Invalid inputs\u0026#39;) ä½†æ˜¯å¦‚æœè¾“å…¥æ¯”è¾ƒç‰¹æ®Šï¼Œå¦‚ x=0, y=1 æ—¶ï¼Œè®¡ç®—ç»“æœä¸º 0ï¼Œå°±ä¼šå‘ç”Ÿé”™è¯¯\nä¸¤ç§è§£å†³åŠæ³•ï¼š\n å°† divide ä¿®æ”¹ def divide(a, b): try: return True, a / b except ZeroDivisionError: return False, None success, result = divide(x, y) if not success: print(\u0026#39;Invalid inputs\u0026#39;)  å°†å¼‚å¸¸æŠ›ç»™ä¸Šä¸€çº§ def divide(a, b): try: return a / b except ZeroDivisionError as e: raise ValueError(\u0026#39;Invalid inputs\u0026#39;) from e x, y = 5, 2 try: result = divide(x, y) except ValueError: print(\u0026#39;Invalid inputs\u0026#39;) else: print(\u0026#39;Result is %.1f\u0026#39; % result)   None, 0, '' éƒ½ä¼šè¢«è¯„ä¼°ä¸º false\näº†è§£é—­åŒ… def sort_priority(values, group): def helper(x): if x in group: return (0, x) return (1, x) values.sort(key=helper) helper å‡½æ•°èƒ½è®¿é—® sort_priority çš„ group å‚æ•°ï¼Œæ˜¯å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªé—­åŒ…ã€‚\npython ä½¿ç”¨ç‰¹æ®Šçš„è§„åˆ™æ¥æ¯”è¾ƒä¸¤ä¸ªå…ƒç»„/åˆ—è¡¨ï¼Œå®ƒé¦–å…ˆæ¯”è¾ƒå…ƒç»„ä¸‹æ ‡ä¸º 0 çš„å¯¹åº”å…ƒç´ ï¼Œå¦‚æœç›¸ç­‰ï¼Œå†æ¯”è¾ƒä¸‹æ ‡ä¸º 1 çš„å¯¹åº”å…ƒç´ ã€‚\nå½“è¡¨è¾¾å¼åœ¨å¼•ç”¨å˜é‡æ—¶ï¼Œpython è§£é‡Šå™¨æŒ‰ç…§å¦‚ä¸‹é¡ºåºéå†å„ä½œç”¨åŸŸï¼Œä»¥è§£æè¯¥å¼•ç”¨ï¼š\n å½“å‰å‡½æ•°çš„ä½œç”¨åŸŸ ä»»ä½•å¤–å›´çš„ä½œç”¨åŸŸ å½“å‰ä»£ç é‚£ä¸ªæ¨¡å—ï¼ˆæ–‡ä»¶ï¼‰çš„ä½œç”¨åŸŸï¼ˆå…¨å±€ä½œç”¨åŸŸï¼‰ å†…ç½®ä½œç”¨åŸŸï¼ˆåŒ…å« len()ã€str() ç­‰å‡½æ•°çš„ä½œç”¨åŸŸï¼‰  å¦‚æœä¸Šé¢è¿™å‡ ä¸ªåœ°æ–¹éƒ½æ²¡æœ‰å®šä¹‰è¿‡åç§°ç›¸ç¬¦çš„å˜é‡ï¼Œå°±æŠ›å‡º NameError å¼‚å¸¸\nç»™å˜é‡èµ‹å€¼æ—¶ï¼Œå¦‚æœå½“å‰ä½œç”¨åŸŸå·²ç»å®šä¹‰äº†è¿™ä¸ªå˜é‡ï¼Œé‚£ä¹ˆå˜é‡å°±ä¼šå…·å¤‡æ–°å€¼ï¼Œè‹¥æ˜¯å½“å‰ä½œç”¨åŸŸæ²¡æœ‰è¿™ä¸ªå˜é‡ï¼Œpython åˆ™ä¼šæŠŠè¿™æ¬¡èµ‹å€¼è§†ä¸ºå¯¹è¯¥å˜é‡çš„å®šä¹‰ã€‚\nnonlocal è¯­å¥è¡¨ç¤º å¦‚æœåœ¨é—­åŒ…å†…ç»™è¯¥å˜é‡èµ‹å€¼ï¼Œåˆ™ä¿®æ”¹çš„å…¶å®æ˜¯é—­åŒ…å¤–é‚£ä¸ªä½œç”¨åŸŸä¸­çš„å˜é‡\nglobal è¡¨ç¤ºå¯¹è¯¥å˜é‡èµ‹å€¼æ“ä½œï¼Œå°†ä¼šç›´æ¥ä¿®æ”¹æ¨¡å—ä½œç”¨åŸŸé‡Œçš„é‚£ä¸ªå˜é‡\n","date":"2020-03-16T00:00:00Z","permalink":"https://saltfishpr.github.io/p/effective-python/","title":"Effective Python"},{"content":"Github Pages ä¸ªäººåšå®¢æ­å»º â€ƒåº”æœ‹å‹çš„å¼ºçƒˆè¦æ±‚åœ¨è¿™é‡Œè®°å½•ä¸‹ä½¿ç”¨ github pages + jekyll æ­å»ºä¸ªäººç½‘ç«™çš„æ­¥éª¤ã€‚æˆ‘è‡ªå·±èŠ±äº†ä¸¤å¤©æ¢³ç†äº†ä¸€ä¸‹ jekyll æ¨¡æ¿çš„ç›®å½•ç»“æ„å’Œä½¿ç”¨æ–¹æ³•ï¼Œåªä¼šäº›ç®€å•çš„ä¿®æ”¹ä½†æ˜¯åŸºæœ¬å¤Ÿç”¨äº†ï¼ˆæ¯•ç«Ÿä¸æ˜¯å‰ç«¯äººï¼‰ã€‚\nå‡†å¤‡  æ³¨å†Œ github è´¦å· åˆ›å»ºä¸€ä¸ªä»“åº“ï¼Œå‘½åå¿…é¡»ä¸º\u0026lt;username\u0026gt;.github.ioï¼Œç™¾åº¦ä¸Šä»»æ„æœç´¢éƒ½æœ‰ä»‹ç»è¿‡ç¨‹ï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°äº†  windows:   å®‰è£… gitï¼Œä¸‹è½½å®‰è£…æœ€æ–°ç‰ˆæœ¬å³å¯\n  å®‰è£… msys2\nå®‰è£…å¥½ msys2 åï¼Œé¦–å…ˆè®°å¾—åˆ‡æ¢é•œåƒæºï¼Œä¸ç„¶ä¼šå› ä¸ºç½‘é€Ÿå¤ªæ…¢å¿ƒæ€å´©æºƒã€‚åœ¨è®¾ç½®é•œåƒæºçš„æ—¶å€™ï¼Œå¯ä»¥å°†é™¤äº†æ¸…åæºçš„å…¶ä»–æºå…¨éƒ¨æ³¨é‡Šæ‰ï¼Œæé«˜ä¸‹è½½é€Ÿåº¦ã€‚\n  å®‰è£… rubyï¼Œè¿™é‡Œä¸‹è½½ 2.6.5 ç‰ˆæœ¬ï¼Œå› ä¸ºæœ€æ–°ç‰ˆåé¢ä¼šæç¤ºç‰ˆæœ¬ä¸åŒ¹é…ã€‚\n ä¸‹è½½è¿‡ç¨‹å¯èƒ½å¾ˆä¹…\u0026hellip;å¦‚æœè§‰å¾—æ…¢å¯ä»¥ä¸‹è½½ä¸åŒ…å« devkit çš„ç‰ˆæœ¬ã€‚ å®‰è£…æ—¶åœ¨ Select Components ç•Œé¢ä¸ç”¨å‹¾é€‰ msys2ï¼Œå› ä¸ºåœ¨ä¸Šä¸€æ­¥ä¸­å·²ç»å®‰è£…å¥½äº†ã€‚ åœ¨ Finish æ—¶å‹¾é€‰ Run \u0026lsquo;ridk install\u0026rsquo; to setup msys2\u0026hellip; å¼¹å‡ºé…ç½®ç•Œé¢ï¼Œåœ¨è¿™é‡Œæˆ‘é€‰æ‹© 3 å¹¶æŒ‰å›è½¦ã€‚ç”±äºé…ç½®è¿‡ msys2 çš„æºï¼Œè¿™é‡Œä¸‹è½½å®‰è£…é€Ÿåº¦å¾ˆå¿«ã€‚ å®‰è£…å®Œæˆåï¼Œæ‰“å¼€ msys2 å‘½ä»¤è¡Œçª—å£ï¼Œä¸º rubygemsé…ç½®æºã€‚    å®‰è£…rubygemsï¼Œä¸‹è½½åè§£å‹ç¼©ï¼Œåœ¨æ–‡ä»¶å¤¹ä¸­æ‰“å¼€ git bash è¾“å…¥ ruby setup.rb\n  å®‰è£… bundlerï¼šåœ¨ msys2 æˆ–è€… git bash å‘½ä»¤è¡Œä¸­è¾“å…¥gem install bundlerï¼Œç­‰å¾…å®‰è£…å®Œæˆ\n å†æ¬¡ä½¿ç”¨æ¸…åæºä¸º bundle é…ç½®é•œåƒæºã€‚    å®‰è£… jekyllï¼šåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥gem install jekyll\n  è‡³æ­¤å‡†å¤‡å·¥ä½œå…¨éƒ¨å®Œæˆï½\nlinux(Debian 10.3) debian ç³»ç»Ÿè‡ªå¸¦å¤§éƒ¨åˆ†åŸºç¡€åŒ…\n git: sudo apt-get install git ä¸º gem é…ç½®é•œåƒæºï¼Œæ­¥éª¤ä¸ Windows ä¸€æ · å®‰è£… bundler: gem install bundler ä¸º bundler é…ç½®é•œåƒæºï¼ŒåŒä¸Š å®‰è£… jekyll: gem install jekyll  è‡³æ­¤å‡†å¤‡å·¥ä½œå…¨éƒ¨å®Œæˆï½\næ­å»º blog ä½¿ç”¨ git clone å°†è‡ªå·±çš„ä»“åº“ clone ä¸‹æ¥ã€‚\nå¦‚æœæœ‰èƒ½åŠ›å¯ä»¥è‡ªå·±ç”Ÿæˆä¸€ä¸ªæ–°çš„ jekyll é¡¹ç›®ï¼šæ–°å»ºä¸€ä¸ªç©ºæ–‡ä»¶å¤¹ï¼Œåœ¨æ­¤æ–‡ä»¶å¤¹ä¸­å‘½ä»¤è¡Œè¾“å…¥jekyll new site-nameï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹çš„æ‰€æœ‰æ–‡ä»¶å¤åˆ¶åˆ° clone ä¸‹æ¥çš„ä»“åº“ä¸­ã€‚\nä¹Ÿå¯ä»¥åœ¨æ¨¡æ¿ç½‘ç«™ä¸‹è½½ä¸€ä¸ªä¿®æ”¹é…ç½®ã€‚æˆ‘çš„github pageä½¿ç”¨çš„æ˜¯ Hux å¤§ä½¬çš„æ¨¡æ¿ã€‚\nè¿™é‡Œå¯ä»¥å…‹éš†æˆ‘çš„æ¨¡æ¿\ngit clone https://github.com/SaltFishPr/saltfishpr.github.io.git\nåœ¨é¡¹ç›®æ–‡ä»¶å¤¹ä¸­æ‰“å¼€ git bash è¾“å…¥bundle installï¼Œç­‰å¾…å®‰è£…å®Œæˆã€‚å¦‚æœå®‰è£…å‡ºç°ä¾èµ–é—®é¢˜ï¼Œå¯ä»¥åœ¨ç™¾åº¦ä¸­æœç´¢ç¼ºå°‘çš„åŒ…çœ‹å¦‚ä½•å®‰è£…ï¼Œéƒ½å¯ä»¥æ‰¾åˆ°è§£å†³ç­”æ¡ˆã€‚\né¡¹ç›®æ–‡ä»¶çš„ç¼–è¾‘æ¨èä½¿ç”¨ vscodeã€‚ç”¨ vscode æ‰“å¼€æ–‡ä»¶å¤¹åï¼Œå¯ä»¥çœ‹åˆ°æ–‡ä»¶å¤¹çš„ç›®å½•ç»“æ„:\nâ”œâ”€â”€ 404.html â”œâ”€â”€ about.html â”œâ”€â”€ archive.html â”œâ”€â”€ CNAME â”œâ”€â”€ _config.yml â”œâ”€â”€ css â”œâ”€â”€ feed.xml â”œâ”€â”€ fonts â”œâ”€â”€ Gemfile â”œâ”€â”€ Gemfile.lock â”œâ”€â”€ Gruntfile.js â”œâ”€â”€ img â”œâ”€â”€ _includes â”œâ”€â”€ index.html â”œâ”€â”€ js â”œâ”€â”€ _layouts â”œâ”€â”€ less â”œâ”€â”€ LICENSE â”œâ”€â”€ offline.html â”œâ”€â”€ _posts â”œâ”€â”€ pwa â”œâ”€â”€ README.md â”œâ”€â”€ _site â””â”€â”€ sw.js  è¿™é‡Œä¸»è¦ä¿®æ”¹é…ç½®æ–‡ä»¶_config.ymlã€‚\nè¿è¡Œæœ¬åœ°æœåŠ¡ åœ¨é¡¹ç›®æ ¹ç›®å½•å‘½ä»¤è¡Œä¸­è¾“å…¥bundle exec jekyll server --watchï¼Œåœ¨æœ¬åœ°å¯åŠ¨ jekyll æœåŠ¡å™¨ï¼Œæµè§ˆå™¨è¾“å…¥ 127.0.0.1:4000 æœ¬åœ°æŸ¥çœ‹ blogã€‚\nå¯¹ç…§_config.yml ä¿®æ”¹ï¼›å–„ç”¨ ctrl+shift+F å…¨å±€æœç´¢ã€‚ä¿®æ”¹å‡ºå±äºè‡ªå·±çš„ blog å§ï¼\nå†™åœ¨æœ€å ç”±äºå»ºç«‹ blog æ˜¯åœ¨ä¸€ä¸ªæ˜ŸæœŸå‰äº†ï¼Œå¯èƒ½æœ‰äº›å°æ­¥éª¤è®°ä¸æ¸…æ¥šï¼Œæœ‰ä»€ä¹ˆé—®é¢˜æ¬¢è¿ä¸‹æ–¹ issue æŒ‡æ­£ O(âˆ©_âˆ©)Oã€‚\nissues ç¼ºå°‘ ruby.in : sudo apt-get install ruby-dev\nzlib is missing; necessary for building libxml2 : sudo apt-get install zlib1g zlib1g.dev\nå‚è€ƒèµ„æ–™ï¼š\n jekyll ç›®å½•ç»“æ„ github pages æ–‡æ¡£  ","date":"2020-03-10T00:00:00Z","image":"https://saltfishpr.github.io/p/github-pages-with-jekyll/cover_hu78d3083f49bf26f8c49322a8654c4914_21023_120x120_fill_q75_box_smart1.jpg","permalink":"https://saltfishpr.github.io/p/github-pages-with-jekyll/","title":"GitHub Pages with Jekyll"},{"content":"æ•°æ®æŒ–æ˜å­¦ä¹ ç¬”è®° ä¹¦ä¸Šçš„æºç åœ¨å®˜ç½‘ä¸Šå¯ä»¥æ³¨å†Œè´¦å·ä¸‹è½½ï¼Œè¿™é‡Œåªä¸ºè®°å½•è‡ªå·±çš„å­¦ä¹ è¿‡ç¨‹ã€‚\nå¦‚æœæœ‰ä¾µæƒæƒ…å†µï¼Œè¯·ç»™æˆ‘å‘é‚®ä»¶é€šçŸ¥æˆ‘åˆ é™¤ 526191197@qq.com\næ­¤ç¬”è®°çš„ä»£ç å‡åœ¨ pycharm - python3.8 ä¸­è¿è¡Œé€šè¿‡\nå­¦ä¹ æ•°æ®æŒ–æ˜ï¼Œè®©æ•°æ®æœåŠ¡äºäººç±»\nç¬¬ä¸€ç«  äº²å’Œæ€§åˆ†æ äº²å’Œæ€§åˆ†ææ ¹æ®æ ·æœ¬ä¸ªä½“ï¼ˆç‰©ä½“ï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œç¡®å®šä»–ä»¬çš„å…³ç³»äº²ç–ã€‚åº”ç”¨åœºæ™¯æœ‰ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š\n å‘ç”¨æˆ·æŠ•æ”¾å®šå‘å¹¿å‘Š ä¸ºç”¨æˆ·æä¾›æ¨èï¼ˆå¦‚æ­Œæ›²æ¨èï¼Œç”µå½±æ¨èç­‰ï¼‰  åè¯ï¼š\n è§„åˆ™ï¼šä¸€æ¡è§„åˆ™ç”±å‰ææ¡ä»¶å’Œç»“è®ºä¸¤éƒ¨åˆ†ç»„æˆ æ”¯æŒåº¦ï¼šæ•°æ®é›†ä¸­è§„åˆ™åº”éªŒçš„æ¬¡æ•° ç½®ä¿¡åº¦ï¼šè§„åˆ™ï¼ˆç»“æœï¼‰å‡ºç°çš„æ¬¡æ•° / æ¡ä»¶å‡ºç°çš„æ¬¡æ•°ï¼ˆæ¡ä»¶ç›¸åŒçš„è§„åˆ™æ•°é‡ï¼‰ï¼Œè¡¡é‡è§„åˆ™çš„å‡†ç¡®ç‡  # -*- coding: utf-8 -*- import numpy as np from collections import defaultdict from operator import itemgetter if __name__ == \u0026#39;__main__\u0026#39;: dataset_filename = \u0026#34;affinity_dataset.txt\u0026#34; X = np.loadtxt(dataset_filename) n_samples, n_features = X.shape # æ ·æœ¬æ•°ï¼Œç‰¹å¾æ•° features = [\u0026#34;bread\u0026#34;, \u0026#34;milk\u0026#34;, \u0026#34;cheese\u0026#34;, \u0026#34;apples\u0026#34;, \u0026#34;bananas\u0026#34;] # å•†å“ååˆ—è¡¨ # å¦‚æœxxxï¼Œé‚£ä¹ˆxxx å°±æ˜¯ä¸€æ¡è§„åˆ™ã€‚è§„åˆ™ç”±å‰ææ¡ä»¶å’Œç»“è®ºä¸¤éƒ¨åˆ†ç»„æˆ # è¿™é‡Œæ³¨æ„\u0026#39;å¦‚æœä¹°Aåˆ™ä»–ä»¬ä¼šä¹°B\u0026#39;å’Œ\u0026#39;å¦‚æœä¹°Båˆ™ä»–ä»¬ä¼šä¹°A\u0026#39;ä¸æ˜¯ä¸€ä¸ªè§„åˆ™ï¼Œåœ¨ä¸‹é¢çš„å¾ªç¯ä¸­ä½“ç°å‡ºæ¥ valid_rules = defaultdict(int) # è§„åˆ™åº”éªŒ invalid_rules = defaultdict(int) # è§„åˆ™æ— æ•ˆ num_occurences = defaultdict(int) # å•†å“è´­ä¹°æ•°é‡å­—å…¸ for sample in X: # å¯¹æ•°æ®é›†é‡Œçš„æ¯ä¸ªæ¶ˆè´¹è€… for premise in range(n_features): if sample[premise] == 0: # å¦‚æœè¿™ä¸ªå•†å“æ²¡æœ‰ä¹°ï¼Œç»§ç»­çœ‹ä¸‹ä¸€ä¸ªå•†å“ continue num_occurences[premise] += 1 # è®°å½•è¿™ä¸ªå•†å“è´­ä¹°æ•°é‡ for conclusion in range(n_features): if premise == conclusion: # è·³è¿‡æ­¤å•†å“ continue if sample[conclusion] == 1: valid_rules[(premise, conclusion)] += 1 # è§„åˆ™åº”éªŒ else: invalid_rules[(premise, conclusion)] += 1 # è§„åˆ™æ— æ•ˆ support = valid_rules # æ”¯æŒåº¦å­—å…¸ï¼Œå³è§„åˆ™åº”éªŒæ¬¡æ•° confidence = defaultdict(float) # ç½®ä¿¡åº¦å­—å…¸ for premise, conclusion in valid_rules.keys(): # æ¡ä»¶/ç»“è®º rule = (premise, conclusion) # ç½®ä¿¡åº¦ = è§„åˆ™å‘ç”Ÿçš„æ¬¡æ•°/æ¡ä»¶å‘ç”Ÿçš„æ¬¡æ•° confidence[rule] = valid_rules[rule] / num_occurences[premise] def print_rule(premise, conclusion, support, confidence, features): premise_name = features[premise] conclusion_name = features[conclusion] print( \u0026#34;Rule: If a person buys {0}they will also buy {1}\u0026#34;.format( premise_name, conclusion_name)) print( \u0026#34; - Confidence: {0:.3f}\u0026#34;.format(confidence[(premise, conclusion)])) print(\u0026#34; - Support: {0}\u0026#34;.format(support[(premise, conclusion)])) print(\u0026#34;\u0026#34;) # å¾—åˆ°æ”¯æŒåº¦æœ€é«˜çš„è§„åˆ™ï¼Œitems()è¿”å›å­—å…¸æ‰€æœ‰å…ƒç´ çš„åˆ—è¡¨ï¼Œitemgetter(1)è¡¨ç¤ºç”¨æ”¯æŒåº¦çš„å€¼ä½œä¸ºé”®ï¼Œè¿›è¡Œé™åºæ’åˆ— sorted_support = sorted(support.items(), key=itemgetter(1), reverse=True) for i in range(5): print(\u0026#34;Rule #{0}\u0026#34;.format(i + 1)) premise, conclusion = sorted_support[i][0] print_rule(premise, conclusion, support, confidence, features) sorted_confidence = sorted(confidence.items(), key=itemgetter(1), reverse=True) for i in range(5): print(\u0026#34;Rule #{0}\u0026#34;.format(i + 1)) premise, conclusion = sorted_confidence[i][0] print_rule(premise, conclusion, support, confidence, features) Outputï¼š\nRule #1 Rule: If a person buys cheese they will also buy bananas - Confidence: 0.659 - Support: 27 Rule #2 Rule: If a person buys bananas they will also buy cheese - Confidence: 0.458 - Support: 27 Rule #3 Rule: If a person buys cheese they will also buy apples - Confidence: 0.610 - Support: 25 Rule #1 Rule: If a person buys apples they will also buy cheese - Confidence: 0.694 - Support: 25 Rule #2 Rule: If a person buys cheese they will also buy bananas - Confidence: 0.659 - Support: 27 Rule #3 Rule: If a person buys bread they will also buy bananas - Confidence: 0.630 - Support: 17  One Rule ç®—æ³• OneR(One Rule)ç®—æ³•æ ¹æ®å·²æœ‰çš„æ•°æ®ä¸­ï¼Œå…·æœ‰ç›¸åŒç‰¹å¾å€¼çš„ä¸ªä½“æœ€å¯èƒ½å±äºå“ªä¸ªç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚One Rule å°±æ˜¯ä»å››ä¸ªç‰¹å¾ä¸­é€‰æ‹©åˆ†ç±»æ•ˆæœæœ€å¥½çš„å“ªä¸ªä½œä¸ºåˆ†ç±»ä¾æ®ã€‚\n å‡å¦‚æ•°æ®é›†çš„æŸä¸€ä¸ªç‰¹å¾å¯ä»¥å– 0 æˆ– 1 ä¸¤ä¸ªå€¼ã€‚æ•°æ®é›†å…±æœ‰ä¸‰ä¸ªç±»åˆ«ã€‚ç‰¹å¾å€¼ä¸º 0 çš„æƒ…å†µä¸‹ï¼ŒA ç±»æœ‰ 20 ä¸ªè¿™æ ·çš„ä¸ªä½“ï¼ŒB ç±»æœ‰ 60 ä¸ªï¼ŒC ç±»ä¹Ÿæœ‰ 20 ä¸ªã€‚é‚£ä¹ˆç‰¹å¾å€¼ä¸º 0 çš„ä¸ªä½“æœ€å¯èƒ½å±äº B ç±»,å½“ç„¶è¿˜æœ‰ 40 ä¸ªä¸ªä½“ç¡®å®æ˜¯ç‰¹å¾å€¼ä¸º 0ï¼Œä½†æ˜¯å®ƒä»¬ä¸å±äº B ç±»ã€‚å°†ç‰¹å¾å€¼ä¸º 0 çš„ä¸ªä½“åˆ†åˆ° B ç±»çš„é”™è¯¯ç‡å°±æ˜¯ 40%ï¼Œå› ä¸ºæœ‰ 40 ä¸ªè¿™æ ·çš„ä¸ªä½“åˆ†åˆ«å±äº A ç±»å’Œ C ç±»ã€‚ç‰¹å¾å€¼ä¸º 1 æ—¶ï¼Œè®¡ç®—æ–¹æ³•ç±»ä¼¼ï¼Œä¸å†èµ˜è¿°ï¼›å…¶ä»–å„ç‰¹å¾å€¼æœ€å¯èƒ½å±äºçš„ç±»åˆ«åŠé”™è¯¯ç‡çš„è®¡ç®—æ–¹æ³•ä¹Ÿä¸€æ ·ã€‚\n # -*- coding: utf-8 -*- import numpy as np from sklearn.datasets import load_iris # Irisæ¤ç‰©åˆ†ç±»æ•°æ®é›† from collections import defaultdict # åˆå§‹åŒ–æ•°æ®å­—å…¸ from operator import itemgetter # å¾—åˆ°ä¸€ä¸ªåˆ—è¡¨çš„åˆ¶å®šå…ƒç´  from sklearn.model_selection import train_test_split # å°†ä¸€ä¸ªæ•°æ®é›†ä¸”åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† from sklearn.metrics import classification_report # åˆ†æé¢„æµ‹ç»“æœ # è¿™é‡Œä¿ç•™å‡½æ•°çš„æ–‡æ¡£æ–¹ä¾¿æŸ¥é˜… def train(X, y_true, feature): \u0026#34;\u0026#34;\u0026#34; Computes the predictors and error for a given feature using the OneR algorithm Parameters ---------- X: array [n_samples, n_features] The two dimensional array that holds the dataset. Each row is a sample, each column is a feature. y_true: array [n_samples,] The one dimensional array that holds the class values. Corresponds to X, such that y_true[i] is the class value for sample X[i]. feature: int An integer corresponding to the index of the variable we wish to test. 0 \u0026lt;= variable \u0026lt; n_features Returns ------- predictors: dictionary of tuples: (value, prediction) For each item in the array, if the variable has a given value, make the given prediction. error: float The ratio of training data that this rule incorrectly predicts. \u0026#34;\u0026#34;\u0026#34; # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­— n_samples, n_features = X.shape assert 0 \u0026lt;= feature \u0026lt; n_features # X[:, feature]ä¸ºnumpyçŸ©é˜µçš„ç´¢å¼•ç”¨æ³•ï¼Œç¬¬ä¸€ç»´ï¼šæ‰€æœ‰æ•°ç»„ï¼Œç¬¬äºŒç»´ï¼šfeatureï¼Œsetå»é‡å¾—åˆ°valueæœ‰å‡ ä¸ªå–å€¼ # è¿™ä¸ªfeatureç‰¹å¾å€¼åœ¨æ¯ä¸ªæ•°æ®ä¸­æœ‰å¤šå°‘ä¸ªå–å€¼ values = set(X[:, feature]) # Stores the predictors array that is returned predictors = dict() errors = [] # å¯¹æ¯ä¸ªç‰¹å¾å€¼çš„æ¯ä¸ªå–å€¼è°ƒç”¨train_feature_valueå‡½æ•°è·å¾—è¯¥å–å€¼å‡ºç°æœ€å¤šçš„ç±»å’Œé”™è¯¯ç‡ for current_value in values: most_frequent_class, error = train_feature_value( X, y_true, feature, current_value) predictors[current_value] = most_frequent_class # è¯¥å–å€¼å‡ºç°æœ€å¤šçš„ç±» errors.append(error) # å­˜å‚¨é”™è¯¯ç‡ total_error = sum(errors) # è¿”å›é¢„æµ‹æ–¹æ¡ˆï¼ˆå³featureçš„å–å€¼åˆ†åˆ«å¯¹åº”å“ªä¸ªç±»åˆ«ï¼‰å’Œæ€»é”™è¯¯ç‡ return predictors, total_error def train_feature_value(X, y_true, feature, value): class_counts = defaultdict(int) # Iterate through each sample and count the frequency of each class/value pair # ç¬¬featureä¸ªç‰¹å¾çš„å€¼ä¸ºvalueçš„æ—¶å€™ï¼Œåœ¨æ¯ä¸ªç§ç±»ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œè¿™é‡Œçš„æ¤ç‰©æœ‰ä¸‰ä¸ªç§ç±» # å› æ­¤æœ€ç»ˆclass_countsæœ‰ä¸‰ä¸ªé”®å€¼å¯¹ for sample, y in zip(X, y_true): if sample[feature] == value: class_counts[y] += 1 # å¯¹class_countä»¥valueç”±å¤§åˆ°å°æ’åˆ— sorted_class_counts = sorted( class_counts.items(), key=itemgetter(1), reverse=True) most_frequent_class = sorted_class_counts[0][0] # å‡ºç°æœ€å¤šæ¬¡çš„ç±» n_samples = X.shape[1] error = sum([class_count for class_value, class_count in class_counts.items( ) if class_value != most_frequent_class]) # errorå°±æ˜¯é™¤å»ä¸Šé¢é‚£ä¸ªç±»çš„å…¶å®ƒvalueçš„å’Œ return most_frequent_class, error # è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»å’Œé”™è¯¯ç‡ def predict(X_test, model): variable = model[\u0026#39;variable\u0026#39;] # ä½¿ç”¨å“ªä¸ªfeatureä½œä¸ºOneRuleè¿›è¡Œé¢„æµ‹ predictor = model[\u0026#39;predictor\u0026#39;] # ä¸€ä¸ªå­—å…¸ï¼Œä¿å­˜ç€featureå–å€¼å¯¹åº”å“ªä¸€ç±» y_predicted = np.array([predictor[int(sample[variable])] for sample in X_test]) return y_predicted # è¿”å›é¢„æµ‹ç»“æœ if __name__ == \u0026#39;__main__\u0026#39;: dataset = load_iris() X = dataset.data y = dataset.target n_samples, n_features = X.shape # è®¡ç®—æ¯ä¸ªå±æ€§çš„å‡å€¼ attribute_means = X.mean(axis=0) assert attribute_means.shape == (n_features,) # å¯¹æ•°æ®é›†ç¦»æ•£åŒ– X_d = np.array(X \u0026gt;= attribute_means, dtype=\u0026#39;int\u0026#39;) random_state = 14 X_train, X_test, y_train, y_test = train_test_split( X_d, y, random_state=random_state) # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† print(\u0026#34;There are {}training samples\u0026#34;.format(y_train.shape)) # è®­ç»ƒé›†æ•°é‡ print(\u0026#34;There are {}testing samples\u0026#34;.format(y_test.shape)) # æµ‹è¯•é›†æ•°é‡ # å¯¹æ¯ä¸ªç‰¹å¾è¿”å›é¢„æµ‹å™¨å’Œé”™è¯¯ç‡[0ï¼š{0: x, 1: x}, sum_errorï¼Œ ...] all_predictors = { variable: train( X_train, y_train, variable) for variable in range( X_train.shape[1])} errors = {variable: error for variable, (mapping, error) in all_predictors.items()} # æŠŠæ¯ä¸ªé¢„æµ‹å™¨çš„å€¼æå–å‡ºæ¥ # æ‰¾å‡ºæœ€å¥½ï¼ˆé”™è¯¯æœ€å°‘ï¼‰çš„é‚£ä¸ªfeatureæ„æˆçš„é¢„æµ‹å™¨ best_variable, best_error = sorted(errors.items(), key=itemgetter(1))[0] print( \u0026#34;The best model is based on variable {0}and has error {1:.2f}%\u0026#34;.format( best_variable, best_error)) # Choose the bset model model = {\u0026#39;variable\u0026#39;: best_variable, \u0026#39;predictor\u0026#39;: all_predictors[best_variable][0]} y_predicted = predict(X_test, model) print(classification_report(y_test, y_predicted)) # ç”Ÿæˆæµ‹è¯•ç»“æœ print(np.mean(y_predicted == y_test) * 100) # é¢„æµ‹æ­£ç¡®ç‡ Outputï¼š\n| | precision | recall | f1-score | support | | 0 | 0.94 | 1.00 | 0.97 | 17 | | 1 | 0.00 | 0.00 | 0.00 | 13 | | 2 | 0.40 | 1.00 | 0.57 | 8 | | | | | | | | accuracy | | | 0.66 | 38 | | macro avg | 0.45 | 0.67 | 0.51 | 38 | | weighted avg | 0.51 | 0.66 | 0.55 | 38 | æ­£ç¡®ç‡ï¼š 65.78947368421053%  ç¬¬äºŒç«  ä¸»è¦å­¦ä¹ æ•°æ®æŒ–æ˜é€šç”¨æ¡†æ¶çš„æ­å»ºæ–¹æ³•\n ä¼°è®¡å™¨(Estimator)ï¼šç”¨äºåˆ†ç±»ã€èšç±»å’Œå›å½’åˆ†æ è½¬æ¢å™¨(Transformer)ï¼šç”¨äºæ•°æ®é¢„å¤„ç†å’Œæ•°æ®è½¬æ¢ æµæ°´çº¿(Pipline)ï¼šç»„åˆæ•°æ®æŒ–æ˜æµç¨‹ï¼Œä¾¿äºå†æ¬¡ä½¿ç”¨  scikit-learn ä¼°è®¡å™¨ ä¼°è®¡å™¨ç”¨äºåˆ†ç±»ï¼Œä¸»è¦åŒ…å«ä¸‹é¢ä¸¤ä¸ªå‡½æ•°ï¼š\n fit(): è®­ç»ƒç®—æ³•ï¼Œè®¾ç½®å†…éƒ¨å‚æ•°ã€‚è¯¥å‡½æ•°æ¥å—è®­ç»ƒé›†å’Œç±»åˆ«ä¸¤ä¸ªå‚æ•° predict(): å‚æ•°ä¸ºæµ‹è¯•é›†ã€‚é¢„æµ‹æµ‹è¯•é›†ç±»åˆ«ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«æµ‹è¯•é›†å„æ¡æ•°æ®ç±»åˆ«çš„æ•°ç»„  è¿‘é‚»ç®—æ³•\n ç”¨é€”å¹¿æ³› è®¡ç®—é‡å¾ˆå¤§  è·ç¦»åº¦é‡\n æ¬§æ°è·ç¦»ï¼šå³çœŸå®è·ç¦» æ›¼å“ˆé¡¿è·ç¦»ï¼šä¸¤ä¸ªç‰¹å¾åœ¨æ ‡å‡†åæ ‡ç³»ä¸­ç»å¯¹è½´è·ä¹‹å’Œ(x1,y1),(x2,y2)å³ abs(x1-x2)+abs(y1-y2) ä½™å¼¦è·ç¦»ï¼šæŒ‡çš„æ˜¯ç‰¹å¾å‘é‡å¤¹è§’çš„ä½™å¼¦å€¼ï¼Œæ›´é€‚åˆè§£å†³å¼‚å¸¸å€¼å’Œæ•°æ®ç¨€ç–é—®é¢˜ã€‚  ç”µç¦»å±‚(Ionosphere)æ•°æ®é›†åˆ†æ\nInput:\n# -*- coding: utf-8 -*- import numpy as np import csv from matplotlib import pyplot as plt from sklearn.neighbors import KNeighborsClassifier # å¯¼å…¥Kè¿‘é‚»åˆ†ç±»å™¨ from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score # å¯¼å…¥äº¤å‰æ£€éªŒçš„ # æŠŠæ¯ä¸ªç‰¹å¾å€¼çš„å€¼åŸŸè§„èŒƒåŒ–åˆ°0ï¼Œ1ä¹‹é—´ï¼Œæœ€å°å€¼ç”¨0ä»£æ›¿ï¼Œæœ€å¤§å€¼ç”¨1ä»£æ›¿ from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline # æµæ°´çº¿ if __name__ == \u0026#39;__main__\u0026#39;: # æ•°æ®é›†å¤§å°å·²çŸ¥æœ‰351è¡Œï¼Œæ¯è¡Œ35ä¸ªå€¼å‰34ä¸ªä¸ºå¤©çº¿é‡‡é›†çš„æ•°æ®ï¼Œæœ€åä¸€ä¸ª g/b è¡¨ç¤ºæ•°æ®çš„å¥½å X = np.zeros((351, 34), dtype=\u0026#39;float\u0026#39;) y = np.zeros((351,), dtype=\u0026#39;bool\u0026#39;) # æ‰“å¼€æ ¹ç›®å½•çš„æ•°æ®é›†æ–‡ä»¶ with open(\u0026#34;ionosphere.data\u0026#34;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as input_file: # åˆ›å»ºcsvé˜…è¯»å™¨å¯¹è±¡ reader = csv.reader(input_file) # ä½¿ç”¨æšä¸¾å‡½æ•°ä¸ºæ¯è¡Œæ•°æ®åˆ›å»ºç´¢å¼• for i, row in enumerate(reader): # è·å–è¡Œæ•°æ®çš„å‰34ä¸ªå€¼ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæµ®ç‚¹å‹ï¼Œä¿å­˜åœ¨Xä¸­ data = [float(datum) for datum in row[:-1]] # Set the appropriate row in our dataset X[i] = data # æ•°æ®é›† # 1 if the class is \u0026#39;g\u0026#39;, 0 otherwise y[i] = row[-1] == \u0026#39;g\u0026#39; # ç±»åˆ« # åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=14) print( \u0026#34;There are {}samples in the training dataset\u0026#34;.format( X_train.shape[0])) print( \u0026#34;There are {}samples in the testing dataset\u0026#34;.format( X_test.shape[0])) print(\u0026#34;Each sample has {}features\u0026#34;.format(X_train.shape[1])) Output:\nThere are 263 samples in the training dataset There are 88 samples in the testing dataset Each sample has 34 features   Input:\n# åˆå§‹åŒ–ä¸€ä¸ªKè¿‘é‚»åˆ†ç±»å™¨å®ä¾‹ï¼Œè¯¥ç®—æ³•é»˜è®¤é€‰æ‹©5ä¸ªè¿‘é‚»ä½œä¸ºåˆ†ç±»ä¾æ® estimator = KNeighborsClassifier() # ç”¨è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒ estimator.fit(X_train, y_train) # ä½¿ç”¨æµ‹è¯•é›†æµ‹è¯•ç®—æ³•ï¼Œè¯„ä»·å…¶è¡¨ç° y_predicted = estimator.predict(X_test) # å‡†ç¡®æ€§ accuracy = np.mean(y_test == y_predicted) * 100 print(\u0026#34;The accuracy is {0:.1f}%\u0026#34;.format(accuracy)) # ä½¿ç”¨äº¤å‰æ£€éªŒçš„æ–¹å¼è·å¾—å¹³å‡å‡†ç¡®æ€§ scores = cross_val_score(estimator, X, y, scoring=\u0026#39;accuracy\u0026#39;) average_accuracy = np.mean(scores) * 100 print(\u0026#34;The average accuracy is {0:.1f}%\u0026#34;.format(average_accuracy)) Output:\nThe accuracy is 86.4% The average accuracy is 82.6%   Input:\n# è®¾ç½®å‚æ•° # å‚æ•°çš„é€‰å–è·Ÿæ•°æ®é›†çš„ç‰¹å¾æ¯æ¯ç›¸å…³ avg_scores = [] all_scores = [] parameter_values = list(range(1, 21)) for n_neighbors in parameter_values: estimator = KNeighborsClassifier(n_neighbors=n_neighbors) scores = cross_val_score(estimator, X, y, scoring=\u0026#39;accuracy\u0026#39;) avg_scores.append(np.mean(scores)) all_scores.append(scores) # ä½œå‡ºn_neighborsä¸åŒå–å€¼å’Œåˆ†ç±»æ­£ç¡®ç‡ä¹‹é—´çš„å…³ç³»çš„æŠ˜çº¿å›¾ plt.figure(figsize=(32, 20)) plt.plot(parameter_values, avg_scores, \u0026#39;-o\u0026#39;, linewidth=5, markersize=24) plt.show() Output:\n result2.1 \nç»è¿‡ä¸Šé¢çš„ä¾‹å­ï¼Œå¯ä»¥æ€»ç»“æ•°æ®æŒ–æ˜æœ€ç®€å•åŸºæœ¬çš„æµç¨‹å¦‚ä¸‹ï¼š\n è½½å…¥æ•°æ®é›†ï¼Œæ•°æ®åˆ†ç±»æå–åˆ°å†…å­˜ä¸­ åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† é€‰æ‹©åˆé€‚çš„ç®—æ³•è¿›è¡Œè®­ç»ƒ ä½¿ç”¨æµ‹è¯•é›†æµ‹è¯•ç®—æ³•ï¼Œè¯„ä¼°å…¶è¡¨ç°  ä¸ºäº†ä¿è¯ç®—æ³•çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥å°†å¤§æ•°æ®é›†åˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†ï¼Œé€šè¿‡äº¤å‰æ£€éªŒçš„æ–¹æ³•æµ‹è¯•ç®—æ³•ã€‚ä½¿ç”¨ cross_val_score å‡½æ•°æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚\nåœ¨å‚æ•°çš„è®¾ç½®ä¸Šï¼Œå¯ä»¥é’ˆå¯¹ä¸åŒçš„å‚æ•°è¿›è¡Œäº¤å‰æµ‹è¯•ï¼Œä½¿ç”¨å›¾è¡¨ç›´è§‚åœ°è¡¨ç¤ºå‡ºå‚æ•°çš„å½±å“ã€‚\næµæ°´çº¿åœ¨é¢„å¤„ç†ä¸­çš„ä½œç”¨ sckit-learn çš„é¢„å¤„ç†å·¥å…·å«åšè½¬æ¢å™¨Transformer\nInput:\n# æ¨¡æ‹Ÿè„æ•°æ® X_broken = np.array(X) X_broken[:, ::2] /= 10 # å¯¹æ¯”ä¸¤ç§æƒ…å†µä¸‹é¢„æµ‹å‡†ç¡®ç‡ estimator = KNeighborsClassifier() original_scores = cross_val_score(estimator, X, y, scoring=\u0026#39;accuracy\u0026#39;) print( \u0026#34;The original average accuracy for is {0:.1f}%\u0026#34;.format( np.mean(original_scores) * 100)) broken_scores = cross_val_score(estimator, X_broken, y, scoring=\u0026#39;accuracy\u0026#39;) print( \u0026#34;The broken average accuracy for is {0:.1f}%\u0026#34;.format( np.mean(broken_scores) * 100)) Output:\nThe original average accuracy for is 82.6% The broken average accuracy for is 73.8%   Input:\n# ç»„åˆæˆä¸ºä¸€ä¸ªå·¥ä½œæµ X_transformed = MinMaxScaler.fit_transform(X_broken) # å®Œæˆè®­ç»ƒå’Œè½¬æ¢ estimator = KNeighborsClassifier() transformed_scores = cross_val_score( estimator, X_transformed, y, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;The average accuracy for is {0:.1f}%\u0026#34;.format( np.mean(transformed_scores) * 100)) Output:\nThe average accuracy for is 82.9%  å°†æ•°æ®ç»è¿‡è§„èŒƒåŒ–åï¼Œæ­£ç¡®ç‡å†æ¬¡æé«˜\nå…¶å®ƒçš„è§„èŒƒåŒ–å‡½æ•°ä¸¾ä¾‹ï¼š\n ä¸ºä½¿æ¯æ¡æ•°æ®å„ç‰¹å¾å€¼çš„å’Œä¸º 1ï¼šsklearn.preprocessing.Normalizer ä¸ºä½¿å„ç‰¹å¾å€¼çš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼šsklearn.preprocessing.StandardScaler ä¸ºå°†æ•°å€¼å‹ç‰¹å¾äºŒå€¼åŒ–ï¼šsklearn.preprocessing.Binarizer  æµæ°´çº¿ sklearn.pipeline.Pipelineç”¨äºåˆ›å»ºæµæ°´çº¿ã€‚æµæ°´çº¿çš„è¾“å…¥ä¸ºä¸€è¿ä¸²çš„æ•°æ®æŒ–æ˜æ­¥éª¤ï¼Œæœ€åä¸€æ­¥å¿…é¡»æ˜¯ä¼°è®¡å™¨ï¼Œå‰å‡ æ­¥æ˜¯è½¬æ¢å™¨ã€‚\nInput:\n# åˆ›å»ºæµæ°´çº¿ # æµæ°´çº¿çš„æ¯ä¸€æ­¥éƒ½ç”¨(\u0026#39;åç§°\u0026#39;,æ­¥éª¤)çš„å…ƒç»„è¡¨ç¤º scaling_pipeline = Pipeline([(\u0026#39;scale\u0026#39;, MinMaxScaler()), # è§„èŒƒç‰¹å¾å–å€¼ (\u0026#39;predict\u0026#39;, KNeighborsClassifier())]) # é¢„æµ‹ # è°ƒç”¨æµæ°´çº¿ scores = cross_val_score(scaling_pipeline, X_broken, y, scoring=\u0026#39;accuracy\u0026#39;) print( \u0026#34;The pipelin scored an average accuracy for is {0:.1f}%\u0026#34;.format( np.mean(scores) * 100)) Output:\nThe pipelin scored an average accuracy for is 82.9%  ç¬¬ä¸‰ç«  å†³ç­–æ ‘ä¹Ÿæ˜¯ä¸€ç§åˆ†ç±»ç®—æ³•ï¼Œå®ƒçš„ä¼˜ç‚¹å¦‚ä¸‹ï¼š\n æœºå™¨å’Œäººéƒ½èƒ½çœ‹æ‡‚ èƒ½å¤Ÿå¤„ç†å¤šç§ä¸åŒçš„ç‰¹å¾  åŠ è½½æ•°æ®é›† pandas(Python Data Analysis çš„ç®€å†™)\né€—å·åˆ†éš”å€¼ï¼ˆComma-Separated Valuesï¼ŒCSVï¼Œæœ‰æ—¶ä¹Ÿç§°ä¸ºå­—ç¬¦åˆ†éš”å€¼ï¼Œå› ä¸ºåˆ†éš”å­—ç¬¦ä¹Ÿå¯ä»¥ä¸æ˜¯é€—å·ï¼‰ï¼Œå…¶æ–‡ä»¶ä»¥çº¯æ–‡æœ¬å½¢å¼å­˜å‚¨è¡¨æ ¼æ•°æ®ï¼ˆæ•°å­—å’Œæ–‡æœ¬ï¼‰ï¼Œæ¥æºç™¾åº¦ç™¾ç§‘ã€‚\nè¿™é‡Œä½¿ç”¨ pandas å¯¼å…¥.csv æ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä¸ª dataframe ï¼ˆæ•°æ®æ¡†ï¼‰çš„ç±»ã€‚å¯¼å…¥ä½¿ç”¨ read_csv() å‡½æ•°ï¼Œå¸¸ç”¨å‚æ•°å¦‚ä¸‹ï¼š\n sep=',' ä»¥,ä¸ºæ•°æ®åˆ†éš”ç¬¦ parse_dates='col_name' å°†æŸä¸ªç‰¹å¾å€¼è¯»å–ä¸ºæ—¥æœŸæ ¼å¼ error_bad_lines=False å½“æŸè¡Œæ•°æ®æœ‰é—®é¢˜æ—¶ï¼Œè·³è¿‡è€Œä¸æŠ¥é”™ skiprows=[\u0026lt;param\u0026gt;] è·³è¿‡åˆ—è¡¨ä¸­æ‰€åŒ…æ‹¬çš„è¡Œï¼Œå‚æ•°å¯ä»¥æ˜¯ 0,1,\u0026hellip;çš„æ•°å­—åºåˆ—ï¼Œä¹Ÿå¯ä»¥ç”¨åˆ‡ç‰‡è¡¨è¾¾å¼[0:] usecols=[\u0026lt;param\u0026gt;] é€‰æ‹©ä½¿ç”¨å“ªå‡ ä¸ªç‰¹å¾å€¼ï¼Œå‚æ•°åŒä¸Š  åœ¨ä½¿ç”¨ dataframe.ix[]è·å– dataframe ä¸­çš„æŸå‡ è¡Œæ•°æ®æ—¶ï¼Œæç¤ºé”™è¯¯ä¿¡æ¯ï¼ŒåŸå› æ˜¯ pandas åœ¨ 0.20.0 ç‰ˆæœ¬åå°±åºŸå¼ƒæ‰äº†è¿™ä¸ªå‡½æ•°ã€‚åœ¨è¿™é‡Œæˆ‘æ”¹ä¸ºä½¿ç”¨ iloc å‡½æ•°ã€‚\nInput:\n# -*- coding: utf-8 -*- import numpy as np import pandas as pd from collections import defaultdict from sklearn.tree import DecisionTreeClassifier # åˆ›å»ºå†³ç­–æ ‘çš„ç±» from sklearn.model_selection import cross_val_score from sklearn.preprocessing import LabelEncoder # èƒ½å°†å­—ç¬¦ä¸²ç±»å‹çš„ç‰¹å¾è½¬åŒ–æˆæ•´å‹ from sklearn.preprocessing import OneHotEncoder # å°†ç‰¹å¾è½¬åŒ–ä¸ºäºŒè¿›åˆ¶æ•°å­— from sklearn.ensemble import RandomForestClassifier # éšæœºæ£®æ— from sklearn.model_selection import GridSearchCV # ç½‘æ ¼æœç´¢ï¼Œæ‰¾åˆ°æœ€ä½³å‚æ•° if __name__ == \u0026#39;__main__\u0026#39;: # æ¸…æ´—æ•°æ®é›† results = pd.read_csv( \u0026#34;NBA_data.csv\u0026#34;, parse_dates=[\u0026#34;Date\u0026#34;], skiprows=[ 0, ], usecols=[ 0, 2, 3, 4, 5, 6, 7, 9]) # åŠ è½½æ•°æ®é›† # ä¿®å¤æ•°æ®ç‰¹å¾å results.columns = [ \u0026#34;Date\u0026#34;, \u0026#34;Visitor Team\u0026#34;, \u0026#34;VisitorPts\u0026#34;, \u0026#34;Home Team\u0026#34;, \u0026#34;HomePts\u0026#34;, \u0026#34;Score Type\u0026#34;, \u0026#34;OT?\u0026#34;, \u0026#34;Notes\u0026#34;] # results.ix[]å·²è¢«å¼ƒç”¨ print(results.loc[:5]) # æŸ¥çœ‹æ•°æ®é›†å‰äº”è¡Œ Output:\n Date Visitor Team VisitorPts ... Score Type OT? Notes 0 2013-10-29 Orlando Magic 87 ... Box Score NaN NaN 1 2013-10-29 Chicago Bulls 95 ... Box Score NaN NaN 2 2013-10-29 Los Angeles Clippers 103 ... Box Score NaN NaN 3 2013-10-30 Brooklyn Nets 94 ... Box Score NaN NaN 4 2013-10-30 Boston Celtics 87 ... Box Score NaN NaN 5 2013-10-30 Miami Heat 110 ... Box Score NaN NaN [6 rows x 8 columns]  å†³ç­–æ ‘ åˆ›å»ºæ–°çš„ç‰¹å¾åˆ—ï¼Œå¯ä»¥ä»æ•°æ®é›†ä¸­å¯¼å…¥ï¼š\ndataset[\u0026quot;New Feature\u0026quot;] = feature_creator()\nä¹Ÿå¯ä»¥ä¸€å¼€å§‹ä¸ºæ–°ç‰¹å¾å€¼è®¾ç½®é»˜è®¤çš„å€¼ï¼š\ndataset[\u0026quot;My New Feature\u0026quot;] = 0\nè¿™é‡Œçš„ X_previouswins = results[[\u0026quot;HomeLastWin\u0026quot;, \u0026quot;VisitorLastWin\u0026quot;]].values ç”Ÿæˆä¸€ä¸ªæ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†æœ‰ä¸¤ä¸ªç‰¹å¾\nDecisionTreeClassifier() ç”¨æ¥åˆ›å»ºå†³ç­–æ ‘ï¼Œå¸¸ç”¨å‚æ•°å¦‚ä¸‹ï¼š\n min_samples_split: æŒ‡å®šäº†åˆ›å»ºä¸€ä¸ªæ–°èŠ‚ç‚¹è‡³å°‘éœ€è¦å¤šå°‘ä¸ªä¸ªä½“ min_samples_leaf: æŒ‡å®šä¸ºäº†ä¿ç•™èŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹è‡³å°‘åº”è¯¥åŒ…å«çš„ä¸ªä½“æ•°é‡ åˆ›å»ºå†³ç­–çš„æ ‡å‡†: åŸºå°¼ä¸çº¯åº¦/ä¿¡æ¯å¢ç›Š  Input:\n# æå–æ–°ç‰¹å¾ï¼Œå€¼ä¸ºè¿™åœºä¸­ä¸»åœºé˜Ÿä¼æ˜¯å¦èƒœåˆ© results[\u0026#34;HomeWin\u0026#34;] = results[\u0026#34;VisitorPts\u0026#34;] \u0026lt; results[\u0026#34;HomePts\u0026#34;] y_true = results[\u0026#34;HomeWin\u0026#34;].values # èƒœè´Ÿæƒ…å†µ # åˆ›å»ºä¸¤ä¸ªæ–°featureï¼Œåˆå§‹å€¼éƒ½è®¾ä¸º0ï¼Œä¿å­˜è¿™åœºæ¯”èµ›çš„ä¸¤ä¸ªé˜Ÿä¼ä¸Šåœºæ¯”èµ›çš„æƒ…å†µ results[\u0026#34;HomeLastWin\u0026#34;] = False results[\u0026#34;VisitorLastWin\u0026#34;] = False won_last = defaultdict(int) for index, row in results.iterrows(): home_team = row[\u0026#34;Home Team\u0026#34;] visitor_team = row[\u0026#34;Visitor Team\u0026#34;] # è¿™åœºæ¯”èµ›ä¹‹å‰ä¸¤ä¸ªçƒé˜Ÿä¸Šæ¬¡æ˜¯å¦è·èƒœä¿å­˜åœ¨resultä¸­ row[\u0026#34;HomeLastWin\u0026#34;] = won_last[home_team] row[\u0026#34;VisitorLastWin\u0026#34;] = won_last[visitor_team] results.iloc[index] = row # è¿™åœºæ¯”èµ›çš„ç»“æœæ›´æ–°won_lastä¸­çš„æƒ…å†µ won_last[home_team] = row[\u0026#34;HomeWin\u0026#34;] won_last[visitor_team] = not row[\u0026#34;HomeWin\u0026#34;] X_previouswins = results[[\u0026#34;HomeLastWin\u0026#34;, \u0026#34;VisitorLastWin\u0026#34;]].values # åˆ›å»ºå†³ç­–æ ‘ç”Ÿæˆå™¨å®ä¾‹ clf = DecisionTreeClassifier(random_state=14) # äº¤å‰è®­ç»ƒ scores = cross_val_score(clf, X_previouswins, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;Using just the last result from the home and visitor teams\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) Output:\nUsing just the last result from the home and visitor teams Accuracy: 56.4%   è¿™é‡Œä¸ºäº†åˆ›å»ºä¸€ä¸ªæ–°çš„ç‰¹å¾å¯¼å…¥äº†ä¸Šä¸€å¹´çš„ NBA æ’åã€‚\nInput:\nladder = pd.read_csv(\u0026#34;NBA_standings.csv\u0026#34;, skiprows=[0, ]) # åˆ›å»ºä¸€ä¸ªæ–°ç‰¹å¾ï¼Œä¸¤ä¸ªé˜Ÿä¼åœ¨ä¸Šä¸ªèµ›å­£çš„æ’åå“ªä¸ªæ¯”è¾ƒé«˜ results[\u0026#34;HomeTeamRanksHigher\u0026#34;] = 0 for index, row in results.iterrows(): home_team = row[\u0026#34;Home Team\u0026#34;] visitor_team = row[\u0026#34;Visitor Team\u0026#34;] # è¿™ä¸ªçƒé˜Ÿæ”¹åäº† if home_team == \u0026#34;New Orleans Pelicans\u0026#34;: home_team = \u0026#34;New Orleans Hornets\u0026#34; elif visitor_team == \u0026#34;New Orleans Pelicans\u0026#34;: visitor_team = \u0026#34;New Orleans Hornets\u0026#34; # è¿™é‡Œæºä»£ç æ— æ³•è¿è¡Œï¼Œå°‘åŠ äº†ä¸€ä¸ªæ‹¬å· ladder[(ladder[\u0026#34;Team\u0026#34;] == home_team)] è¡¨ç¤ºæ ¹æ®æ¡ä»¶è·å–è¿™ä¸€è¡Œçš„æ•°æ® home_row = ladder[(ladder[\u0026#34;Team\u0026#34;] == home_team)] visitor_row = ladder[(ladder[\u0026#34;Team\u0026#34;] == visitor_team)] home_rank = home_row[\u0026#34;Rk\u0026#34;].values[0] visitor_rank = visitor_row[\u0026#34;Rk\u0026#34;].values[0] row[\u0026#34;HomeTeamRanksHigher\u0026#34;] = int(home_rank \u0026gt; visitor_rank) results.iloc[index] = row X_homehigher = results[[\u0026#34;HomeLastWin\u0026#34;, \u0026#34;VisitorLastWin\u0026#34;, \u0026#34;HomeTeamRanksHigher\u0026#34;]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_homehigher, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;Using whether the home team is ranked higher\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) Output:\nUsing whether the home team is ranked higher Accuracy: 60.0%   Input:\n# åˆ›å»ºæ–°ç‰¹å¾ï¼Œä¸¤ä¸ªé˜Ÿä¼ä¸Šä¸€æ¬¡è¿›è¡Œæ¯”èµ›æ—¶çš„è·èƒœè€… last_match_winner = defaultdict(int) results[\u0026#34;HomeTeamWonLast\u0026#34;] = 0 for index, row in results.iterrows(): home_team = row[\u0026#34;Home Team\u0026#34;] visitor_team = row[\u0026#34;Visitor Team\u0026#34;] # æŒ‰ç…§è‹±æ–‡å­—æ¯è¡¨æ’åºï¼Œä¸å»è€ƒè™‘å“ªä¸ªæ˜¯ä¸»åœºçƒé˜Ÿ teams = tuple(sorted([home_team, visitor_team])) # æ‰¾åˆ°ä¸¤æ”¯çƒé˜Ÿä¸Šæ¬¡æ¯”èµ›çš„èµ¢å®¶ï¼Œæ›´æ–°æ¡†ä¸­çš„æ•°æ®ï¼Œåˆå§‹ä¸º0 # è¿™é‡Œçš„HomeTeamWonLastè·Ÿä¸»åœºå®¢åœºæ²¡æœ‰ä»€ä¹ˆå…³ç³»ï¼Œä¹Ÿå¯ä»¥å«WhichTeamWonLastï¼Œè¿™é‡Œä¸ºäº†å’Œæºç å°½é‡ä¿æŒä¸€è‡´ä½¿ç”¨äº†æºç  row[\u0026#34;HomeTeamWonLast\u0026#34;] = 1 if last_match_winner[teams] == row[\u0026#34;Home Team\u0026#34;] else 0 results.iloc[index] = row winner = row[\u0026#34;Home Team\u0026#34;] if row[\u0026#34;HomeWin\u0026#34;] else row[\u0026#34;Visitor Team\u0026#34;] # å°†ä¸¤ä¸ªçƒé˜Ÿä¸Šæ¬¡é‡è§æ¯”èµ›çš„æƒ…å†µå­˜åˆ°å­—å…¸ä¸­å» last_match_winner[teams] = winner X_home_higher = results[[\u0026#34;HomeTeamRanksHigher\u0026#34;, \u0026#34;HomeTeamWonLast\u0026#34;]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_home_higher, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;Using whether the home team is ranked higher\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) Output:\nUsing whether the home team is ranked higher Accuracy: 59.9%  éšæœºæ£®æ— LabelEncoder() ç”¨æ¥å°†ä¸€ä¸ªå­—ç¬¦ä¸²å‹çš„ç‰¹å¾è½¬åŒ–ä¸ºæ•´å‹\nOneHotEncoder() å°†æ•´æ•°è½¬åŒ–æˆæ¶ˆé™¤å·®å¼‚çš„äºŒè¿›åˆ¶æ•°å­—ï¼Œå³å°† 1,2,3 è½¬æ¢æˆ 001,010,100\nstacking ï¼ˆå‘é‡ç»„åˆï¼‰ï¼Œè¿™é‡Œ np.vstack() å°†ä¸¤ä¸ªé˜Ÿä¼åå‘é‡çºµå‘ç»„åˆæˆä¸€ä¸ªçŸ©é˜µ.Tè¡¨ç¤ºå°†çŸ©é˜µè½¬ç½®\nå†³ç­–æ ‘å­˜åœ¨çš„é—®é¢˜ï¼š\n åˆ›å»ºçš„å¤šé¢—å†³ç­–æ ‘åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç›¸åŒçš„ï¼Œè®­ç»ƒé›†ç›¸åŒï¼Œåˆ™ç”Ÿæˆçš„å†³ç­–æ ‘ä¹Ÿç›¸åŒã€‚ä¸€ä¸ªè§£å†³åŠæ³•æ˜¯è£…è¢‹(bagging) ç”¨äºå‰å‡ ä¸ªå†³ç­–èŠ‚ç‚¹çš„ç‰¹å¾éå¸¸çªå‡ºï¼Œå³ä½¿é‡‡ç”¨ä¸åŒçš„è®­ç»ƒé›†ï¼Œåˆ›å»ºçš„å†³ç­–æ ‘ç›¸ä¼¼æ€§ä¾æ—§å¾ˆå¤§ã€‚è§£å†³åŠæ³•æ˜¯éšæœºé€‰å–éƒ¨åˆ†ç‰¹å¾ä½œä¸ºå†³ç­–æ•°æ®  RandomForestClassifier() ç”¨æ¥è°ƒç”¨éšæœºæ£®æ—ç®—æ³•ï¼Œå› ä¸ºå®ƒè°ƒç”¨äº† DecisionTreeClassifier çš„å¤§é‡å®ä¾‹ï¼Œæ‰€ä»¥ä»–ä»¬çš„å‚æ•°æœ‰å¾ˆå¤šæ˜¯ä¸€è‡´çš„ã€‚å…¶å¼•å…¥çš„ä¸€éƒ¨åˆ†æ–°å‚æ•°å¦‚ä¸‹ï¼š\n n_estimators ç”¨æ¥æŒ‡å®šåˆ›å»ºå†³ç­–æ ‘çš„æ•°é‡ï¼Œå€¼è¶Šé«˜ï¼Œè€—æ—¶è¶Šé•¿ï¼Œå‡†ç¡®ç‡(å¯èƒ½)è¶Šé«˜ oob_score å¦‚æœè®¾ç½®ä¸ºçœŸï¼Œæµ‹è¯•æ—¶å°†ä¸é€‚ç”¨è®­ç»ƒæ¨¡å‹æ—¶ç”¨è¿‡çš„æ•°æ® n_jobs é‡‡ç”¨å¹¶è¡Œç®—æ³•è®­ç»ƒæ—¶æ‰€ç”¨åˆ°çš„å†…æ ¸æ•°é‡ï¼Œè®¾ç½®ä¸º -1 åˆ™å¯ç”¨å…¨éƒ¨å†…æ ¸  Input:\n# åˆ›å»ºä¸€ä¸ªè½¬åŒ–å™¨å®ä¾‹ encoding = LabelEncoder() # å°†çƒé˜Ÿåè½¬åŒ–ä¸ºæ•´å‹ encoding.fit(results[\u0026#34;Home Team\u0026#34;].values) # æŠ½å–æ‰€æœ‰æ¯”èµ›ä¸­ä¸»å®¢åœºçƒé˜Ÿçš„çƒé˜Ÿåï¼Œç»„åˆèµ·æ¥å½¢æˆä¸€ä¸ªçŸ©é˜µ home_teams = encoding.transform(results[\u0026#34;Home Team\u0026#34;].values) visitor_teams = encoding.transform(results[\u0026#34;Visitor Team\u0026#34;].values) # å»ºç«‹è®­ç»ƒé›†ï¼Œ[[\u0026#34;Home Team Feature\u0026#34;ï¼Œ\u0026#34;Visitor Team Feature\u0026#34;],[\u0026#34;Home Team Feature\u0026#34;ï¼Œ\u0026#34;Visitor Team Feature\u0026#34;]...] X_teams = np.vstack([home_teams, visitor_teams]).T # åˆ›å»ºè½¬åŒ–å™¨å®ä¾‹ onehot = OneHotEncoder() # ç”Ÿæˆè½¬åŒ–åçš„ç‰¹å¾ X_teams = onehot.fit_transform(X_teams).todense() clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_teams, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_teams, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;Using full team labels is ranked higher\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) Output:\nAccuracy: 60.5% Using full team labels is ranked higher Accuracy: 61.4%   å°†ä¸Šé¢ç”Ÿæˆçš„ç‰¹å¾æ•´åˆèµ·æ¥ï¼Œåˆ›å»ºæ–°çš„å†³ç­–æ–¹æ¡ˆ\nè¿™é‡Œä½¿ç”¨ np.hstack()æ¨ªå‘æ‹¼æ¥ä¸¤ä¸ªå†³ç­–æ–¹æ¡ˆçŸ©é˜µ\nInput:\nX_all = np.hstack([X_home_higher, X_teams]) # å°†ä¸Šé¢è®¡ç®—çš„ç‰¹å¾è¿›è¡Œç»„åˆ print(X_all.shape) scores = cross_val_score(clf, X_all, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;Using whether the home team is ranked higher\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) Output:\n(1319, 62) Using whether the home team is ranked higher Accuracy: 61.6%   ä½¿ç”¨ GridSearchCV ï¼ˆç½‘æ ¼æœç´¢ï¼‰æœç´¢æœ€ä½³å‚æ•°\nInput:\n# è®¾ç½®å‚æ•°æœç´¢èŒƒå›´ parameter_space = { \u0026#34;max_features\u0026#34;: [2, 10, \u0026#39;auto\u0026#39;], \u0026#34;n_estimators\u0026#34;: [100, ], \u0026#34;criterion\u0026#34;: [\u0026#34;gini\u0026#34;, \u0026#34;entropy\u0026#34;], \u0026#34;min_samples_leaf\u0026#34;: [2, 4, 6], } grid = GridSearchCV(clf, parameter_space) grid.fit(X_all, y_true) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(grid.best_score_ * 100)) # è¾“å‡ºæœ€ä½³æ–¹æ¡ˆ print(grid.best_estimator_) Output:\nAccuracy: 65.6% RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=14, verbose=0, warm_start=False)  è¯¾åç»ƒä¹  æ‹¿åˆ°äº†æ•°æ®ï¼Œå¦‚ä½•åˆ›å»ºæ–°çš„ç‰¹å¾ï¼Œå¦‚ä½•åœ¨æ•°æ®ä¸­å‘ç°å…¶å…³é”®ç‚¹ï¼Œå¦‚ä½•æ‰¾å‡ºæ•°æ®å†…éƒ¨çš„è”ç³»ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªéœ€è¦æ–Ÿé…Œçš„æ–¹é¢\nåˆ›å»ºä¸‹è¿°ç‰¹å¾å¹¶çœ‹ä¸€ä¸‹æ•ˆæœ:\n çƒé˜Ÿä¸Šæ¬¡æ‰“æ¯”èµ›è·ä»Šæœ‰å¤šé•¿æ—¶é—´ï¼Ÿ ä¸¤æ”¯çƒé˜Ÿè¿‡å»äº”åœºæ¯”èµ›ç»“æœå¦‚ä½•ï¼Ÿ çƒé˜Ÿæ˜¯ä¸æ˜¯è·ŸæŸæ”¯ç‰¹å®šçƒé˜Ÿæ‰“æ¯”èµ›æ—¶å‘æŒ¥æ›´å¥½ï¼Ÿ  åœ¨è¿™é‡Œä½¿ç”¨äº†ä¸Šé¢ä¹¦ä¸­çš„æ–¹æ³•ï¼Œå®Œæˆäº†å‰ä¸¤ä¸ªç‚¹ï¼Œç¬¬ä¸‰ä¸ªç‚¹å®ç°èµ·æ¥æœ‰ç‚¹éº»çƒ¦ï¼Œç°åœ¨åªæœ‰ä¸€ä¸ªæ€è·¯ï¼šå»ºç«‹ä¸€ä¸ªå­—å…¸ï¼Œæ•°æ®å½¢å¼ä¸º (ä¸¤æ”¯çƒé˜Ÿå»ºç«‹ä¸€ä¸ªå…ƒç»„:(å‰ä¸€ä¸ªé˜Ÿä¼è·èƒœçš„æ¬¡æ•°ï¼Œåä¸€ä¸ªé˜Ÿä¼è·èƒœçš„æ¬¡æ•°))\nåœ¨å¤„ç† dataset ä¸­çš„æ•°æ®é¡¹æ—¶ï¼Œå¯¹äº pandas ä¸­çš„ Timestamp ç±»å‹æ²¡æœ‰äº†è§£ï¼Œè€—è´¹äº†å¤ªé•¿æ—¶é—´ï¼ŒæŸ¥é˜…æ–‡æ¡£åå‘ç°å¯ä»¥ç”¨ date() å°†å…¶è½¬åŒ–ä¸º datetime.date æ—¥æœŸã€‚\nä½¿ç”¨å‰ä¸¤ä¸ªç‰¹å¾ä½œä¸ºå†³ç­–æ ‡å‡†æ—¶ï¼Œæ•ˆæœè¿˜ç®—å¯ä»¥ï¼ŒåŠ ä¸Šä¹¦ä¸Šçš„æ‰€æœ‰ç‰¹å¾åï¼Œå‡†ç¡®ç‡åè€Œè¾ƒä¸Šé¢çš„ç»“æœé™ä½äº†ã€‚ï¼ˆä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼‰\nè¿™ä¸ªâ€œè¯¾åç»ƒä¹ â€ä½¿æˆ‘å¯¹äºæ ‡å‡†åº“äº†è§£åŒ®ä¹çš„çŸ­æ¿æ˜¾ç°å‡ºæ¥ï¼Œè¦æŠ½å‡ºæ—¶é—´å­¦ä¹ ä¸€ä¸‹ python, numpy å’Œ pandas æ ‡å‡†åº“ä¸­å¸¸ç”¨å‡½æ•°åŠå…¶å‚æ•°ã€‚\nInput:\n#!/usr/bin/env python3 # -*- coding: utf-8 -*- import datetime import numpy as np import pandas as pd from collections import defaultdict from sklearn.model_selection import cross_val_score from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from ch3.nba_test import X_all from sklearn.model_selection import GridSearchCV # ç½‘æ ¼æœç´¢ï¼Œæ‰¾åˆ°æœ€ä½³å‚æ•° if __name__ == \u0026#39;__main__\u0026#39;: \u0026#34;\u0026#34;\u0026#34; - çƒé˜Ÿä¸Šæ¬¡æ‰“æ¯”èµ›è·ä»Šæœ‰å¤šé•¿æ—¶é—´ï¼Ÿ - ä¸¤æ”¯çƒé˜Ÿè¿‡å»äº”åœºæ¯”èµ›ç»“æœå¦‚ä½•ï¼Ÿ - çƒé˜Ÿæ˜¯ä¸æ˜¯è·ŸæŸæ”¯ç‰¹å®šçƒé˜Ÿæ‰“æ¯”èµ›æ—¶å‘æŒ¥æ›´å¥½ï¼Ÿ \u0026#34;\u0026#34;\u0026#34; dataset = pd.read_csv( \u0026#34;NBA_data.csv\u0026#34;, parse_dates=[\u0026#34;Date\u0026#34;], skiprows=[ 0, ], usecols=[ 0, 2, 3, 4, 5, 6, 7, 9]) # åŠ è½½æ•°æ®é›† dataset.columns = [ \u0026#34;Date\u0026#34;, \u0026#34;Visitor Team\u0026#34;, \u0026#34;VisitorPts\u0026#34;, \u0026#34;Home Team\u0026#34;, \u0026#34;HomePts\u0026#34;, \u0026#34;Score Type\u0026#34;, \u0026#34;OT?\u0026#34;, \u0026#34;Notes\u0026#34;] dataset[\u0026#34;HomeWin\u0026#34;] = dataset[\u0026#34;VisitorPts\u0026#34;] \u0026lt; dataset[\u0026#34;HomePts\u0026#34;] y_true = dataset[\u0026#34;HomeWin\u0026#34;].values # èƒœè´Ÿæƒ…å†µ # ä¿å­˜ä¸Šæ¬¡æ‰“æ¯”èµ›çš„æ—¶é—´ last_played_date = defaultdict(datetime.date) # æ‰‹åŠ¨ä¸ºæ¯ä¸ªçƒé˜Ÿåˆå§‹åŒ– for team in set(dataset[\u0026#34;Home Team\u0026#34;]): last_played_date[team] = datetime.date(year=2013, month=10, day=25) # ä¸¤æ”¯çƒé˜Ÿè¿‡å»çš„æ¯”èµ›ç»“æœï¼Œæ¯ä¸ªçƒé˜Ÿçš„æ•°æ®æ˜¯[True,False,,,]çš„åºåˆ— last_five_games = defaultdict(list) # å­˜æ”¾Homeå’ŒVisitorå‰äº”æ¬¡æ¯”èµ›çš„è·èƒœæ¬¡æ•° dataset[\u0026#34;HWinTimes\u0026#34;] = 0 dataset[\u0026#34;VWinTimes\u0026#34;] = 0 # å­˜æ”¾è·ç¦»ä¸Šæ¬¡æ¯”èµ›çš„æ—¶é—´é—´éš”ï¼Œç”¨å¤©è®¡æ•° dataset[\u0026#34;HLastPlayedSpan\u0026#34;] = 0 dataset[\u0026#34;VLastPlayedSpan\u0026#34;] = 0 for index, row in dataset.iterrows(): home_team = row[\u0026#34;Home Team\u0026#34;] visitor_team = row[\u0026#34;Visitor Team\u0026#34;] row[\u0026#34;HWinTimes\u0026#34;] = sum(last_five_games[home_team][-5:]) row[\u0026#34;VWinTimes\u0026#34;] = sum(last_five_games[visitor_team][-5:]) row[\u0026#34;HLastPlayedSpan\u0026#34;] = ( row[\u0026#34;Date\u0026#34;].date() - last_played_date[home_team]).days row[\u0026#34;VLastPlayedSpan\u0026#34;] = ( row[\u0026#34;Date\u0026#34;].date() - last_played_date[visitor_team]).days dataset.iloc[index] = row last_played_date[home_team] = row[\u0026#34;Date\u0026#34;].date() last_played_date[visitor_team] = row[\u0026#34;Date\u0026#34;].date() last_five_games[home_team].append(row[\u0026#34;HomeWin\u0026#34;]) last_five_games[visitor_team].append(not row[\u0026#34;HomeWin\u0026#34;]) X_1 = dataset[[\u0026#34;HLastPlayedSpan\u0026#34;, \u0026#34;VLastPlayedSpan\u0026#34;, \u0026#34;HWinTimes\u0026#34;, \u0026#34;VWinTimes\u0026#34;]].values clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_1, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;DecisionTree: Using time span and win times\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_1, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;RandomForest: Using time span and win times\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) print(\u0026#34;---------------------------------\u0026#34;) X_all = np.hstack([X_1, X_all]) clf = DecisionTreeClassifier(random_state=14) scores = cross_val_score(clf, X_all, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;DecisionTree: Using time span and win times\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) clf = RandomForestClassifier(random_state=14, n_jobs=-1) scores = cross_val_score(clf, X_all, y_true, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#34;RandomForest: Using time span and win times\u0026#34;) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(np.mean(scores) * 100)) print(\u0026#34;---------------------------------\u0026#34;) parameter_space = { \u0026#34;max_features\u0026#34;: [2, 10, \u0026#39;auto\u0026#39;], \u0026#34;n_estimators\u0026#34;: [100, ], \u0026#34;criterion\u0026#34;: [\u0026#34;gini\u0026#34;, \u0026#34;entropy\u0026#34;], \u0026#34;min_samples_leaf\u0026#34;: [2, 4, 6], } grid = GridSearchCV(clf, parameter_space) grid.fit(X_all, y_true) print(\u0026#34;Accuracy: {0:.1f}%\u0026#34;.format(grid.best_score_ * 100)) print(grid.best_estimator_) Output:\nDecisionTree: Using time span and win times Accuracy: 56.4% RandomForest: Using time span and win times Accuracy: 58.3% --------------------------------- DecisionTree: Using time span and win times Accuracy: 57.2% RandomForest: Using time span and win times Accuracy: 61.0% --------------------------------- Accuracy: 64.6% RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='entropy', max_depth=None, max_features=2, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=14, verbose=0, warm_start=False)   æ•°æ®é›†æƒ…å†µ \nç¬¬å››ç«  æœ¬ç« é‡ç‚¹ï¼š\n äº²å’Œæ€§åˆ†æ ç”¨ Apriori ç®—æ³•æŒ–æ˜å…³è”ç‰¹å¾ æ•°æ®ç¨€ç–é—®é¢˜  äº²å’Œæ€§åˆ†æ Apriori ç®—æ³•æ˜¯ç»å…¸çš„äº²å’Œæ€§åˆ†æç®—æ³•ï¼Œå®ƒåªä»æ•°æ®é›†ä¸­é¢‘ç¹å‡ºç°çš„å•†å“ä¸­é€‰å–å‡ºå…±åŒå‡ºç°çš„å•†å“ç»„æˆé¢‘ç¹é¡¹é›†ï¼Œé¿å…äº†å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿çš„é—®é¢˜ã€‚ä¸€æ—¦æ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œç”Ÿæˆå…³è”è§„åˆ™å°±å˜å¾—å®¹æ˜“äº†ã€‚\nåŸç†ï¼šç¡®ä¿äº†è§„åˆ™åœ¨æ•°æ®é›†ä¸­æœ‰è¶³å¤Ÿçš„æ”¯æŒåº¦ã€‚Apriori ç®—æ³•ä¸€ä¸ªé‡è¦å‚æ•°å°±æ˜¯æœ€å°æ”¯æŒåº¦ï¼Œå¦‚æœæƒ³è¦ç”Ÿæˆ(A,B,C)çš„é¢‘ç¹é¡¹é›†ï¼Œåˆ™å…¶å­é›†å¿…é¡»éƒ½è¦æ»¡è¶³æœ€å°æ”¯æŒåº¦æ ‡å‡†ã€‚\nå…¶å®ƒäº²å’Œæ€§ç®—æ³•è¿˜æœ‰ Eclat å’Œé¢‘ç¹é¡¹é›†æŒ–æ˜ç®—æ³•(FP-growth)ã€‚è¿™äº›ç®—æ³•æ¯”èµ·åŸºç¡€çš„ Apriori ç®—æ³•æœ‰å¾ˆå¤šæ”¹è¿›ï¼Œæ€§èƒ½ä¹Ÿæœ‰è¿›ä¸€æ­¥æå‡ã€‚\nç¬¬ä¸€é˜¶æ®µï¼Œä¸º Apriori ç®—æ³•æŒ‡å®šä¸€ä¸ªé¡¹é›†è¦æˆä¸ºé¢‘ç¹é¡¹é›†æ‰€éœ€çš„æœ€å°æ”¯æŒåº¦ã€‚ç¬¬äºŒé˜¶æ®µï¼Œæ ¹æ®ç½®ä¿¡åº¦å–å…³è”è§„åˆ™ï¼Œè®¾å®šæœ€å°ç½®ä¿¡åº¦ï¼Œè¿”å›å¤§äºæ­¤å€¼çš„è§„åˆ™ã€‚\nç”µå½±æ¨èé—®é¢˜ ä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†\nInput:\nimport sys import pandas as pd from collections import defaultdict from operator import itemgetter if __name__ == \u0026#39;__main__\u0026#39;: # header=None ä¸æŠŠç¬¬ä¸€è¡Œå½“åšè¡¨å¤´ all_ratings = pd.read_csv( \u0026#34;ml-100k/u.data\u0026#34;, delimiter=\u0026#34;\\t\u0026#34;, header=None, names=[ \u0026#34;UserID\u0026#34;, \u0026#34;MovieID\u0026#34;, \u0026#34;Rating\u0026#34;, \u0026#34;Datetime\u0026#34;]) # è½¬åŒ–æ—¶é—´æˆ³ä¸ºdatetime all_ratings[\u0026#34;Datetime\u0026#34;] = pd.to_datetime(all_ratings[\u0026#34;Datetime\u0026#34;], unit=\u0026#39;s\u0026#39;) # è¾“å‡ºç”¨æˆ·-ç”µå½±-è¯„åˆ†ç¨€ç–çŸ©é˜µ print(all_ratings[:5]) print() # åˆ›å»ºFavoriteç‰¹å¾ï¼Œå°†è¯„åˆ†å±æ€§äºŒå€¼åŒ–ä¸ºæ˜¯å¦å–œæ¬¢ all_ratings[\u0026#34;Favorable\u0026#34;] = all_ratings[\u0026#34;Rating\u0026#34;] \u0026gt; 3 # å–ç”¨æˆ·IDä¸ºå‰200çš„ç”¨æˆ·çš„æ‰“åˆ†æ•°æ® ratings = all_ratings[all_ratings[\u0026#34;UserID\u0026#34;].isin(range(200))] favorable_ratings = ratings[ratings[\u0026#34;Favorable\u0026#34;]] # åˆ›å»ºç”¨æˆ·å–œæ¬¢å“ªäº›ç”µå½±çš„å­—å…¸ favorable_reviews_by_users = dict( (k, frozenset( v.values)) for k, v in favorable_ratings.groupby(\u0026#34;UserID\u0026#34;)[\u0026#34;MovieID\u0026#34;]) # åˆ›å»ºä¸€ä¸ªæ•°æ®æ¡†ï¼Œäº†è§£æ¯éƒ¨ç”µå½±çš„å½±è¿·æ•°é‡ num_favorable_by_movie = ratings[[ \u0026#34;MovieID\u0026#34;, \u0026#34;Favorable\u0026#34;]].groupby(\u0026#34;MovieID\u0026#34;).sum() # æŸ¥çœ‹æœ€å—æ¬¢è¿çš„äº”éƒ¨ç”µå½± print(num_favorable_by_movie.sort_values(\u0026#34;Favorable\u0026#34;, ascending=False)[:5]) Output:\nUserID MovieID Rating Datetime 0 196 242 3 1997-12-04 15:55:49 1 186 302 3 1998-04-04 19:22:22 2 22 377 1 1997-11-07 07:18:36 3 244 51 2 1997-11-27 05:02:03 4 166 346 1 1998-02-02 05:33:16 Favorable MovieID 50 100.0 100 89.0 258 83.0 181 79.0 174 74.0  Apriori ç®—æ³•çš„å®ç°  æŠŠå„é¡¹ç›®æ”¾åˆ°åªåŒ…å«è‡ªå·±çš„é¡¹é›†ä¸­ï¼Œç”Ÿæˆæœ€åˆçš„é¢‘ç¹é¡¹é›†ã€‚åªä½¿ç”¨è¾¾åˆ°æœ€å°æ”¯æŒåº¦çš„é¡¹ç›®ã€‚ æŸ¥æ‰¾ç°æœ‰é¢‘ç¹é¡¹é›†çš„è¶…é›†ï¼Œå‘ç°æ–°çš„é¢‘ç¹é¡¹é›†ï¼Œå¹¶ç”¨å…¶ç”Ÿæˆæ–°çš„å¤‡é€‰é¡¹é›†ã€‚ æµ‹è¯•æ–°ç”Ÿæˆçš„å¤‡é€‰é¡¹é›†çš„é¢‘ç¹ç¨‹åº¦ï¼ˆä¸æœ€å°æ”¯æŒåº¦æ¯”è¾ƒï¼‰ï¼Œå¦‚æœä¸å¤Ÿé¢‘ç¹åˆ™èˆå¼ƒã€‚å¦‚æœæ²¡æœ‰æ–°çš„é¢‘ç¹é¡¹é›†ï¼Œå°±è·³åˆ°æœ€åä¸€æ­¥ã€‚ å­˜å‚¨æ–°å‘ç°çš„é¢‘ç¹é¡¹é›†ï¼Œè·³åˆ°æ­¥éª¤ 2 è¿”å›æ‰€æœ‰çš„é¢‘ç¹é¡¹é›†  Input:\n# å­—å…¸ä¿å­˜æœ€æ–°å‘ç°çš„é¢‘ç¹é¡¹é›† frequent_itemsets = {} min_support = 50 # ç¬¬ä¸€æ­¥ï¼Œæ¯ä¸€æ­¥ç”µå½±ç”ŸæˆåªåŒ…å«å®ƒè‡ªå·±çš„é¡¹é›† # frozenset() è¿”å›ä¸€ä¸ªå†»ç»“çš„é›†åˆï¼Œå†»ç»“åé›†åˆä¸èƒ½å†æ·»åŠ æˆ–åˆ é™¤ä»»ä½•å…ƒç´  # æ™®é€šé›†åˆå¯å˜ï¼Œé›†åˆä¸­ä¸èƒ½æœ‰å¯å˜çš„å…ƒç´ ï¼Œå› æ­¤æ™®é€šé›†åˆä¸èƒ½è¢«æ”¾åœ¨é›†åˆä¸­ï¼›å†»ç»“é›†åˆä¸å¯å˜ï¼Œå› æ­¤å¯ä»¥è¢«æ”¾å…¥é›†åˆ frequent_itemsets[1] = dict((frozenset((movie_id,)), row[\u0026#34;Favorable\u0026#34;]) for movie_id, row in num_favorable_by_movie.iterrows() if row[\u0026#34;Favorable\u0026#34;] \u0026gt; min_support) # ä¼šæœ‰é‡å¤ï¼Œå¯¼è‡´å–œæ¬¢ç”µå½±1,50çš„äººåˆ†åˆ«ä¸º50,100ä½†æ˜¯ {1,50} çš„é›†åˆæœ‰100ä¸ª # ä¸¤ä¸ªåŸå› ï¼Œç¬¬ä¸€åœ¨current_supersetæ—¶é¡¹é›†æœ‰æ—¶å€™ä¼šçªç„¶è°ƒæ¢ä½ç½® def find_frequent_itemsets( favorable_reviews_by_users, k_1_itemsets, min_support): counts = defaultdict(int) # éå†æ¯ä¸€ä¸ªç”¨æˆ·ï¼Œè·å–å…¶å–œæ¬¢çš„ç”µå½± for user, reviews in favorable_reviews_by_users.items(): # éå†æ¯ä¸ªé¡¹é›† for itemset in k_1_itemsets: if itemset.issubset(reviews): # åˆ¤æ–­itemsetæ˜¯å¦æ˜¯ç”¨æˆ·å–œæ¬¢çš„ç”µå½±çš„å­é›† # å¯¹ç”¨æˆ·å–œæ¬¢çš„ç”µå½±ä¸­é™¤äº†è¿™ä¸ªå­é›†çš„ç”µå½±è¿›è¡Œéå† for other_reviewed_movie in reviews - itemset: # å°†è¯¥ç”µå½±å¹¶å…¥é¡¹é›†ä¸­ current_superset = itemset | frozenset( {other_reviewed_movie}) counts[current_superset] += 1 # è¿™ä¸ªé¡¹é›†çš„æ”¯æŒåº¦+1 # è¿”å›å…ƒç´ æ•°ç›®+1çš„é¡¹é›†å’Œæ•°é‡ res = dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency \u0026gt;= min_support]) return res for k in range(2, 20): cur_frequent_itemsets = find_frequent_itemsets( favorable_reviews_by_users, frequent_itemsets[k - 1], min_support) frequent_itemsets[k] = cur_frequent_itemsets if len(cur_frequent_itemsets) == 0: print(\u0026#34;Did not find any frequent itemsets of length {}\u0026#34;.format(k)) sys.stdout.flush() # å°†ç¼“å†²åŒºå†…å®¹è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œä¸å®œå¤šç”¨ï¼Œè¾“å‡ºæ“ä½œå¸¦æ¥çš„è®¡ç®—å¼€é”€ä¼šæ‹–æ…¢ç¨‹åºè¿è¡Œé€Ÿåº¦ break else: print( \u0026#34;I found {}frequent itemsets of length {}\u0026#34;.format( len(cur_frequent_itemsets), k)) sys.stdout.flush() # é™¤å»åªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„åˆå§‹é›†åˆ del frequent_itemsets[1] Output:\nI found 93 frequent itemsets of length 2 I found 295 frequent itemsets of length 3 I found 593 frequent itemsets of length 4 I found 785 frequent itemsets of length 5 I found 677 frequent itemsets of length 6 I found 373 frequent itemsets of length 7 I found 126 frequent itemsets of length 8 I found 24 frequent itemsets of length 9 I found 2 frequent itemsets of length 10 Did not find any frequent itemsets of length 11  æŠ½å–å…³è”è§„åˆ™ å¯¹æ¯ä¸ªé¢‘ç¹é¡¹é›†ï¼Œé€‰å‡ºå…¶ä¸­çš„ä¸€ä¸ªå…ƒç´ å½“ç»“è®ºï¼Œå‰©ä¸‹çš„å…ƒç´ éƒ½ä½œä¸ºæ¡ä»¶ï¼Œç”Ÿæˆè§„åˆ™ã€‚\nInput:\n# è§„åˆ™å½¢å¼ï¼šå¦‚æœç”¨æˆ·å–œæ¬¢å‰æä¸­çš„æ‰€æœ‰ç”µå½±ï¼Œé‚£ä¹ˆä»–ä»¬ä¹Ÿä¼šå–œæ¬¢ç»“è®ºä¸­çš„ç”µå½± candidate_rules = [] for itemset_length, itemset_counts in frequent_itemsets.items(): for itemset in itemset_counts.keys(): for conclusion in itemset: premise = itemset - {conclusion} candidate_rules.append((premise, conclusion)) print(candidate_rules[:5]) Output:\n[(frozenset({7}), 1), (frozenset({1}), 7), (frozenset({50}), 1), (frozenset({1}), 50), (frozenset({1}), 56)]   ç½®ä¿¡åº¦è®¡ç®—ï¼Œæ–¹æ³•ä¸ç¬¬ä¸€ç« ç±»ä¼¼ã€‚\n# è®¡ç®—ç½®ä¿¡åº¦ correct_counts = defaultdict(int) incorrect_counts = defaultdict(int) # éå†æ¯ä¸€ä¸ªç”¨æˆ·ï¼Œè·å–å…¶å–œæ¬¢çš„ç”µå½± for user, reviews in favorable_reviews_by_users.items(): # éå†æ¯ä¸ªè§„åˆ™ for candidate_rule in candidate_rules: # è·å–è§„åˆ™çš„æ¡ä»¶å’Œç»“è®º premise, conclusion = candidate_rule # å¦‚æœæ¡ä»¶æ˜¯å–œæ¬¢ç”µå½±çš„å­é›†ï¼ˆæ¡ä»¶æˆç«‹ï¼‰ if premise.issubset(reviews): # å¦‚æœç”¨æˆ·ä¹Ÿå–œæ¬¢ç»“è®ºçš„ç”µå½± if conclusion in reviews: correct_counts[candidate_rule] += 1 else: incorrect_counts[candidate_rule] += 1 # è®¡ç®—ç½®ä¿¡åº¦ï¼Œç»“è®ºå‘ç”Ÿçš„æ¬¡æ•°é™¤ä»¥æ¡ä»¶å‘ç”Ÿçš„æ¬¡æ•° rule_confidence = { candidate_rule: correct_counts[candidate_rule] / float( correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in candidate_rules} # ç»™ç½®ä¿¡åº¦æ’åº sorted_confidence = sorted( rule_confidence.items(), key=itemgetter(1), reverse=True) for index in range(5): print(\u0026#34;Rule #{}\u0026#34;.format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] print( \u0026#34;Rule: If a person recommends {}they will also recommand {}\u0026#34;.format( premise, conclusion)) print( \u0026#34;- Confidence: {0:.3f}\u0026#34;.format(rule_confidence[(premise, conclusion)])) print(\u0026#34;--------------------\u0026#34;) Output:\nRule #1 Rule: If a person recommends frozenset({98, 181}) they will also recommand 50 - Confidence: 1.000 -------------------- Rule #2 Rule: If a person recommends frozenset({172, 79}) they will also recommand 174 - Confidence: 1.000 -------------------- Rule #3 Rule: If a person recommends frozenset({258, 172}) they will also recommand 174 - Confidence: 1.000 -------------------- Rule #4 Rule: If a person recommends frozenset({1, 181, 7}) they will also recommand 50 - Confidence: 1.000 -------------------- Rule #5 Rule: If a person recommends frozenset({1, 172, 7}) they will also recommand 174 - Confidence: 1.000 --------------------   è°ƒæ•´è¾“å‡ºï¼ŒåŠ ä¸Šç”µå½±å\nInput:\nmovie_name_data = pd.read_csv( \u0026#34;ml-100k/u.item\u0026#34;, delimiter=\u0026#39;|\u0026#39;, header=None, encoding=\u0026#34;mac-roman\u0026#34;) movie_name_data.columns = [ \u0026#39;MovieID\u0026#39;, \u0026#39;Title\u0026#39;, \u0026#39;Release Date\u0026#39;, \u0026#39;Video Release\u0026#39;, \u0026#39;IMDB\u0026#39;, \u0026#39;\u0026lt;UNK\u0026gt;\u0026#39;, \u0026#39;Action\u0026#39;, \u0026#39;Adventure\u0026#39;, \u0026#39;Animation\u0026#39;, \u0026#34;Children\u0026#39;s\u0026#34;, \u0026#39;Comedy\u0026#39;, \u0026#39;Crime\u0026#39;, \u0026#39;Documentary\u0026#39;, \u0026#39;Drama\u0026#39;, \u0026#39;Fantasy\u0026#39;, \u0026#39;Film-Noir\u0026#39;, \u0026#39;Horror\u0026#39;, \u0026#39;Musical\u0026#39;, \u0026#39;Mystery\u0026#39;, \u0026#39;Romance\u0026#39;, \u0026#39;Sci-Fi\u0026#39;, \u0026#39;Thriller\u0026#39;, \u0026#39;War\u0026#39;, \u0026#39;Western\u0026#39;] for index in range(5): print(\u0026#39;Rule #{0}\u0026#39;.format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] premise_names = \u0026#39;, \u0026#39;.join(get_movie_name(idx) for idx in premise) conclusion_name = get_movie_name(conclusion) print( \u0026#39;Rule: if a person recommends {0}they will also recommend {1}\u0026#39;.format( premise_names, conclusion_name)) print( \u0026#39; - Confidence: {0:.3f}\u0026#39;.format(rule_confidence[(premise, conclusion)])) print(\u0026#34;--------------------\u0026#34;) Output:\nRule #1 Rule: if a person recommends Silence of the Lambs, The (1991), Return of the Jedi (1983) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #2 Rule: if a person recommends Empire Strikes Back, The (1980), Fugitive, The (1993) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #3 Rule: if a person recommends Contact (1997), Empire Strikes Back, The (1980) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #4 Rule: if a person recommends Toy Story (1995), Return of the Jedi (1983), Twelve Monkeys (1995) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #5 Rule: if a person recommends Toy Story (1995), Empire Strikes Back, The (1980), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 --------------------  è¯„ä¼°æµ‹è¯• ä½¿ç”¨å‰©ä¸‹çš„æ•°æ®é›†è®¡ç®—è§„åˆ™çš„ç½®ä¿¡åº¦ï¼Œä¹Ÿæ˜¯æŸ¥çœ‹æ¯æ¡è§„åˆ™è¡¨ç°çš„ä¸€ä¸ªæ–¹æ³•ã€‚\nInput:\n# è¯„ä¼°æµ‹è¯• test_dataset = all_ratings[~all_ratings[\u0026#39;UserID\u0026#39;].isin(range(200))] test_favorable = test_dataset[test_dataset[\u0026#34;Favorable\u0026#34;]] test_favorable_by_users = dict((k, frozenset(v.values)) for k, v in test_favorable.groupby(\u0026#34;UserID\u0026#34;)[\u0026#34;MovieID\u0026#34;]) correct_counts = defaultdict(int) incorrect_counts = defaultdict(int) for user, reviews in test_favorable_by_users.items(): for candidate_rule in candidate_rules: premise, conclusion = candidate_rule if premise.issubset(reviews): if conclusion in reviews: correct_counts[candidate_rule] += 1 else: incorrect_counts[candidate_rule] += 1 test_confidence = { candidate_rule: correct_counts[candidate_rule] / float( correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in rule_confidence} for index in range(5): print(\u0026#34;Rule #{0}\u0026#34;.format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] premise_names = \u0026#34;, \u0026#34;.join(get_movie_name(idx) for idx in premise) conclusion_name = get_movie_name(conclusion) print( \u0026#39;Rule: if a person recommends {0}they will also recommend {1}\u0026#39;.format( premise_names, conclusion_name)) print( \u0026#39; - Confidence: {0:.3f}\u0026#39;.format(rule_confidence[(premise, conclusion)])) print(\u0026#34;--------------------\u0026#34;) Output:\nRule #1 Rule: if a person recommends Silence of the Lambs, The (1991), Return of the Jedi (1983) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #2 Rule: if a person recommends Empire Strikes Back, The (1980), Fugitive, The (1993) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #3 Rule: if a person recommends Contact (1997), Empire Strikes Back, The (1980) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 -------------------- Rule #4 Rule: if a person recommends Toy Story (1995), Return of the Jedi (1983), Twelve Monkeys (1995) they will also recommend Star Wars (1977) - Confidence: 1.000 -------------------- Rule #5 Rule: if a person recommends Toy Story (1995), Empire Strikes Back, The (1980), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981) - Confidence: 1.000 --------------------  è¿™ä¸€ç« ç”¨ç”µå½±è¿›è¡Œäº²å’Œåº¦åˆ†æï¼Œç”±äºå…ƒç´ çš„æ•°é‡å˜å¤šäº†ï¼Œæ—¶é—´å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œéå†çš„ç¬¨æ–¹æ³•å·²ç»ä¸é€‚ç”¨ã€‚éœ€è¦å¯»æ‰¾æ›´åŠ å·§å¦™åœ°è§£å†³æ–¹æ¡ˆã€‚\nåœ¨ç”¨é›†åˆè®¡ç®—ç”µå½±çš„é¡¹é›†æ—¶ï¼Œ{1, 2} ä¸ {2, 1} æ˜¯åŒä¸€ä¸ªäº‹ä»¶ï¼Œä½†åœ¨éå†çš„æ—¶å€™ä¼šè¢«å¤šæ¬¡è®¡ç®—ï¼Œå¯èƒ½è¿™æ˜¯ä¸€ä¸ªé”™è¯¯çš„ç‚¹ã€‚\nç¬¬äº”ç«  æœ¬ç« è®¨è®ºå¦‚ä½•ä»æ•°æ®é›†ä¸­æŠ½å–æ•°å€¼å’Œç±»åˆ«å‹ç‰¹å¾ï¼Œå¹¶é€‰å‡ºæœ€ä½³ç‰¹å¾ã€‚è¿˜ä¼šä»‹ç»ç‰¹å¾æŠ½å–çš„å¸¸ç”¨æ¨¡å¼å’ŒæŠ€å·§ã€‚\nç‰¹å¾æŠ½å– æŠŠå®ä½“ç”¨ç‰¹å¾è¡¨ç¤ºå‡ºæ¥ï¼Œé€šè¿‡ç‰¹å¾å»ºæ¨¡ï¼Œå†é€šè¿‡æœºå™¨æŒ–æ˜ç®—æ³•èƒ½å¤Ÿç†è§£çš„è¿‘ä¼¼æ–¹å¼æ¥è¡¨ç¤ºç°å®ã€‚\nç‰¹å¾å¯ä»¥æ˜¯æ•°å€¼å‹æˆ–ç±»åˆ«å‹ã€‚æ•°å€¼ç‰¹å¾å¯ä»¥ç¦»æ•£åŒ–ç”Ÿæˆç±»åˆ«ç‰¹å¾ã€‚\nInput:\nimport numpy as np import pandas as pd if __name__ == \u0026#39;__main__\u0026#39;: adult = pd.read_csv(\u0026#34;adult.data\u0026#34;, header=None, names=[\u0026#34;Age\u0026#34;, \u0026#34;Work-Class\u0026#34;, \u0026#34;fnlwgt\u0026#34;, \u0026#34;Education\u0026#34;, \u0026#34;Education-Num\u0026#34;, \u0026#34;Marital-Status\u0026#34;, \u0026#34;Occupation\u0026#34;, \u0026#34;Relationship\u0026#34;, \u0026#34;Race\u0026#34;, \u0026#34;Sex\u0026#34;, \u0026#34;Capital-gain\u0026#34;, \u0026#34;Capital-loss\u0026#34;, \u0026#34;Hours-per-week\u0026#34;, \u0026#34;Native-Country\u0026#34;, \u0026#34;Earnings-Raw\u0026#34;]) # å»é™¤ç©ºå€¼ adult.dropna(how=\u0026#39;all\u0026#39;, inplace=True) # è¾“å‡ºè¯¦ç»†æè¿° print(adult[\u0026#34;Hours-per-week\u0026#34;].describe()) # è¾“å‡ºä¸­ä½æ•° print(adult[\u0026#34;Education-Num\u0026#34;].median()) # è¾“å‡ºå·¥ä½œçš„ç§ç±» print(adult[\u0026#34;Work-Class\u0026#34;].unique()) # å°†å·¥ä½œæ—¶é•¿äºŒå€¼åŒ–ä¸ºæ˜¯å¦è¶…è¿‡40h adult[\u0026#34;LongHours\u0026#34;] = adult[\u0026#34;Hours-per-week\u0026#34;] \u0026gt; 40 Output:\ncount 32561.000000 mean 40.437456 std 12.347429 min 1.000000 25% 40.000000 50% 40.000000 75% 45.000000 max 99.000000 Name: Hours-per-week, dtype: float64 10.0 [' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov' ' ?' ' Self-emp-inc' ' Without-pay' ' Never-worked']  ç‰¹å¾é€‰æ‹© å®ç‰©çš„ç‰¹å¾æœ‰å¾ˆå¤šï¼Œæˆ‘ä»¬åªé€‰æ‹©å…¶ä¸­ä¸€å°éƒ¨åˆ†ã€‚\n é™ä½å¤æ‚åº¦ï¼Œæé«˜ç®—æ³•è¿è¡Œé€Ÿåº¦ å‡ä½å™ªéŸ³ï¼Œå¢åŠ æ— å…³çš„ç‰¹å¾ä¼šå¹²æ‰°ç®—æ³•çš„å·¥ä½œ å¢åŠ æ¨¡å‹å¯è¯»æ€§ï¼Œç‰¹å¾è¾ƒå°‘ï¼Œäººä»¬æ˜“äºç†è§£  æ‹¿åˆ°æ•°æ®åï¼Œå…ˆè¿›è¡Œç®€å•ç›´æ¥çš„åˆ†æï¼Œäº†è§£æ•°æ®çš„ç‰¹ç‚¹ã€‚\nsklearn.feature_selection.VarianceThreshold è½¬æ¢å™¨å¯ä»¥ç”¨æ¥åˆ é™¤ç‰¹å¾å€¼çš„æ–¹å·®è¾¾ä¸åˆ°æœ€ä½æ ‡å‡†çš„ç‰¹å¾ã€‚\nInput:\n# æ„é€ æµ‹è¯•æ•°æ®é›† X = np.arange(30).reshape((10, 3)) X[:, 1] = 1 print(X) print(\u0026#34;----------------\u0026#34;) vt = VarianceThreshold() Xt = vt.fit_transform(X) # ç¬¬äºŒåˆ—æ¶ˆå¤±äº†ï¼Œå› ä¸ºç¬¬äºŒåˆ—éƒ½æ˜¯1ï¼Œæ–¹å·®ä¸º0ï¼Œä¸åŒ…æ‹¬å…·æœ‰åŒºåˆ«æ„ä¹‰çš„ä¿¡æ¯ print(Xt) print(\u0026#34;----------------\u0026#34;) print(vt.variances_) Output:\n[[ 0 1 2] [ 3 1 5] [ 6 1 8] [ 9 1 11] [12 1 14] [15 1 17] [18 1 20] [21 1 23] [24 1 26] [27 1 29]] ---------------- [[ 0 2] [ 3 5] [ 6 8] [ 9 11] [12 14] [15 17] [18 20] [21 23] [24 26] [27 29]] ---------------- [27. 0. 27.]   é€‰æ‹©æœ€ä½³ç‰¹å¾\néšç€ç‰¹å¾æ•°é‡çš„å¢åŠ ï¼Œå¯»æ‰¾æœ€ä½³ç‰¹å¾ç»„åˆçš„ä»»åŠ¡å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚åˆ†ç±»ä»»åŠ¡é€šå¸¸çš„åšæ³•æ˜¯å¯»æ‰¾è¡¨ç°å¥½çš„å•ä¸ªç‰¹å¾ï¼Œä¾æ®æ˜¯ä»–ä»¬èƒ½è¾¾åˆ°çš„ç²¾ç¡®åº¦ã€‚\nscikit-learn æä¾›äº†å‡ ä¸ªç”¨äºé€‰æ‹©å•å˜é‡ç‰¹å¾çš„è½¬æ¢å™¨ã€‚\n SelectKBest è¿”å› k ä¸ªæœ€ä½³ç‰¹å¾ SelectPercentile è¿”å›è¡¨ç°æœ€ä½³çš„ r%ä¸ªç‰¹å¾  è¿™ä¸¤ä¸ªè½¬æ¢å™¨éƒ½æä¾›è®¡ç®—ç‰¹å¾è¡¨ç°çš„ä¸€ç³»åˆ—æ–¹æ³•ã€‚\nå•ä¸ªç‰¹å¾å’ŒæŸä¸€ç±»åˆ«ä¹‹é—´çš„ç›¸å…³æ€§è®¡ç®—æ–¹æ³•æœ‰å¡æ–¹æ£€éªŒ(xÂ²)ã€äº’ä¿¡æ¯å’Œä¿¡æ¯ç†µç­‰ã€‚\nInput:\n# æ„é€ æ•°æ®é›† X = adult[[\u0026#34;Age\u0026#34;, \u0026#34;Education-Num\u0026#34;, \u0026#34;Capital-gain\u0026#34;, \u0026#34;Capital-loss\u0026#34;, \u0026#34;Hours-per-week\u0026#34;]] y = (adult[\u0026#34;Earnings-Raw\u0026#34;] == \u0026#39; \u0026gt;50K\u0026#39;).values # ä½¿ç”¨SelectKBestè½¬æ¢å™¨ï¼Œç”¨å¡æ–¹æ‰“åˆ† transformer = SelectKBest(score_func=chi2, k=3) # è°ƒç”¨fit_transformæ–¹æ³•å¯¹ç›¸åŒçš„æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†å’Œè½¬æ¢ Xt_chi2 = transformer.fit_transform(X, y) # è¾“å‡ºæ¯ä¸ªç‰¹å¾çš„å¾—åˆ† print(transformer.scores_) print(\u0026#34;----------------\u0026#34;) # ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°è®¡ç®—ç›¸å…³æ€§,åˆ›å»ºåŒ…è£…å‡½æ•° def mutivariate_pearsonr(X, y): scores, pvalues = [], [] for column in range(X.shape[1]): cur_score, cur_p = pearsonr(X[:, column], y) scores.append(abs(cur_score)) pvalues.append(cur_p) return np.array(scores), np.array(pvalues) transformer = SelectKBest(score_func=mutivariate_pearsonr, k=3) Xt_pearson = transformer.fit_transform(X, y) print(transformer.scores_) print(\u0026#34;----------------\u0026#34;) clf = DecisionTreeClassifier(random_state=14) scores_chi2 = cross_val_score(clf, Xt_chi2, y, scoring=\u0026#39;accuracy\u0026#39;) scores_pearson = cross_val_score(clf, Xt_pearson, y, scoring=\u0026#39;accuracy\u0026#39;) print(\u0026#39;å¡æ–¹: {}\u0026#39;.format(np.mean(scores_chi2))) print(\u0026#34;----------------\u0026#34;) print(\u0026#34;pearson: {}\u0026#34;.format(np.mean(scores_pearson))) Output:\n[8.60061182e+03 2.40142178e+03 8.21924671e+07 1.37214589e+06 6.47640900e+03] ---------------- [0.2340371 0.33515395 0.22332882 0.15052631 0.22968907] ---------------- å¡æ–¹: 0.8291514400795839 ---------------- pearson: 0.7721507467016449  åˆ›å»ºç‰¹å¾ ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§å¾ˆå¼ºï¼Œæˆ–è€…ç‰¹å¾å†—ä½™ï¼Œä¼šå¢åŠ ç®—æ³•å¤„ç†éš¾åº¦ã€‚\nè¿™é‡Œåœ¨åŠ è½½ ad æ•°æ®é›†ä¹‹å‰å…ˆåˆ›å»ºäº†ä¸€ä¸ªè½¬æ¢å™¨ï¼Œç”¨äºåœ¨åŠ è½½æ—¶è½¬æ¢æ•°æ®é›†ä¸­çš„å€¼ã€‚\næºç è¿è¡Œä¼šäº§ç”ŸæŠ¥é”™ï¼Œç¬¬ä¸€ä¸ªåŸå› æ˜¯ï¼Œç”¨å‡½æ•°åˆå§‹åŒ–è½¬æ¢å™¨å¹¶æ²¡æœ‰æŠŠå‡½æ•°åä¼ å…¥ï¼Œå› æ­¤å°† defaultdict ä¸­æ¯ä¸€ä¸ªç´¢å¼•éƒ½è¿›è¡Œäº†åˆå§‹åŒ–ã€‚ç¬¬äºŒä¸ªåŸå› æ˜¯ï¼ŒPCA è½¬æ¢å™¨æ— æ³•å¯¹ NaN æ•°æ®è¿›è¡Œå¤„ç†ï¼Œäºæ˜¯æˆ‘åœ¨å¤„ç”Ÿæˆæ•°æ®é›†ä¹‹å‰å°†æ‰€æœ‰å«æœ‰ NaN çš„è¡Œåˆ æ‰ã€‚\nInput:\n# -*- coding: utf-8 -*- import numpy as np import pandas as pd from collections import defaultdict from sklearn.decomposition import PCA from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score from matplotlib import pyplot as plt # åˆ›å»ºè½¬æ¢å‡½æ•° def convert_number(x): try: res = float(x) return res except ValueError: return np.nan if __name__ == \u0026#39;__main__\u0026#39;: # åˆ›å»ºæ•°æ®åŠ è½½çš„è½¬æ¢å™¨ converters = defaultdict(convert_number, {i: convert_number for i in range(1588)}) converters[1558] = lambda x: 1 if x.strip() == \u0026#34;ad.\u0026#34; else 0 # ä½¿ç”¨è½¬æ¢å™¨è¯»å–æ•°æ®é›† temp = pd.read_csv(\u0026#34;ad.data\u0026#34;, header=None, converters=converters) # åˆ é™¤æ‰€æœ‰å«æœ‰nançš„è¡Œ,axis=0æ˜¯æ•°æ®ç´¢å¼•(index)ï¼Œaxis=1æ˜¯åˆ—æ ‡ç­¾(column) ads = temp.dropna(axis=0, how=\u0026#39;any\u0026#39;) print(ads[10:15]) Output:\n 0 1 2 3 4 5 ... 1553 1554 1555 1556 1557 1558 11 90.0 52.0 0.5777 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 12 90.0 60.0 0.6666 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 13 90.0 60.0 0.6666 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 14 33.0 230.0 6.9696 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1 15 60.0 468.0 7.8000 1.0 0.0 0.0 ... 0.0 1.0 1.0 0.0 0.0 1 [5 rows x 1559 columns]   ä¸»æˆåˆ†åˆ†æ(PCA)\nç›®çš„æ˜¯æ‰¾åˆ°èƒ½ç”¨è¾ƒå°‘ä¿¡æ¯æè¿°æ•°æ®é›†çš„ç‰¹å¾ç»„åˆã€‚ä¸»æˆåˆ†çš„æ–¹å·®è·Ÿæ•´ä½“æ–¹å·®æ²¡æœ‰å¤šå¤§å·®è·ã€‚ç»è¿‡åˆ†æä¸»æˆåˆ†ï¼Œç¬¬ä¸€ä¸ªç‰¹å¾çš„æ–¹å·®å¯¹æ•°æ®é›†æ–¹å·®çš„è´¡çŒ®ç‡ä¸º 85.4%ï¼Œç¬¬äºŒä¸ªä¸º 14.5%ï¼Œåé¢è¶Šæ¥è¶Šå°‘ã€‚\nInput:\nX = ads.drop(1558, axis=1).values y = ads[1558] # å‚æ•°ä¸ºä¸»æˆåˆ†æ•°é‡ pca = PCA(n_components=5) Xd = pca.fit_transform(X) # è®¾ç½®è¾“å‡ºé€‰é¡¹ # ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºè¾“å‡ºç²¾åº¦ä½æ•°ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä½¿ç”¨å®šç‚¹è¡¨ç¤ºæ³•æ‰“å°æµ®ç‚¹æ•° np.set_printoptions(precision=3, suppress=True) print(pca.explained_variance_ratio_) Output:\n[0.854 0.145 0.001 0. 0. ]   ä½¿ç”¨éšæœºæ£®æ—éªŒè¯æ¨¡å‹æ­£ç¡®ç‡ï¼Œå¹¶å°† pca è½¬æ¢ç»“æœç»˜åˆ¶å‡ºæ¥ã€‚\nInput:\nclf = DecisionTreeClassifier(random_state=14) scores_reduced = cross_val_score(clf, Xd, y, scoring=\u0026#39;accuracy\u0026#39;) print(np.mean(scores_reduced)) # è·å–æ•°æ®é›†ç±»åˆ«çš„æ‰€æœ‰å–å€¼ classes = set(y) # æŒ‡å®šåœ¨å›¾å½¢ä¸­ç”¨ä»€ä¹ˆé¢œè‰²è¡¨ç¤ºè¿™ä¸¤ä¸ªç±»åˆ« colors = [\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;] # åŒæ—¶éå†è¿™ä¸¤ä¸ªå®¹å™¨ for cur_class, color in zip(classes, colors): # ä¸ºå±äºå½“å‰ç±»åˆ«çš„æ‰€æœ‰ä¸ªä½“åˆ›å»ºé®ç½©å±‚ mask = (y == cur_class).values plt.scatter(Xd[mask, 0], Xd[mask, 1], marker=\u0026#39;o\u0026#39;, color=color, label=int(cur_class)) plt.legend() plt.show() Output:\n0.936405592140775   pca \nåˆ›å»ºè‡ªå·±çš„è½¬æ¢å™¨ è½¬æ¢å™¨æœ‰ä¸¤ä¸ªå…³é”®å‡½æ•°\n fit() æ¥æ”¶è®­ç»ƒæ•°æ®ï¼Œè®¾ç½®å†…éƒ¨å‚æ•° transform() è½¬æ¢è¿‡ç¨‹ã€‚æ¥æ”¶è®­ç»ƒæ•°æ®é›†æˆ–ç›¸åŒæ ¼å¼çš„æ–°æ•°æ®é›†  æ¥å£è¦ä¸ scikit-learn æ¥å£ä¸€è‡´ï¼Œä¾¿äºåœ¨æµæ°´çº¿ä¸­ä½¿ç”¨ã€‚\nInput:\n# -*- coding: utf-8 -*- import numpy as np from sklearn.base import TransformerMixin from sklearn.utils import as_float_array from numpy.testing import assert_array_equal class MeanDiscrete(TransformerMixin): def fit(self, X): # å°è¯•å¯¹Xè¿›è¡Œè½¬æ¢ï¼Œæ•°æ®è½¬æ¢æˆfloatç±»å‹ X = as_float_array(X) # è®¡ç®—æ•°æ®é›†çš„å‡å€¼ self.mean = X.mean(axis=0) # è¿”å›å®ƒæœ¬èº«ï¼Œè¿›è¡Œé“¾å¼è°ƒç”¨transformer.fit(X).transform(X) return self def transform(self, X): X = as_float_array(X) # æ£€æŸ¥è¾“å…¥æ˜¯å¦åˆæ³• assert X.shape[1] == self.mean.shape[0] # è¿”å›Xä¸­å¤§äºå‡å€¼çš„æ•°æ® return X \u0026gt; self.mean def test_meandiscrete(): X_test = np.array([[0, 2], [3, 5], [6, 8], [9, 11], [12, 14], [15, 17], [18, 20], [21, 23], [24, 26], [27, 29]]) mean_discrete = MeanDiscrete() mean_discrete.fit(X_test) # ä¸æ­£ç¡®çš„è®¡ç®—ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œæ£€æŸ¥å†…éƒ¨å‚æ•°æ˜¯å¦æ­£ç¡®è®¾ç½® assert_array_equal(mean_discrete.mean, np.array([13.5, 15.5])) # è½¬æ¢åçš„X X_transfromed = mean_discrete.transform(X_test) # éªŒè¯æ•°æ® X_expected = np.array([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]) assert_array_equal(X_transfromed, X_expected) if __name__ == \u0026#39;__main__\u0026#39;: test_meandiscrete() Output:\n# æ²¡æœ‰è¾“å‡ºï¼Œè¯´æ˜æµ‹è¯•é€šè¿‡  ç¬¬å…­ç«  æœ¬ç« ä»‹ç»å¦‚ä½•ä»æ–‡æœ¬æ•°æ®ä¸­æå–ç‰¹å¾ã€‚é€šè¿‡å¼ºå¤§å´ç®€å•çš„æœ´ç´ è´å¶æ–¯ç®—æ³•æ¶ˆé™¤ç¤¾ä¼šåª’ä½“ç”¨è¯­çš„æ­§ä¹‰ã€‚\næœ´ç´ è´å¶æ–¯ç®—æ³•åœ¨è®¡ç®—ç”¨äºåˆ†ç±»çš„æ¦‚ç‡æ—¶ï¼Œä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œå‡å®šå„ç‰¹å¾é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå› æ­¤åå­—ä¸­å«æœ‰æœ´ç´ äºŒå­—ã€‚\næ¶ˆæ­§ ç”±äºæ— æ³•ç”³è¯·åˆ° Twitter app æš‚æ—¶æç½®ã€‚ã€‚ã€‚%\u0026gt;_\u0026lt;%\næ–‡æœ¬è½¬æ¢å™¨ è¯è¢‹ï¼šä¸€ç§æœ€ç®€å•å´éå¸¸æœ‰æ•ˆçš„æ¨¡å‹å°±æ˜¯åªç»Ÿè®¡æ•°æ®é›†ä¸­æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°ã€‚æ¨¡å‹ä¸»è¦åˆ†ä¸ºä»¥ä¸‹ä¸‰ç§\n ä½¿ç”¨è¯è¯­å®é™…å‡ºç°çš„æ¬¡æ•°ä½œä¸ºè¯é¢‘ã€‚ç¼ºç‚¹æ˜¯å½“æ–‡ç« é•¿åº¦æ˜æ˜¾å·®å¼‚æ—¶ï¼Œè¯é¢‘å·®è·ä¼šéå¸¸å¤§ã€‚ ä½¿ç”¨å½’ä¸€åŒ–åçš„è¯é¢‘ï¼Œæ¯ç¯‡æ–‡ç« ä¸­æ‰€æœ‰è¯è¯­çš„è¯é¢‘ä¹‹å’Œä¸º 1 ç›´æ¥ä½¿ç”¨äºŒå€¼ç‰¹å¾æ¥è¡¨ç¤ºï¼Œå•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°å€¼ä¸º 1ï¼Œä¸å‡ºç°å€¼ä¸º 0  è¿˜æœ‰ä¸€ç§æ›´é€šç”¨çš„è§„èŒƒåŒ–æ–¹æ³•å«åšè¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡æ³•ï¼Œè¯¥åŠ æƒæ–¹æ³•ç”¨è¯é¢‘æ¥ä»£æ›¿è¯çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åå†ç”¨è¯é¢‘é™¤ä»¥åŒ…å«è¯¥è¯çš„æ–‡æ¡£çš„æ•°é‡ã€‚\nInput:\n# -*- coding: utf-8 -*- from collections import Counter if __name__ == \u0026#39;__main__\u0026#39;: s = \u0026#34;\u0026#34;\u0026#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in halls of stone, Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne In the Land of Mordor where the Shadows lie. One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them. In the Land of Mordor where the Shadows lie\u0026#34;\u0026#34;\u0026#34;.lower() words = s.split() c = Counter(words) # è¾“å‡ºå‡ºç°æ¬¡æ•°æœ€å¤šçš„å‰5ä¸ªè¯ print(c.most_common(5)) Output:\n[('the', 9), ('for', 4), ('in', 4), ('to', 4), ('one', 4)]   N å…ƒè¯­æ³•æ˜¯æŒ‡ç”±å‡ ä¸ªè¿ç»­çš„è¯ç»„æˆçš„å­åºåˆ—ã€‚\næœ´ç´ è´å¶æ–¯ æˆ‘ä»¬ç”¨ C è¡¨ç¤ºæŸç§ç±»åˆ«ï¼Œç”¨ D è¡¨ç¤ºæ•°æ®é›†ä¸­ä¸€ç¯‡æ–‡æ¡£ï¼Œæ¥è®¡ç®—è´å¶æ–¯å…¬å¼æ‰€è¦ç”¨åˆ°çš„å„ç§ç»Ÿè®¡é‡ï¼Œå¯¹äºä¸å¥½è®¡ç®—ï¼Œå‡ºæœ´ç´ å‡è®¾ï¼Œç®€åŒ–è®¡ç®—ã€‚æœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•ä½¿ç”¨è´å¶æ–¯å®šç†è®¡ç®—ä¸ªä½“ä»å±äºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ã€‚\nP(C) ä¸ºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ï¼Œå¯ä»¥ä»è®­ç»ƒé›†ä¸­è®¡ç®—å¾—åˆ°ï¼ˆæ–¹æ³•è·Ÿä¸Šæ–‡æ£€æµ‹åƒåœ¾é‚®ä»¶ä¾‹å­æ‰€ç”¨åˆ°çš„ä¸€è‡´ï¼‰ã€‚ç»Ÿè®¡è®­ç»ƒé›†æ‰€æœ‰æ–‡æ¡£ä»å±äºç»™å®šç±»åˆ«çš„ç™¾åˆ†æ¯”ã€‚\nP(D) ä¸ºæŸä¸€æ–‡æ¡£çš„æ¦‚ç‡ï¼Œå®ƒç‰µæ‰¯åˆ°å„ç§ç‰¹å¾ï¼Œè®¡ç®—èµ·æ¥å¾ˆå›°éš¾ï¼Œä½†æ˜¯åœ¨è®¡ç®—æ–‡æ¡£å±äºå“ªä¸ªç±»åˆ«æ—¶ï¼Œå¯¹äºæ‰€æœ‰ç±»åˆ«æ¥è¯´ï¼ŒP(D)ç›¸åŒï¼Œå› æ­¤æ ¹æœ¬å°±ä¸ç”¨è®¡ç®—å®ƒã€‚ç¨åæˆ‘ä»¬æ¥çœ‹ä¸‹æ€ä¹ˆå¤„ç†ã€‚\nP(D|C) ä¸ºæ–‡æ¡£ D å±äº C ç±»çš„æ¦‚ç‡ã€‚ç”±äº D åŒ…å«å¤šä¸ªç‰¹å¾ï¼Œè®¡ç®—èµ·æ¥å¯èƒ½å¾ˆå›°éš¾ï¼Œè¿™æ—¶æœ´ç´ è´å¶æ–¯ç®—æ³•å°±æ´¾ä¸Šç”¨åœºäº†ã€‚æˆ‘ä»¬æœ´ç´ åœ°å‡å®šå„ä¸ªç‰¹å¾ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œåˆ†åˆ«è®¡ç®—æ¯ä¸ªç‰¹å¾ï¼ˆD1ã€D2ã€D3 ç­‰ï¼‰åœ¨ç»™å®šç±»åˆ«å‡ºç°çš„æ¦‚ç‡ï¼Œå†æ±‚å®ƒä»¬çš„ç§¯ã€‚\nP(D|C) = P(D1|C) x P(D2|C) ... x P(Dn|C)\nä¸¾ä¾‹è¯´æ˜ä¸‹è®¡ç®—è¿‡ç¨‹ï¼Œå‡å¦‚æ•°æ®é›†ä¸­æœ‰ä»¥ä¸‹ä¸€æ¡ç”¨äºŒå€¼ç‰¹å¾è¡¨ç¤ºçš„æ•°æ®ï¼š[1, 0, 0, 1]\nè®­ç»ƒé›†ä¸­æœ‰ 75% çš„æ•°æ®å±äºç±»åˆ« 0ï¼Œ 25% å±äºç±»åˆ« 1ï¼Œä¸”æ¯ä¸ªç‰¹å¾å±äºæ¯ä¸ªç±»åˆ«çš„ä¼¼ç„¶åº¦å¦‚ä¸‹ã€‚\n ç±»åˆ« 0ï¼š[0.3, 0.4, 0.4, 0.7] ç±»åˆ« 1ï¼š[0.7, 0.3, 0.4, 0.9]  æ‹¿ç±»åˆ« 0 ä¸­ç‰¹å¾ 1 çš„ä¼¼ç„¶åº¦ä¸¾ä¾‹å­ï¼Œä¸Šé¢è¿™ä¸¤è¡Œæ•°æ®å¯ä»¥è¿™æ ·ç†è§£ï¼šç±»åˆ« 0 ä¸­æœ‰ 30%çš„æ•°æ®ï¼Œç‰¹å¾ 1 çš„å€¼ä¸º 1ã€‚\næˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹è¿™æ¡æ•°æ®å±äºç±»åˆ« 0 çš„æ¦‚ç‡ã€‚ç±»åˆ«ä¸º 0 æ—¶ï¼ŒP(C=0) = 0.75ã€‚\næœ´ç´ è´å¶æ–¯ç®—æ³•ç”¨ä¸åˆ° P(D)ï¼Œå› æ­¤æˆ‘ä»¬ä¸ç”¨è®¡ç®—å®ƒã€‚\nP(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0) = 0.3 x 0.6 x 0.6 x 0.7 = 0.0756\næˆ‘ä»¬å°±å¯ä»¥è®¡ç®—è¯¥æ¡æ•°æ®ä»å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚æˆ‘ä»¬æ²¡æœ‰è®¡ç®— P(D)ï¼Œå› æ­¤ï¼Œè®¡ç®—ç»“æœä¸æ˜¯å®é™…çš„æ¦‚ç‡ã€‚ç”±äºä¸¤æ¬¡éƒ½ä¸è®¡ç®— P(D)ï¼Œç»“æœå…·æœ‰å¯æ¯”è¾ƒæ€§ï¼Œèƒ½å¤ŸåŒºåˆ†å‡ºå¤§å°å°±è¶³å¤Ÿäº†ã€‚æ¥çœ‹ä¸‹è®¡ç®—ç»“æœã€‚\nP(C=0|D) = P(C=0) P(D|C=0) = 0.75 * 0.0756 = 0.0567\nP(D|C=1) = P(D1|C=1) x P(D2|C=1) x P(D3|C=1) x P(D4|C=1) = 0.7 x 0.7 x 0.6 x 0.9 = 0.2646\nP(C=1|D) = P(C=1)P(D|C=1) = 0.25 * 0.2646 = 0.06615\nå› æ­¤è¿™æ¡æ•°æ®å±äºç±»åˆ« 1 çš„æ¦‚ç‡å¤§äºå±äºç±»åˆ« 2 çš„æ¦‚ç‡\nåº”ç”¨ åˆ›å»ºæµæ°´çº¿ï¼Œæ¥æ”¶ä¸€æ¡æ¶ˆæ¯ï¼Œä»…æ ¹æ®æ¶ˆæ¯å†…å®¹ï¼Œç¡®å®šå®ƒä¸ç¼–ç¨‹è¯­è¨€ Python æ˜¯å¦ç›¸å…³ã€‚\n ç”¨ NLTK çš„ word_tokenize å‡½æ•°ï¼Œå°†åŸå§‹æ–‡æ¡£è½¬æ¢ä¸ºç”±å•è¯åŠå…¶æ˜¯å¦å‡ºç°ç»„æˆçš„å­—å…¸ã€‚ ç”¨ scikit-learn ä¸­çš„ DictVectorizer è½¬æ¢å™¨å°†å­—å…¸è½¬æ¢ä¸ºå‘é‡çŸ©é˜µï¼Œè¿™æ ·æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨å°±èƒ½ä½¿ç”¨ç¬¬ä¸€æ­¥ä¸­æŠ½å–çš„ç‰¹å¾ã€‚ æ­£å¦‚å‰å‡ ç« åšè¿‡çš„é‚£æ ·ï¼Œè®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ã€‚ è¿˜éœ€è¦æ–°å»ºä¸€ä¸ªç¬”è®°æœ¬æ–‡ä»¶ ch6_classify_twitterï¼ˆæœ¬ç« æœ€åä¸€ä¸ªï¼‰ï¼Œç”¨äºåˆ†ç±»ã€‚  F1 å€¼æ¥è¯„ä¼°ç®—æ³•\nF1 å€¼æ˜¯ä»¥æ¯ä¸ªç±»åˆ«ä¸ºåŸºç¡€è¿›è¡Œå®šä¹‰çš„ï¼ŒåŒ…æ‹¬ä¸¤å¤§æ¦‚å¿µï¼šå‡†ç¡®ç‡ï¼ˆprecisionï¼‰å’Œå¬å›ç‡ï¼ˆrecallï¼‰ã€‚å‡†ç¡®ç‡æ˜¯æŒ‡é¢„æµ‹ç»“æœå±äºæŸä¸€ç±»çš„ä¸ªä½“ï¼Œå®é™…å±äºè¯¥ç±»çš„æ¯”ä¾‹ã€‚å¬å›ç‡æ˜¯æŒ‡è¢«æ­£ç¡®é¢„æµ‹ä¸ºæŸä¸ªç±»åˆ«çš„ä¸ªä½“æ•°é‡ä¸æ•°æ®é›†ä¸­è¯¥ç±»åˆ«ä¸ªä½“æ€»é‡çš„æ¯”ä¾‹\nInput:\n# -*- coding: utf-8 -*- import json import numpy as np from sklearn.base import TransformerMixin from nltk import word_tokenize from sklearn.feature_extraction import DictVectorizer # æ¥å—å…ƒç´ ä¸ºå­—å…¸çš„åˆ—è¡¨ï¼Œå°†å…¶è½¬æ¢ä¸ºçŸ©é˜µ from sklearn.model_selection import cross_val_score from sklearn.naive_bayes import BernoulliNB # ç”¨äºäºŒå€¼ç‰¹å¾åˆ†ç±»çš„ BernoulliNB åˆ†ç±»å™¨ï¼Œ from sklearn.pipeline import Pipeline # åˆ›å»ºè½¬æ¢å™¨ç±» class NLTKBOW(TransformerMixin): def fit(self, X, y=None): return self def transform(self, X): return [{word: True for word in word_tokenize(document)} for document in X] if __name__ == \u0026#39;__main__\u0026#39;: tweets = [] input_filename = \u0026#34;\u0026#34; classes_filename = \u0026#34;\u0026#34; with open(input_filename) as inf: for line in inf: if len(line.strip()) == 0: continue tweets.append(json.loads(line)[\u0026#39;text\u0026#39;]) with open(classes_filename, \u0026#39;r\u0026#39;) as inf: labels = json.load(inf) # ç»„è£…æµæ°´çº¿ pipline = Pipeline([(\u0026#39;bag-of-words\u0026#39;, NLTKBOW()), (\u0026#39;vectorizer\u0026#39;, DictVectorizer()), (\u0026#39;naive-bayes\u0026#39;, BernoulliNB())]) # ç”¨F1å€¼æ¥è¯„ä¼° scores = cross_val_score(pipline, tweets, labels, scoring=\u0026#39;f1\u0026#39;) print(\u0026#34;Score: {:.3f}\u0026#34;.format(np.mean(scores))) model = pipline.fit(tweets, labels) nb = model.named_steps[\u0026#39;naive-bayes\u0026#39;] feature_probabilities = nb.feature_log_prob_ top_features = np.argsort(-feature_probabilities[1])[:50] dv = model.named_steps[\u0026#39;vectorizer\u0026#39;] for i, feature_index in enumerate(top_features): print(i, dv.feature_names_[feature_index], np.exp(feature_probabilities[1][feature_index])) Output:\næš‚æ—¶æ²¡æœ‰æ•°æ®é›†  ç¬¬ä¸ƒç«  æœ¬ç« ä»‹ç»çš„ç®—æ³•å¼•å…¥èšç±»åˆ†ææ¦‚å¿µ\u0026ndash;æ ¹æ®ç›¸ä¼¼åº¦ï¼ŒæŠŠå¤§æ•°æ®é›†åˆ’åˆ†ä¸ºå‡ ä¸ªå­é›†ã€‚\nåŠ è½½æ•°æ®é›† ç”±äºç”³è¯·ä¸åˆ° Twitter å¼€å‘è€…è´¦å·ï¼Œæˆ‘æƒ³åŠæ³•çˆ¬äº†ä¸€äº› b ç«™ç”¨æˆ·å…³æ³¨æ•°æ®ï¼Œåšæˆäº†æœ¬æ¬¡è¯•éªŒç›¸ä»¿çš„å½¢å¼\nInput:\n# -*- coding: utf-8 -*- import json import pandas as pd import networkx as nx from matplotlib import pyplot as plt import numpy as np from scipy.optimize import minimize from sklearn.metrics import silhouette_score if __name__ == \u0026#39;__main__\u0026#39;: with open(\u0026#39;bili.txt\u0026#39;, mode=\u0026#39;r\u0026#39;) as fin: temp = json.load(fin) users = pd.DataFrame(temp) users.columns = [\u0026#39;Id\u0026#39;, \u0026#39;Friends\u0026#39;] print(users[:5]) Output:\n Id Friends 0 214582845 [4370617, 259345180, 186334806, 546195, 477132... 1 4370617 [74507, 883968, 122879, 585267] 2 259345180 [] 3 186334806 [] 4 546195 []   å°†æ¯ä¸ªè®°å½•çš„ç”¨æˆ·å·¦å³ main_usersï¼ŒæŠŠä»–ä»¬å…³æ³¨çš„äººä½œä¸ºè¾¹ï¼Œç”Ÿæˆæœ‰å‘å›¾\nç”±äºå¯¹ matplotlib åº“å’Œ networkx åº“äº†è§£å¤ªå°‘ï¼Œåœ¨ä½œå›¾æ—¶é‡åˆ°äº†è®¸å¤šå›°éš¾ï¼ˆæ ¹åŸºä¸ç‰¢ï¼Œåœ°åŠ¨å±±æ‘‡ã€‚(\u0026gt;_\u0026lt;)ï¼‰\nInput:\nG = nx.DiGraph() main_users = list(users[\u0026#39;Id\u0026#39;].values) for u in main_users: G.add_node(u, label=u) for user in users.values: friends = user[1] for friend in friends: if friend in main_users: G.add_edge(user[0], int(friend)) print(\u0026#39;graph finished\u0026#39;) plt.figure(3, figsize=(100, 100)) nx.draw(G, alpha=0.1, edge_color=\u0026#39;b\u0026#39;, with_labels=True, font_size=16, node_size=30, node_color=\u0026#39;r\u0026#39;) plt.savefig(\u0026#39;fix1.png\u0026#39;) Output:\n fix1 \n åˆ›å»ºç”¨æˆ·ç›¸ä¼¼åº¦å›¾\nç”±äºæ¯ä¸ªç”¨æˆ·å…³æ³¨çš„äººæ•°å¯èƒ½ç›¸å·®å¾ˆå¤§ï¼Œå› æ­¤ä½¿ç”¨æ°å¡å¾·ç›¸ä¼¼ç³»æ•°ï¼ˆä¸¤ä¸ªç”¨æˆ·å…³æ³¨çš„é›†åˆçš„äº¤é›†é™¤ä»¥å¹¶é›†ï¼‰ï¼Œè¯¥ç³»æ•°åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œä»£è¡¨ä¸¤è€…é‡åˆçš„æ¯”ä¾‹ã€‚\nè§„èŒƒåŒ–æ˜¯æ•°æ®æŒ–æ˜çš„ä¸€ä¸ªé‡è¦æ–¹æ³•ï¼Œè¦åšæŒä½¿ç”¨ï¼ˆé™¤éæœ‰å……è¶³çš„ç†ç”±ä¸è¿™æ ·åšï¼‰\nè®¿é—®http://networkx.lanl.gov/reference/drawing/htmläº†è§£ networkx çš„å¸ƒå±€æ–¹æ³•\nInput:\nfriends = {user: set(friends) for user, friends in users.values} def compute_similarity(friends1, friends2): return len(friends1 \u0026amp; friends2) / len(friends1 | friends2) def create_graph(followers, threshold=0.0): G = nx.Graph() for user1 in friends.keys(): if len(friends[user1]) == 0: continue for user2 in friends.keys(): if len(friends[user2]) == 0: continue if user1 == user2: continue weight = compute_similarity(friends[user1], friends[user2]) if weight \u0026gt;= threshold: G.add_node(user1, lable=user1) G.add_node(user2, lable=user2) G.add_edge(user1, user2, weight=weight) return G G = create_graph(friends) plt.figure(3, figsize=(100, 100)) pos = nx.spring_layout(G) nx.draw_networkx_nodes(G, pos, node_size=30) edgewidth = [d[\u0026#39;weight\u0026#39;] for (u, v, d) in G.edges(data=True)] nx.draw_networkx_edges(G, pos, width=edgewidth) plt.savefig(\u0026#39;fix2.png\u0026#39;) Output:\n fix2 \nå¯»æ‰¾å­å›¾ networkx çš„ connected_component_subgraphs() å‡½æ•°åœ¨ 2.1 ç‰ˆæœ¬ä¸­è¢«ç§»é™¤äº†ï¼ˆä»£ç è¿‡æ—¶çš„æ¯”è¾ƒå¤šï¼Œå¹¶ä¸”ä½¿ç”¨ Twitter ä½œä¸ºæ¼”ç¤ºæ•°æ®é›†è®©æˆ‘è¿™ä¸¤ç« åšçš„å¾ˆå¤´ç–¼ï¼‰ï¼Œæˆ‘æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£åå‘ç°å¯ä»¥ä½¿ç”¨ connected_components() æ›¿ä»£ï¼Œä½†æ˜¯æ­¤å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œä¸€æ¬¡ç”Ÿæˆä¸€ç»„è¿é€šé¡¶ç‚¹ï¼Œå¯ä»¥é…åˆ G.subgraph(nodes) ä½¿ç”¨è·å¾—è¿é€šåˆ†æ”¯\nInput:\n# ç”Ÿæˆæ–°å›¾ï¼ŒæŒ‡å®šæœ€ä½é˜ˆå€¼ä¸º0.1 G = create_graph(friends, 0.1) sub_graphs = nx.connected_components(G) for i, sub_graphs in enumerate(sub_graphs): n_nodes = len(sub_graphs) print(\u0026#34;Subgraph{}has {}nodes\u0026#34;.format(i, n_nodes)) print(\u0026#39;---------------------\u0026#39;) G = create_graph(friends, 0.15) sub_graphs = nx.connected_components(G) for i, sub_graphs in enumerate(sub_graphs): n_nodes = len(sub_graphs) print(\u0026#34;Subgraph{}has {}nodes\u0026#34;.format(i, n_nodes)) sub_graphs = [c for c in sorted(nx.connected_components(G), key=len, reverse=True)] n_subgraphs = nx.number_connected_components(G) fig = plt.figure(figsize=(20, (n_subgraphs*3))) for i, sub_graph in enumerate(sub_graphs): # sub_graphæ˜¯ä¸€ä¸ªè¿é€šåˆ†æ”¯é¡¶ç‚¹çš„é›†åˆ ax = fig.add_subplot(int(n_subgraphs / 3) + 1, 3, i + 1) # å°†åæ ‡è½´æ ‡ç­¾å…³æ‰ ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) pos = nx.spring_layout(G) nx.draw(G=G.subgraph(sub_graph), alpha=0.1, edge_color=\u0026#39;b\u0026#39;, with_labels=True, font_size=16, node_size=30, node_color=\u0026#39;r\u0026#39;, ax=ax) plt.show() Output:\n fix3 \n è½®å»“ç³»æ•°å®šä¹‰ï¼š s = (b - a) / max(a, b)\nå…¶ä¸­ a ä¸ºç°‡å†…è·ç¦»ï¼Œè¡¨ç¤ºä¸ç°‡å†…å…¶å®ƒä¸ªä½“ä¹‹é—´çš„å¹³å‡è·ç¦»ã€‚b ä¸ºç°‡é—´è·ç¦»ï¼Œä¹Ÿå°±æ˜¯æœ€è¿‘ç°‡å†…å„ä¸ªä¸ªä½“ä¹‹é—´çš„å¹³å‡è·ç¦»\nInput:\ndef compute_silhouette(threshold, friends): G = create_graph(friends, threshold=threshold)\\ # å›¾æ˜¯å¦è‡³å°‘æœ‰ä¸¤ä¸ªé¡¶ç‚¹ if len(G.nodes()) \u0026lt; 2: # è¿”å›-99è¡¨ç¤ºé—®é¢˜æ— æ•ˆ return -99 # æŠ½å–è¿é€šåˆ†æ”¯ sub_graphs = nx.connected_components(G) # è‡³å°‘æœ‰ä¸¤ä¸ªè¿é€šåˆ†æ”¯ if not (2 \u0026lt;= nx.number_connected_components(G) \u0026lt; len(G.nodes()) - 1): return -99 label_dict = {} for i, sub_graph in enumerate(sub_graphs): for node in sub_graph: # ç»™ä¸åŒè¿é€šåˆ†æ”¯çš„é¡¶ç‚¹åˆ†é…ä¸åŒçš„æ ‡ç­¾ label_dict[node] = i labels = np.array([label_dict[node] for node in G.nodes()]) X = nx.to_scipy_sparse_matrix(G).todense() # è¿™é‡Œè¦å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºè·ç¦»ï¼Œæ‰€ä»¥ç”¨æœ€å¤§ç›¸ä¼¼åº¦å‡å»ç°æœ‰ç›¸ä¼¼åº¦ï¼ŒæŠŠç›¸ä¼¼åº¦è½¬åŒ–ä¸ºè·ç¦» X = 1 - X # è¿™é‡Œå°†è·ç¦»çŸ©é˜µçš„å¯¹è§’çº¿å¤„ç†ä¸º0ï¼Œå› ä¸ºè‡ªå·±åˆ°è‡ªå·±çš„è·ç¦»ä¸º0 np.fill_diagonal(X, 0) return silhouette_score(X, labels, metric=\u0026#39;precomputed\u0026#39;) def inverted_silhouette(threshold, friends): # å¯¹è½®å»“ç³»æ•°å–åï¼Œå°†æ‰“åˆ†å‡½æ•°è½¬åŒ–æˆæŸå¤±å‡½æ•° res = compute_silhouette(threshold, friends=friends) return - res # minimizeå‡½æ•°æ˜¯ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå€¼è¶Šå°è¶Šå¥½ # å‚æ•°ï¼šinverted_silhouetteè¦å¯»æ‰¾çš„å‡½æ•°ï¼›0.1å¼€å§‹æ—¶çŒœæµ‹çš„é˜ˆå€¼ï¼›options={\u0026#39;maxiter\u0026#39;: 10} åªè¿›è¡Œ10è½®è¿­ä»£ï¼Œå¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œæ•ˆæœå¯èƒ½æ›´å¥½ï¼Œä½†è¿è¡Œæ—¶é—´ä¼šå¢åŠ ï¼Œmethod=\u0026#39;nelder-mead\u0026#39;ä½¿ç”¨\u0026#34;ä¸‹å±±å•çº¯å½¢æ³•\u0026#34;ä¼˜åŒ–æ–¹æ³• result = minimize(inverted_silhouette, 0.1, args=(friends,), options={\u0026#39;maxiter\u0026#39;: 10}) print(result.x) Output:\n[0.10005086]   æœ¬ç« æ¢è®¨äº†ç¤¾äº¤ç½‘ç»œå’Œå›¾ä»¥åŠå¦‚ä½•å¯¹å…¶è¿›è¡Œèšç±»åˆ†æã€‚ç›®æ ‡æ˜¯æ¨èç”¨æˆ·ï¼Œä½¿ç”¨èšç±»åˆ†ææ–¹æ³•èƒ½å¤Ÿæ‰¾åˆ°ä¸åŒçš„ç”¨æˆ·ç°‡ï¼Œä¸»è¦æ­¥éª¤æœ‰æ ¹æ®ç›¸ä¼¼åº¦åˆ›å»ºåŠ æƒå›¾ï¼Œä»å›¾ä¸­å¯»æ‰¾è¿é€šåˆ†æ”¯ã€‚åˆ›å»ºå›¾æ—¶ç”¨åˆ°äº† NetworkX åº“ã€‚\nè¿˜æ¯”è¾ƒäº†å‡ å¯¹æ„ä¹‰ç›¸åçš„æ¦‚å¿µã€‚å¯¹äºä¸¤è€…ä¹‹é—´çš„ç›¸ä¼¼åº¦è¿™ä¸ªæ¦‚å¿µï¼Œå€¼è¶Šå¤§ï¼Œè¡¨æ˜ä¸¤è€…ä¹‹é—´æ›´ç›¸åƒã€‚ç›¸åï¼Œå¯¹äºè·ç¦»è€Œè¨€ï¼Œå€¼è¶Šå°ï¼Œä¸¤è€…æ›´ç›¸åƒã€‚å¦å¤–ä¸€å¯¹æ˜¯æŸå¤±å‡½æ•°å’Œæ‰“åˆ†å‡½æ•°ã€‚å¯¹äºæŸå¤±å‡½æ•°ï¼Œå€¼è¶Šå°ï¼Œæ•ˆæœè¶Šå¥½ï¼ˆä¹Ÿå°±æ˜¯æŸå¤±è¶Šå°‘ï¼‰ã€‚è€Œå¯¹äºæ‰“åˆ†å‡½æ•°ï¼Œå€¼è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ã€‚\nç¬¬å…«ç«  æœ¬ç« ä½¿ç”¨ç¥ç»ç½‘ç»œåˆ†æè‡ªå·±ç”Ÿæˆçš„éªŒè¯ç å›¾åƒ\näººå·¥ç¥ç»ç½‘ç»œ ç¥ç»ç½‘ç»œç®—æ³•æœ€åˆæ˜¯æ ¹æ®äººç±»å¤§è„‘çš„å·¥ä½œæœºåˆ¶è®¾è®¡çš„ã€‚ç¥ç»ç½‘ç»œç”±ä¸€ç³»åˆ—ç›¸äº’è¿æ¥çš„ç¥ç»å…ƒç»„æˆã€‚æ¯ä¸ªç¥ç»å…ƒéƒ½æ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œæ¥æ”¶ä¸€å®šè¾“å…¥ï¼Œç»™å‡ºç›¸åº”è¾“å‡ºã€‚\nç¥ç»å…ƒå¯ä»¥ä½¿ç”¨ä»»ä½•æ ‡å‡†å‡½æ•°æ¥å¤„ç†æ•°æ®ï¼Œæ¯”å¦‚çº¿æ€§å‡½æ•°ï¼Œè¿™äº›å‡½æ•°ç»Ÿç§°ä¸ºæ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œç¥ç»ç½‘ç»œå­¦ä¹ ç®—æ³•è¦èƒ½æ­£å¸¸å·¥ä½œï¼Œæ¿€æ´»å‡½æ•°åº”å½“æ˜¯å¯å¯¼ï¼ˆderivableï¼‰å’Œå…‰æ»‘çš„ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æœ‰é€»è¾‘æ–¯è°›å‡½æ•°ï¼Œå‡½æ•°è¡¨è¾¾å¼å¦‚ä¸‹ï¼ˆx ä¸ºç¥ç»å…ƒçš„è¾“å…¥ï¼Œkã€L é€šå¸¸ä¸º 1ï¼Œè¿™æ—¶å‡½æ•°è¾¾åˆ°æœ€å¤§å€¼ï¼‰ã€‚\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_{0})}} $$\næ¯ä¸ªç¥ç»å…ƒæ¥æ”¶å‡ ä¸ªè¾“å…¥ï¼Œæ ¹æ®è¿™å‡ ä¸ªè¾“å…¥ï¼Œè®¡ç®—è¾“å‡ºã€‚è¿™æ ·çš„ä¸€ä¸ªä¸ªç¥ç»å…ƒè¿æ¥åœ¨ä¸€èµ·ç»„æˆäº†ç¥ç»ç½‘ç»œï¼Œå¯¹æ•°æ®æŒ–æ˜åº”ç”¨æ¥è¯´ï¼Œå®ƒéå¸¸å¼ºå¤§ã€‚è¿™äº›ç¥ç»å…ƒç´§å¯†è¿æ¥ï¼Œå¯†åˆ‡é…åˆï¼Œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ å¾—åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œæˆä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸæœ€å¼ºå¤§çš„æ¦‚å¿µä¹‹ä¸€ã€‚\næ•°æ®æŒ–æ˜åº”ç”¨çš„ç¥ç»ç½‘ç»œï¼Œç¥ç»å…ƒæŒ‰ç…§å±‚çº§è¿›è¡Œæ’åˆ—ï¼Œè‡³å°‘æœ‰ä¸‰å±‚\n ç¬¬ä¸€å±‚ï¼šè¾“å…¥å±‚ã€‚ç”¨æ¥æ¥æ”¶æ•°æ®é›†çš„è¾“å…¥ã€‚ç¬¬ä¸€å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒå¯¹è¾“å…¥è¿›è¡Œè®¡ç®—ï¼ŒæŠŠå¾—åˆ°çš„ç»“æœä¼ ç»™ç¬¬äºŒå±‚çš„ç¥ç»å…ƒã€‚è¿™ç§å«ä½œå‰å‘ç¥ç»ç½‘ç»œ éšå«å±‚ï¼šæ•°æ®è¡¨ç°æ–¹å¼ä»¤äººéš¾ä»¥ç†è§£ï¼Œä¸€å±‚æˆ–å¤šå±‚ æœ€åä¸€å±‚ï¼šè¾“å‡ºå±‚ã€‚è¾“å‡ºç»“æœè¡¨ç¤ºçš„æ˜¯ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ç»™å‡ºçš„åˆ†ç±»ç»“æœ  ç¥ç»å…ƒæ¿€æ´»å‡½æ•°é€šå¸¸ä½¿ç”¨é€»è¾‘æ–¯è°›å‡½æ•°ï¼Œæ¯å±‚ç¥ç»å…ƒä¹‹é—´ä¸ºå…¨è¿æ¥ï¼Œåˆ›å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œè¿˜éœ€è¦ç”¨åˆ°å…¶ä»–å‡ ä¸ªå‚æ•°ã€‚\nåˆ›å»ºè¿‡ç¨‹ï¼ŒæŒ‡å®šç¥ç»ç½‘ç»œçš„è§„æ¨¡éœ€è¦ç”¨åˆ°ä¸¤ä¸ªå‚æ•°ï¼šç¥ç»ç½‘ç»œå…±æœ‰å¤šå°‘å±‚ï¼Œéšå«å±‚æ¯å±‚æœ‰å¤šå°‘ä¸ªç¥ç»å…ƒï¼ˆè¾“å…¥å±‚å’Œè¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡é€šå¸¸ç”±æ•°æ®é›†æ¥å®šï¼‰ã€‚\nåˆ›å»ºæ•°æ®é›† ä½¿ç”¨é•¿åº¦ä¸º 4 ä¸ªå­—æ¯çš„è‹±æ–‡å•è¯ä½œä¸ºéªŒè¯ç \nInput:\n# -*- coding: utf-8 -*- import numpy as np from PIL import Image, ImageDraw, ImageFont from skimage import transform as tf from skimage.transform import resize from matplotlib import pyplot as plt from skimage.measure import label, regionprops # ç”¨äºå›¾åƒåˆ†å‰² from sklearn.utils import check_random_state from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from pybrain.datasets.supervised import SupervisedDataSet # ç¥ç»ç½‘ç»œæ•°æ®é›† from pybrain.tools.shortcuts import buildNetwork # æ„å»ºç¥ç»ç½‘ç»œ from pybrain.supervised.trainers.backprop import BackpropTrainer # åå‘ä¼ æ’­ç®—æ³• from sklearn.metrics import f1_score from nltk.corpus import words # å¯¼å…¥è¯­æ–™åº“ ç”¨äºç”Ÿæˆå•è¯ from sklearn.metrics import confusion_matrix # æ··æ·†çŸ©é˜µ from nltk.metrics import edit_distance # ç¼–è¾‘è·ç¦» from operator import itemgetter # ç”¨äºç”ŸæˆéªŒè¯ç ï¼Œæ¥æ”¶ä¸€ä¸ªå•è¯å’Œé”™åˆ‡å€¼ï¼Œè¿”å›ç”¨numpyæ•°ç»„æ ¼å¼è¡¨ç¤ºçš„å›¾åƒ def create_captcha(text, shear=0.0, size=(100, 26)): im = Image.new(\u0026#34;L\u0026#34;, size, \u0026#34;black\u0026#34;) draw = ImageDraw.Draw(im) # éªŒè¯ç æ–‡å­—æ‰€ç”¨å­—ä½“ï¼Œè¯¥å¼€æºå­—ä½“å¯åœ¨githubä¸‹è½½ font = ImageFont.truetype(\u0026#34;FiraCode-Medium.otf\u0026#34;, 22) draw.text((0, 0), text, fill=1, font=font) # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œä»¥ä¾¿ç”¨scikit-imageåº“ä¸ºå›¾åƒæ·»åŠ é”™åˆ‡å˜åŒ–æ•ˆæœ image = np.array(im) # åº”ç”¨é”™åˆ‡å˜åŒ–æ•ˆæœ affine_tf = tf.AffineTransform(shear=shear) image = tf.warp(image, affine_tf) # å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œç¡®ä¿ç‰¹å¾å€¼è½åœ¨0åˆ°1ä¹‹é—´ return image / image.max() if __name__ == \u0026#39;__main__\u0026#39;: image = create_captcha(\u0026#39;GENE\u0026#39;, shear=0.5) plt.imshow(image, cmap=\u0026#39;Greys\u0026#39;) plt.show() Output:\n 8.1 \n å°†å›¾åƒåˆ‡åˆ†ä¸ºå•ä¸ªçš„å­—æ¯\nInput:\ndef segment_image(image): \u0026#34;\u0026#34;\u0026#34; æ¥æ”¶å›¾åƒï¼Œè¿”å›å°å›¾åƒåˆ—è¡¨ :param image: :return: \u0026#34;\u0026#34;\u0026#34; # æ‰¾å‡ºåƒç´ å€¼ç›¸åŒåˆè¿æ¥åœ¨ä¸€èµ·çš„åƒç´ å—ï¼Œç±»ä¼¼ä¸Šä¸€ç« çš„è¿é€šåˆ†æ”¯ labeled_image = label(image \u0026gt; 0) subimages = [] for region in regionprops(labeled_image): # è·å–å½“å‰ä½ç½®çš„èµ·å§‹å’Œç»“æŸåæ ‡ start_x, start_y, end_x, end_y = region.bbox subimages.append(image[start_x:end_x, start_y:end_y]) # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å°å›¾åƒï¼Œåˆ™å°†åŸå›¾åƒä½œä¸ºå­å›¾è¿”å› if len(subimages) == 0: return [image, ] return subimages subimages = segment_image(image) f, axes = plt.subplots(1, len(subimages), figsize=(10, 3)) for i in range(len(subimages)): axes[i].imshow(subimages[i], cmap=\u0026#39;gray\u0026#39;) plt.show() Output:\n 8.2 \n åˆ›å»ºè®­ç»ƒé›†\nInput:\n# æŒ‡å®šéšæœºçŠ¶æ€å€¼ random_state = check_random_state(14) letters = list(\u0026#34;ABCDEFGHIJKLMNOPQRSTUVWXYZ\u0026#34;) shear_values = np.arange(0, 0.5, 0.05) # ç”¨æ¥ç”Ÿæˆä¸€æ¡è®­ç»ƒæ•°æ® def generate_sample(random_state=None): random_state = check_random_state(random_state) letter = random_state.choice(letters) shear = random_state.choice(shear_values) return create_captcha(letter, shear=shear, size=(25, 25)), letters.index(letter) image, target = generate_sample(random_state) plt.imshow(image, cmap=\u0026#39;Greys\u0026#39;) print(\u0026#34;The target for this image is {}\u0026#34;.format(target)) plt.show() # è°ƒç”¨3000æ¬¡æ­¤å‡½æ•°ï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®ä¼ åˆ°numpyçš„æ•°ç»„é‡Œ dataset, targets = zip(*(generate_sample(random_state) for i in range(3000))) dataset = np.array(dataset, dtype=float) targets = np.array(targets) # å¯¹26ä¸ªå­—æ¯ç±»åˆ«è¿›è¡Œç¼–ç  onehot = OneHotEncoder() y = onehot.fit_transform(targets.reshape(targets.shape[0], 1)) # å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ y = y.todense() # è°ƒæ•´å›¾åƒå¤§å° dataset = np.array([resize(segment_image(sample)[0], (20, 20)) for sample in dataset]) # å°†æœ€åä¸‰ç»´çš„datasetçš„åäºŒç»´æ‰å¹³åŒ– X = dataset.reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2])) X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9) Output:\nThe target for this image is 11   8.3 \nè®­ç»ƒå’Œåˆ†ç±» åå‘ä¼ æ’­ç®—æ³•ï¼ˆback propagationï¼Œbackpropï¼‰çš„å·¥ä½œæœºåˆ¶ä¸ºå¯¹é¢„æµ‹é”™è¯¯çš„ç¥ç»å…ƒæ–½ä»¥æƒ©ç½šã€‚ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œå‘ä¸Šå±‚å±‚æŸ¥æ‰¾é¢„æµ‹é”™è¯¯çš„ç¥ç»å…ƒï¼Œå¾®è°ƒè¿™äº›ç¥ç»å…ƒè¾“å…¥å€¼çš„æƒé‡ï¼Œä»¥è¾¾åˆ°ä¿®å¤è¾“å‡ºé”™è¯¯çš„ç›®çš„ã€‚\nç¥ç»å…ƒä¹‹æ‰€ä»¥ç»™å‡ºé”™è¯¯çš„é¢„æµ‹ï¼ŒåŸå› åœ¨äºå®ƒå‰é¢ä¸ºå…¶æä¾›è¾“å…¥çš„ç¥ç»å…ƒï¼Œæ›´ç¡®åˆ‡æ¥è¯´æ˜¯ç”±è¿™ä¸¤ä¸ªç¥ç»å…ƒä¹‹é—´è¾¹çš„æƒé‡åŠè¾“å…¥å€¼å†³å®šçš„ã€‚æˆ‘ä»¬å¯ä»¥å°è¯•å¯¹æƒé‡è¿›è¡Œå¾®è°ƒã€‚æ¯æ¬¡è°ƒæ•´çš„å¹…åº¦å–å†³äºä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢\n ç¥ç»å…ƒå„è¾¹æƒé‡çš„è¯¯å·®å‡½æ•°çš„åå¯¼æ•° ä¸€ä¸ªå«ä½œå­¦ä¹ é€Ÿç‡çš„å‚æ•°ï¼ˆé€šå¸¸ä½¿ç”¨å¾ˆå°çš„å€¼ï¼‰  è®¡ç®—å‡ºå‡½æ•°è¯¯å·®çš„æ¢¯åº¦ï¼Œå†ä¹˜ä»¥å­¦ä¹ é€Ÿç‡ï¼Œç”¨æ€»æƒé‡å‡å»å¾—åˆ°çš„å€¼ã€‚æ¢¯åº¦çš„ç¬¦å·ç”±è¯¯å·®å†³å®šï¼Œæ¯æ¬¡å¯¹æƒé‡çš„ä¿®æ­£éƒ½æ˜¯æœç€ç»™å‡ºæ­£ç¡®çš„é¢„æµ‹å€¼åŠªåŠ›ã€‚æœ‰æ—¶å€™ï¼Œä¿®æ­£ç»“æœä¸ºå±€éƒ¨æœ€ä¼˜ï¼ˆlocal optimaï¼‰ï¼Œæ¯”èµ·å…¶ä»–æƒé‡ç»„åˆè¦å¥½ï¼Œä½†æ‰€å¾—åˆ°çš„å„æƒé‡è¿˜ä¸æ˜¯æœ€ä¼˜ç»„åˆã€‚\nåå‘ä¼ æ’­ç®—æ³•ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œå±‚å±‚å‘ä¸Šå›æº¯åˆ°è¾“å…¥å±‚ã€‚åˆ°è¾¾è¾“å…¥å±‚åï¼Œæ‰€æœ‰è¾¹çš„æƒé‡æ›´æ–°å®Œæ¯•ã€‚\nè¿™é‡Œåœ¨å¯¼å…¥ SupervisedDataSet æ—¶å‘ç”Ÿäº†é”™è¯¯ï¼Œä½¿ç”¨ pip install pybrain å®‰è£…çš„åŒ…ä¼šæœ‰æ‰¾ä¸åˆ°æ–¹æ³•çš„ç°è±¡ï¼Œå› æ­¤æˆ‘ä» github-pybrain ä¸‹è½½äº†æºç åŒ…ï¼Œåœ¨è§£å‹åçš„æ–‡ä»¶å¤¹ä¸­è¾“å…¥ python setup.py install è¿›è¡Œå®‰è£…ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚è¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯åŸæ–‡ä½¿ç”¨ from pybrain.datasets import SupervisedDataSet æ¥å¯¼å…¥ SupervisedDataSet ä½†æ˜¯æˆ‘åœ¨å¯¼å…¥æ—¶å‘ç°å¹¶æ²¡æœ‰è¿™ä¸ªç±»ï¼Œäºæ˜¯çœ‹äº†é¡¹ç›®ç»“æ„åä½¿ç”¨ from pybrain.datasets.supervised import SupervisedDataSet è¿›è¡Œå¯¼å…¥ã€‚è¿˜æœ‰å‡ å¤„ç›¸åŒçš„é—®é¢˜å‡æ˜¯è¿™æ ·è§£å†³çš„ã€‚\nè¿™é‡Œåœ¨ä½¿ç”¨ f1_score è¿›è¡Œè¯„ä¼°æ—¶ä¹Ÿå‡ºç°äº†é”™è¯¯ï¼ŒåŸå› è§ä»£ç æ³¨é‡Šã€‚\nInput:\n# ä¸ºpybrainåº“åˆ›å»ºæ ¼å¼é€‚é…çš„æ•°æ®é›† training = SupervisedDataSet(X.shape[1], y.shape[1]) for i in range(X_train.shape[0]): training.addSample(X_train[i], y_train[i]) testing = SupervisedDataSet(X.shape[1], y.shape[1]) for i in range(X_test.shape[0]): testing.addSample(X_test[i], y_test[i]) # æŒ‡å®šç»´åº¦ï¼Œåˆ›å»ºç¥ç»ç½‘ç»œï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºè¾“å…¥å±‚ç¥ç»å…ƒæ•°é‡ï¼Œç¬¬äºŒä¸ªå‚æ•°éšå«å±‚ç¥ç»å…ƒæ•°é‡ï¼Œç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºè¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡ # biasåœ¨æ¯ä¸€å±‚ä½¿ç”¨ä¸€ä¸ªä¸€ç›´å¤„äºæ¿€æ´»çŠ¶æ€çš„åç½®ç¥ç»å…ƒ net = buildNetwork(X.shape[1], 100, y.shape[1], bias=True) # ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•è°ƒæ•´æƒé‡ trainer = BackpropTrainer(net, training, learningrate=0.01, weightdecay=0.01) # è®¾å®šä»£ç çš„è¿è¡Œæ­¥æ•° trainer.trainEpochs(epochs=20) # é¢„æµ‹å€¼ predictions = trainer.testOnClassData(dataset=testing) # f1_scoreçš„averageé»˜è®¤å€¼ä¸º\u0026#39;binary\u0026#39;ï¼Œå¦‚æœä¸æŒ‡å®šaverageåˆ™ä¼šå‘ç”ŸValueError print(\u0026#34;F-score:{0:.2f}\u0026#34;.format(f1_score(y_test.argmax(axis=1), predictions, average=\u0026#39;weighted\u0026#39;))) print(\u0026#34;F-score:{0:.2f}\u0026#34;.format(f1_score(y_test.argmax(axis=1), predictions, average=\u0026#39;micro\u0026#39;))) print(\u0026#34;F-score:{0:.2f}\u0026#34;.format(f1_score(y_test.argmax(axis=1), predictions, average=\u0026#39;macro\u0026#39;))) Output:\nF-score:1.00 F-score:1.00 F-score:1.00   é¢„æµ‹å•è¯\nInput:\n# æ¥æ”¶éªŒè¯ç ï¼Œç”¨ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œè¿”å›å•è¯é¢„æµ‹ç»“æœ def predict_captcha(captcha_image, neural_network): subimages = segment_image(captcha_image) predicted_word = \u0026#34;\u0026#34; # éå†å››å¼ å°å›¾åƒ for subimage in subimages: # è°ƒæ•´æ¯å¼ å°å›¾åƒçš„å¤§å°ä¸º20*20åƒç´  subimage = resize(subimage, (20,20)) # æŠŠå°å›¾åƒæ•°æ®ä¼ å…¥ç¥ç»ç½‘ç»œçš„è¾“å…¥å±‚ï¼Œæ¿€æ´»ç¥ç»ç½‘ç»œã€‚è¿™äº›æ•°æ®å°†åœ¨ç¥ç»ç½‘ç»œä¸­è¿›è¡Œä¼ æ’­ï¼Œè¿”å›è¾“å‡ºç»“æœ outputs = net.activate(subimage.flatten()) # ç¥ç»ç½‘ç»œè¾“å‡º26ä¸ªå€¼ï¼Œæ¯ä¸ªå€¼éƒ½æœ‰ç´¢å¼•å·ï¼Œåˆ†åˆ«å¯¹åº”lettersåˆ—è¡¨ä¸­æœ‰ç€ç›¸åŒç´¢å¼•çš„å­—æ¯ï¼Œæ¯ä¸ªå€¼çš„å¤§å°è¡¨ç¤ºä¸å¯¹åº”å­—æ¯çš„ç›¸ä¼¼åº¦ã€‚ä¸ºäº†è·å¾—å®é™…çš„é¢„æµ‹å€¼ï¼Œæˆ‘ä»¬å–åˆ°æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå†é€šè¿‡lettersåˆ—è¡¨æ‰¾åˆ°å¯¹åº”çš„å­—æ¯ prediction = np.argmax(outputs) # æŠŠä¸Šé¢å¾—åˆ°çš„å­—æ¯æ·»åŠ åˆ°æ­£åœ¨é¢„æµ‹çš„å•è¯ä¸­ predicted_word += letters[prediction] return predicted_word word = \u0026#34;GENE\u0026#34; captcha = create_captcha(word, shear=0.2) print(predict_captcha(captcha, net)) Output:\nGENE   nltk ä¸‹è½½è¯­æ–™åº“æ—¶å¯èƒ½ä¼šå¾ˆæ…¢ï¼Œéœ€è¦çš„å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½ã€‚å¦‚ä½•ç¦»çº¿å®‰è£… nltk è¯­æ–™åº“è‡ªè¡Œç™¾åº¦ã€‚\nInput:\ndef test_prediction(word, net, shear=0.2): captcha = create_captcha(word, shear=shear) prediction = predict_captcha(captcha, net) prediction = prediction[:4] # è¿”å›é¢„æµ‹ç»“æœæ˜¯å¦æ­£ç¡®ï¼ŒéªŒè¯ç ä¸­çš„å•è¯å’Œé¢„æµ‹ç»“æœçš„å‰å››ä¸ªå­—ç¬¦ return word == prediction, word, prediction # è¯­æ–™åº“ä¸­å­—é•¿ä¸º4çš„å•è¯åˆ—è¡¨ valid_words = [word.upper() for word in words.words() if len(word) == 4] num_correct = 0 num_incorrect = 0 for word in valid_words: correct, word, prediction = test_prediction(word, net, shear=0.2) if correct: num_correct += 1 else: num_incorrect += 1 print(\u0026#34;Number correct is {}\u0026#34;.format(num_correct)) print(\u0026#34;Number incorrect is {}\u0026#34;.format(num_incorrect)) # äºŒç»´æ··æ·†çŸ©é˜µï¼Œ æ¯è¡Œæ¯åˆ—å‡ä¸ºä¸€ä¸ªç±»åˆ« cm = confusion_matrix(np.argmax(y_test,axis=1), predictions) # æ··æ·†çŸ©é˜µä½œå›¾ plt.figure(figsize=(20, 20)) plt.imshow(cm) tick_marks = np.arange(len(letters)) plt.xticks(tick_marks, letters) plt.yticks(tick_marks, letters) plt.ylabel(\u0026#39;Actual\u0026#39;) plt.xlabel(\u0026#39;Predicted\u0026#39;) plt.show() Output:\nNumber correct is 3738 Number incorrect is 1775   8.4 \nç”¨è¯å…¸æå‡å‡†ç¡®ç‡ å‡è®¾éªŒè¯ç å…¨éƒ¨éƒ½æ˜¯è‹±è¯­å•è¯\nåˆ—æ–‡æ–¯å¦ç¼–è¾‘è·ç¦»ï¼ˆLevenshtein edit distanceï¼‰æ˜¯ä¸€ç§é€šè¿‡æ¯”è¾ƒä¸¤ä¸ªçŸ­å­—ç¬¦ä¸²ï¼Œç¡®å®šå®ƒä»¬ç›¸ä¼¼åº¦çš„æ–¹æ³•ã€‚å®ƒä¸å¤ªé€‚åˆæ‰©å±•ï¼Œå­—ç¬¦ä¸²å¾ˆé•¿æ—¶é€šå¸¸ä¸ç”¨è¿™ç§æ–¹æ³•ã€‚ç¼–è¾‘è·ç¦»éœ€è¦è®¡ç®—ä»ä¸€ä¸ªå•è¯å˜ä¸ºå¦ä¸€ä¸ªå•è¯æ‰€éœ€è¦çš„æ­¥éª¤æ•°ã€‚ä»¥ä¸‹æ“ä½œéƒ½ç®—ä¸€æ­¥\n åœ¨å•è¯çš„ä»»æ„ä½ç½®æ’å…¥ä¸€ä¸ªæ–°å­—æ¯ ä»å•è¯ä¸­åˆ é™¤ä»»æ„ä¸€ä¸ªå­—æ¯ æŠŠä¸€ä¸ªå­—æ¯æ›¿æ¢ä¸ºå¦å¤–ä¸€ä¸ªå­—æ¯  Input:\n# è·å¾—ä¸¤ä¸ªå•è¯çš„ç¼–è¾‘è·ç¦» steps = edit_distance(\u0026#34;STEP\u0026#34;, \u0026#34;STOP\u0026#34;) print(\u0026#34;The num of steps needed is: {}\u0026#34;.format(steps)) # ç”¨è¯é•¿4å‡å»åŒç­‰ä½ç½®ä¸Šç›¸åŒçš„å­—æ¯æ•°é‡ï¼Œå¾—åˆ°çš„å€¼è¶Šå°è¡¨ç¤ºä¸¤ä¸ªè¯ç›¸ä¼¼åº¦è¶Šé«˜ def compute_distance(prediction, word): return len(prediction) - sum(prediction[i] == word[i] for i in range(len(prediction))) # æ”¹è¿›é¢„æµ‹å‡½æ•° def improved_prediction(word, net, dictionary, shear=0.2): captcha = create_captcha(word, shear=shear) prediction = predict_captcha(captcha, net) prediction = prediction[:4] # å¦‚æœå•è¯ä¸åœ¨è¯å…¸ä¸­åˆ™æ¯”è¾ƒå–è¯å…¸ä¸­è·ç¦»æœ€å°çš„å•è¯ if prediction not in dictionary: distance = sorted([(w, compute_distance(prediction, w)) for w in dictionary], key=itemgetter(1)) best_word = distance[0] prediction = best_word[0] return word == prediction, word, prediction num_correct = 0 num_incorrect = 0 for word in valid_words: correct, word, prediction = improved_prediction(word, net, valid_words,shear=0.2) if correct: num_correct += 1 else: num_incorrect += 1 print(\u0026#34;Number correct is {}\u0026#34;.format(num_correct)) print(\u0026#34;Number incorrect is {}\u0026#34;.format(num_incorrect)) Output:\nThe num of steps needed is: 1 Number correct is 3785 Number incorrect is 1728  æ­£ç¡®ç‡ç¨æœ‰æå‡\nç¬¬ä¹ç«  æ˜¨å¤©è·‘å»æ wordpress æ­å»ºç½‘ç«™äº† (à¹‘â€¢Ì â‚ƒâ€¢Ì€à¹‘) ï¼ˆæ‘¸é±¼çœŸèˆ’æœ\næœ¬ç« ä¸»è¦ä»‹ç»å¦‚ä¸‹å†…å®¹\n ç‰¹å¾å·¥ç¨‹å’Œå¦‚ä½•æ ¹æ®åº”ç”¨é€‰æ‹©ç‰¹å¾ å¸¦ç€æ–°é—®é¢˜ï¼Œé‡æ–°å›é¡¾è¯è¢‹æ¨¡å‹ ç‰¹å¾ç±»å‹å’Œå­—ç¬¦ N å…ƒè¯­æ³•æ¨¡å‹ æ”¯æŒå‘é‡æœº æ•°æ®é›†æ¸…æ´—  ä¸ºä½œå“æ‰¾åˆ°ä½œè€… ä½œè€…å½’å±å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§åˆ†ç±»é—®é¢˜ï¼Œå·²çŸ¥ä¸€éƒ¨åˆ†ä½œè€…ï¼Œæ•°æ®é›†ä¸ºå¤šä¸ªä½œè€…çš„ä½œå“ï¼ˆè®­ç»ƒé›†ï¼‰ï¼Œç›®æ ‡æ˜¯ç¡®å®šä¸€ç»„ä½œè€…ä¸è¯¦çš„ä½œå“ï¼ˆæµ‹è¯•é›†ï¼‰æ˜¯è°å†™çš„ã€‚å¦‚æœä½œè€…æ°å¥½æ˜¯å·²çŸ¥çš„ä½œè€…é‡Œé¢çš„ï¼Œè¿™ç§é—®é¢˜å«ä½œå°é—­é—®é¢˜\nå¦‚æœä½œè€…å¯èƒ½ä¸åœ¨é‡Œé¢ï¼Œè¿™ç§é—®é¢˜å°±å«ä½œå¼€æ”¾é—®é¢˜\nè·å–æ•°æ®ï¼Œä¹¦ä¸­çš„é“¾æ¥æœ‰å¾ˆå¤šå·²ç»å¤±æ•ˆï¼Œæˆ‘å‚è€ƒç½‘ä¸Šçš„å–å¾—äº†ä¸‹è½½æ–¹å¼ã€‚\nInput:\n# -*- coding: utf-8 -*- # get_data.py import requests import os import time from collections import defaultdict titles = {\u0026#39;burton\u0026#39;: [4657, 2400, 5760, 6036, 7111, 8821, 18506, 4658, 5761, 6886, 7113], \u0026#39;dickens\u0026#39;: [24022, 1392, 1414, 1467, 2324, 580, 786, 888, 963, 27924, 1394, 1415, 15618, 25985, 588, 807, 914, 967, 30127, 1400, 1421, 16023, 28198, 644, 809, 917, 968, 1023, 1406, 1422, 17879, 30368, 675, 810, 924, 98, 1289, 1413, 1423, 17880, 32241, 699, 821, 927], \u0026#39;doyle\u0026#39;: [2349, 11656, 1644, 22357, 2347, 290, 34627, 5148, 8394, 26153, 12555, 1661, 23059, 2348, 294, 355, 5260, 8727, 10446, 126, 17398, 2343, 2350, 3070, 356, 5317, 903, 10581, 13152, 2038, 2344, 244, 32536, 423, 537, 108, 139, 2097, 2345, 24951, 32777, 4295, 7964, 11413, 1638, 21768, 2346, 2845, 3289, 439, 834], \u0026#39;gaboriau\u0026#39;: [1748, 1651, 2736, 3336, 4604, 4002, 2451, 305, 3802, 547], \u0026#39;nesbit\u0026#39;: [34219, 23661, 28804, 4378, 778, 20404, 28725, 33028, 4513, 794], \u0026#39;tarkington\u0026#39;: [1098, 15855, 1983, 297, 402, 5798, 8740, 980, 1158, 1611, 2326, 30092, 483, 5949, 8867, 13275, 18259, 2595, 3428, 5756, 6401, 9659], \u0026#39;twain\u0026#39;: [1044, 1213, 245, 30092, 3176, 3179, 3183, 3189, 74, 86, 1086, 142, 2572, 3173, 3177, 3180, 3186, 3192, 76, 91, 119, 1837, 2895, 3174, 3178, 3181, 3187, 3432, 8525]} assert len(titles) == 7 assert len(titles[\u0026#39;tarkington\u0026#39;]) == 22 assert len(titles[\u0026#39;dickens\u0026#39;]) == 44 assert len(titles[\u0026#39;nesbit\u0026#39;]) == 10 assert len(titles[\u0026#39;doyle\u0026#39;]) == 51 assert len(titles[\u0026#39;twain\u0026#39;]) == 29 assert len(titles[\u0026#39;burton\u0026#39;]) == 11 assert len(titles[\u0026#39;gaboriau\u0026#39;]) == 10 url_base = \u0026#39;http://www.gutenberg.org/files/\u0026#39; url_format = \u0026#39;{url_base}{id}/{id}-0.txt\u0026#39; # ä¿®å¤URL url_fix_format = \u0026#39;http://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\u0026#39; fiexes = defaultdict(list) # fixes = {} # fixes[4657] = \u0026#39;http://www.gutenberg.org/cache/epub/4657/pg4657.txt\u0026#39; # make parent folder if not exists # data_folder = os.path.join(os.path.expanduser(\u0026#39;~\u0026#39;),\u0026#39;Data\u0026#39;,\u0026#39;books\u0026#39;) # # è¿™æ˜¯åœ¨ç”¨æˆ·userç›®å½•ä¸­å­˜å‚¨ data_folder = os.path.dirname(os.path.abspath(__file__)) if __name__ == \u0026#39;__main__\u0026#39;: if not os.path.exists(data_folder): os.makedirs(data_folder) print(data_folder) for author in titles: print(\u0026#39;Downloading titles from\u0026#39;, author) # make author\u0026#39;s folder if not exists author_folder = os.path.join(data_folder, author) if not os.path.exists(author_folder): os.makedirs(author_folder) # download each title to this folder for bookid in titles[author]: # if bookid in fixes: # print(\u0026#39; - Applying fix to book with id\u0026#39;, bookid) # url = fixes[bookid] # else: # print(\u0026#39; - Getting book with id\u0026#39;, bookid) # url = url_format.format(url_base=url_base, id=bookid) url = url_format.format(url_base=url_base, id=bookid) print(\u0026#39; - \u0026#39;, url) filename = os.path.join(author_folder, \u0026#39;%s.txt\u0026#39; % bookid) if os.path.exists(filename): print(\u0026#39; - File already exists, skipping\u0026#39;) else: r = requests.get(url) if r.status_code == 404: print(\u0026#39;url 404:\u0026#39;, author, bookid, \u0026#39;add to fixes list\u0026#39;) fiexes[author].append(bookid) else: txt = r.text with open(filename, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(txt) time.sleep(1) print(\u0026#39;Download complete\u0026#39;) print(\u0026#39;å¼€å§‹ä¸‹è½½ä¿®å¤åˆ—è¡¨\u0026#39;) for author in fiexes: print(\u0026#39;å¼€å§‹ä¸‹è½½\u0026lt;%s\u0026gt;çš„ä½œå“\u0026#39; % author) author_folder = os.path.join(data_folder, author) if not os.path.exists(author_folder): os.makedirs(author_folder) for bookid in fiexes[author]: filename = os.path.join(author_folder, \u0026#39;%s.txt\u0026#39; % bookid) if os.path.exists(filename): print(\u0026#39;æ–‡ä»¶å·²ç»ä¸‹è½½ï¼Œè·³è¿‡\u0026#39;) else: url_fix = url_fix_format.format(id=bookid) print(\u0026#39; - \u0026#39;, url_fix) r = requests.get(url_fix) if r.status_code == 404: print(\u0026#39;åˆå‡ºé”™äº†ï¼\u0026#39;, author, bookid) else: with open(filename, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(r.text) time.sleep(1) print(\u0026#39;ä¿®å¤åˆ—è¡¨ä¸‹è½½å®Œæ¯•\u0026#39;) æœ€åä¸‹è½½å®Œæˆæœ‰ 177 æœ¬ä¹¦\n æ”¯æŒå‘é‡æœºæ˜¯ä¸€ç§äºŒç±»åˆ†ç±»å™¨ï¼Œæ‰©å±•åå¯ç”¨æ¥å¯¹å¤šä¸ªç±»åˆ«è¿›è¡Œåˆ†ç±» 9ï¼ˆå¯¹äºå¤šç§ç±»åˆ«çš„åˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºå¤šä¸ª SVM åˆ†ç±»å™¨â€”â€”æ¯ä¸ªè¿˜æ˜¯äºŒç±»åˆ†ç±»å™¨ï¼‰\nC å‚æ•°å¯¹äºè®­ç»ƒ SVM æ¥è¯´å¾ˆé‡è¦ï¼ŒC å‚æ•°ä¸åˆ†ç±»å™¨æ­£ç¡®åˆ†ç±»æ¯”ä¾‹ç›¸å…³ï¼Œä½†å¯èƒ½å¸¦æ¥è¿‡æ‹Ÿåˆçš„é£é™©ã€‚C å€¼è¶Šé«˜ï¼Œé—´éš”è¶Šå°ï¼Œè¡¨ç¤ºè¦å°½å¯èƒ½æŠŠæ‰€æœ‰æ•°æ®æ­£ç¡®åˆ†ç±»ã€‚C å€¼è¶Šå°ï¼Œé—´éš”è¶Šå¤§â€”â€”æœ‰äº›æ•°æ®å°†æ— æ³•æ­£ç¡®åˆ†ç±»ã€‚C å€¼ä½ï¼Œè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„å¯èƒ½æ€§å°±ä½ï¼Œä½†æ˜¯åˆ†ç±»æ•ˆæœå¯èƒ½ä¼šç›¸å¯¹è¾ƒå·®\nSVMï¼ˆåŸºç¡€å½¢å¼ï¼‰å±€é™æ€§ä¹‹ä¸€å°±æ˜¯åªèƒ½ç”¨æ¥å¯¹çº¿æ€§å¯åˆ†çš„æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚å¦‚æœæ•°æ®çº¿æ€§ä¸å¯åˆ†ï¼Œå°±è¦ç”¨åˆ°å†…æ ¸å‡½æ•°ï¼Œå°†å…¶ç½®å…¥æ›´é«˜ç»´çš„ç©ºé—´ä¸­ï¼ŒåŠ å…¥æ›´å¤šä¼ªç‰¹å¾ç›´åˆ°æ•°æ®çº¿æ€§å¯åˆ†ã€‚å¸¸ç”¨çš„å†…æ ¸å‡½æ•°æœ‰å‡ ç§ã€‚çº¿æ€§å†…æ ¸æœ€ç®€å•ï¼Œå®ƒæ— å¤–ä¹ä¸¤ä¸ªä¸ªä½“çš„ç‰¹å¾å‘é‡çš„ç‚¹ç§¯ã€å¸¦æƒé‡çš„ç‰¹å¾å’Œåç½®é¡¹ã€‚å¤šé¡¹å¼æ ¸æé«˜ç‚¹ç§¯çš„é˜¶æ•°ï¼ˆæ¯”å¦‚ 2ï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰é«˜æ–¯å†…æ ¸ï¼ˆrbfï¼‰ã€Sigmoind å†…æ ¸\nInput:\n# -*- coding: utf-8 -*- # author_test.py import os import numpy as np from sklearn.feature_extraction.text import CountVectorizer from sklearn.svm import SVC # æ”¯æŒå‘é‡æœº from sklearn.model_selection import cross_val_score from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV from ch9 import getdata # å»æ‰å¤è—¤å ¡çš„è¯´æ˜ def clean_book(document): lines = document.split(\u0026#34;\\n\u0026#34;) start = 0 end = len(lines) for i in range(len(lines)): line = lines[i] if line.startswith(\u0026#34;*** START OF THIS PROJECT GUTENBERG\u0026#34;): start = i + 1 elif line.startswith(\u0026#34;*** END OF THIS PROJECT GUTENBERG\u0026#34;): end = i - 1 return \u0026#34;\\n\u0026#34;.join(lines[start:end]) def load_books_data(folder=getdata.data_folder): # å­˜å‚¨æ–‡æ¡£å’Œä½œè€… documents = [] authors = [] # éå†å­æ–‡ä»¶å¤¹ subfolders = [subfolder for subfolder in os.listdir(folder) if os.path.isdir(os.path.join(folder, subfolder))] for author_number, subfolder in enumerate(subfolders): full_subfolder_path = os.path.join(folder, subfolder) for document_name in os.listdir(full_subfolder_path): # è·³è¿‡ç›®å½•ä¸‹çš„getdata.pyæ–‡ä»¶ if document_name == \u0026#39;getdata.cpython-38.pyc\u0026#39;: continue with open(os.path.join(full_subfolder_path, document_name), \u0026#39;r\u0026#39;) as inf: documents.append(clean_book(inf.read())) authors.append(author_number) return documents, np.array(authors, \u0026#39;int\u0026#39;) # åŠŸèƒ½è¯ function_words = [\u0026#34;a\u0026#34;, \u0026#34;able\u0026#34;, \u0026#34;aboard\u0026#34;, \u0026#34;about\u0026#34;, \u0026#34;above\u0026#34;, \u0026#34;absent\u0026#34;, \u0026#34;according\u0026#34;, \u0026#34;accordingly\u0026#34;, \u0026#34;across\u0026#34;, \u0026#34;after\u0026#34;, \u0026#34;against\u0026#34;, \u0026#34;ahead\u0026#34;, \u0026#34;albeit\u0026#34;, \u0026#34;all\u0026#34;, \u0026#34;along\u0026#34;, \u0026#34;alongside\u0026#34;, \u0026#34;although\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;amid\u0026#34;, \u0026#34;amidst\u0026#34;, \u0026#34;among\u0026#34;, \u0026#34;amongst\u0026#34;, \u0026#34;amount\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;another\u0026#34;, \u0026#34;anti\u0026#34;, \u0026#34;any\u0026#34;, \u0026#34;anybody\u0026#34;, \u0026#34;anyone\u0026#34;, \u0026#34;anything\u0026#34;, \u0026#34;are\u0026#34;, \u0026#34;around\u0026#34;, \u0026#34;as\u0026#34;, \u0026#34;aside\u0026#34;, \u0026#34;astraddle\u0026#34;, \u0026#34;astride\u0026#34;, \u0026#34;at\u0026#34;, \u0026#34;away\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;barring\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;because\u0026#34;, \u0026#34;been\u0026#34;, \u0026#34;before\u0026#34;, \u0026#34;behind\u0026#34;, \u0026#34;being\u0026#34;, \u0026#34;below\u0026#34;, \u0026#34;beneath\u0026#34;, \u0026#34;beside\u0026#34;, \u0026#34;besides\u0026#34;, \u0026#34;better\u0026#34;, \u0026#34;between\u0026#34;, \u0026#34;beyond\u0026#34;, \u0026#34;bit\u0026#34;, \u0026#34;both\u0026#34;, \u0026#34;but\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;can\u0026#34;, \u0026#34;certain\u0026#34;, \u0026#34;circa\u0026#34;, \u0026#34;close\u0026#34;, \u0026#34;concerning\u0026#34;, \u0026#34;consequently\u0026#34;, \u0026#34;considering\u0026#34;, \u0026#34;could\u0026#34;, \u0026#34;couple\u0026#34;, \u0026#34;dare\u0026#34;, \u0026#34;deal\u0026#34;, \u0026#34;despite\u0026#34;, \u0026#34;down\u0026#34;, \u0026#34;due\u0026#34;, \u0026#34;during\u0026#34;, \u0026#34;each\u0026#34;, \u0026#34;eight\u0026#34;, \u0026#34;eighth\u0026#34;, \u0026#34;either\u0026#34;, \u0026#34;enough\u0026#34;, \u0026#34;every\u0026#34;, \u0026#34;everybody\u0026#34;, \u0026#34;everyone\u0026#34;, \u0026#34;everything\u0026#34;, \u0026#34;except\u0026#34;, \u0026#34;excepting\u0026#34;, \u0026#34;excluding\u0026#34;, \u0026#34;failing\u0026#34;, \u0026#34;few\u0026#34;, \u0026#34;fewer\u0026#34;, \u0026#34;fifth\u0026#34;, \u0026#34;first\u0026#34;, \u0026#34;five\u0026#34;, \u0026#34;following\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;four\u0026#34;, \u0026#34;fourth\u0026#34;, \u0026#34;from\u0026#34;, \u0026#34;front\u0026#34;, \u0026#34;given\u0026#34;, \u0026#34;good\u0026#34;, \u0026#34;great\u0026#34;, \u0026#34;had\u0026#34;, \u0026#34;half\u0026#34;, \u0026#34;have\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;heaps\u0026#34;, \u0026#34;hence\u0026#34;, \u0026#34;her\u0026#34;, \u0026#34;hers\u0026#34;, \u0026#34;herself\u0026#34;, \u0026#34;him\u0026#34;, \u0026#34;himself\u0026#34;, \u0026#34;his\u0026#34;, \u0026#34;however\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;if\u0026#34;, \u0026#34;in\u0026#34;, \u0026#34;including\u0026#34;, \u0026#34;inside\u0026#34;, \u0026#34;instead\u0026#34;, \u0026#34;into\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;it\u0026#34;, \u0026#34;its\u0026#34;, \u0026#34;itself\u0026#34;, \u0026#34;keeping\u0026#34;, \u0026#34;lack\u0026#34;, \u0026#34;less\u0026#34;, \u0026#34;like\u0026#34;, \u0026#34;little\u0026#34;, \u0026#34;loads\u0026#34;, \u0026#34;lots\u0026#34;, \u0026#34;majority\u0026#34;, \u0026#34;many\u0026#34;, \u0026#34;masses\u0026#34;, \u0026#34;may\u0026#34;, \u0026#34;me\u0026#34;, \u0026#34;might\u0026#34;, \u0026#34;mine\u0026#34;, \u0026#34;minority\u0026#34;, \u0026#34;minus\u0026#34;, \u0026#34;more\u0026#34;, \u0026#34;most\u0026#34;, \u0026#34;much\u0026#34;, \u0026#34;must\u0026#34;, \u0026#34;my\u0026#34;, \u0026#34;myself\u0026#34;, \u0026#34;near\u0026#34;, \u0026#34;need\u0026#34;, \u0026#34;neither\u0026#34;, \u0026#34;nevertheless\u0026#34;, \u0026#34;next\u0026#34;, \u0026#34;nine\u0026#34;, \u0026#34;ninth\u0026#34;, \u0026#34;no\u0026#34;, \u0026#34;nobody\u0026#34;, \u0026#34;none\u0026#34;, \u0026#34;nor\u0026#34;, \u0026#34;nothing\u0026#34;, \u0026#34;notwithstanding\u0026#34;, \u0026#34;number\u0026#34;, \u0026#34;numbers\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;off\u0026#34;, \u0026#34;on\u0026#34;, \u0026#34;once\u0026#34;, \u0026#34;one\u0026#34;, \u0026#34;onto\u0026#34;, \u0026#34;opposite\u0026#34;, \u0026#34;or\u0026#34;, \u0026#34;other\u0026#34;, \u0026#34;ought\u0026#34;, \u0026#34;our\u0026#34;, \u0026#34;ours\u0026#34;, \u0026#34;ourselves\u0026#34;, \u0026#34;out\u0026#34;, \u0026#34;outside\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;part\u0026#34;, \u0026#34;past\u0026#34;, \u0026#34;pending\u0026#34;, \u0026#34;per\u0026#34;, \u0026#34;pertaining\u0026#34;, \u0026#34;place\u0026#34;, \u0026#34;plenty\u0026#34;, \u0026#34;plethora\u0026#34;, \u0026#34;plus\u0026#34;, \u0026#34;quantities\u0026#34;, \u0026#34;quantity\u0026#34;, \u0026#34;quarter\u0026#34;, \u0026#34;regarding\u0026#34;, \u0026#34;remainder\u0026#34;, \u0026#34;respecting\u0026#34;, \u0026#34;rest\u0026#34;, \u0026#34;round\u0026#34;, \u0026#34;save\u0026#34;, \u0026#34;saving\u0026#34;, \u0026#34;second\u0026#34;, \u0026#34;seven\u0026#34;, \u0026#34;seventh\u0026#34;, \u0026#34;several\u0026#34;, \u0026#34;shall\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;should\u0026#34;, \u0026#34;similar\u0026#34;, \u0026#34;since\u0026#34;, \u0026#34;six\u0026#34;, \u0026#34;sixth\u0026#34;, \u0026#34;so\u0026#34;, \u0026#34;some\u0026#34;, \u0026#34;somebody\u0026#34;, \u0026#34;someone\u0026#34;, \u0026#34;something\u0026#34;, \u0026#34;spite\u0026#34;, \u0026#34;such\u0026#34;, \u0026#34;ten\u0026#34;, \u0026#34;tenth\u0026#34;, \u0026#34;than\u0026#34;, \u0026#34;thanks\u0026#34;, \u0026#34;that\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;their\u0026#34;, \u0026#34;theirs\u0026#34;, \u0026#34;them\u0026#34;, \u0026#34;themselves\u0026#34;, \u0026#34;then\u0026#34;, \u0026#34;thence\u0026#34;, \u0026#34;therefore\u0026#34;, \u0026#34;these\u0026#34;, \u0026#34;they\u0026#34;, \u0026#34;third\u0026#34;, \u0026#34;this\u0026#34;, \u0026#34;those\u0026#34;, \u0026#34;though\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;through\u0026#34;, \u0026#34;throughout\u0026#34;, \u0026#34;thru\u0026#34;, \u0026#34;thus\u0026#34;, \u0026#34;till\u0026#34;, \u0026#34;time\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;tons\u0026#34;, \u0026#34;top\u0026#34;, \u0026#34;toward\u0026#34;, \u0026#34;towards\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;under\u0026#34;, \u0026#34;underneath\u0026#34;, \u0026#34;unless\u0026#34;, \u0026#34;unlike\u0026#34;, \u0026#34;until\u0026#34;, \u0026#34;unto\u0026#34;, \u0026#34;up\u0026#34;, \u0026#34;upon\u0026#34;, \u0026#34;us\u0026#34;, \u0026#34;used\u0026#34;, \u0026#34;various\u0026#34;, \u0026#34;versus\u0026#34;, \u0026#34;via\u0026#34;, \u0026#34;view\u0026#34;, \u0026#34;wanting\u0026#34;, \u0026#34;was\u0026#34;, \u0026#34;we\u0026#34;, \u0026#34;were\u0026#34;, \u0026#34;what\u0026#34;, \u0026#34;whatever\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;whenever\u0026#34;, \u0026#34;where\u0026#34;, \u0026#34;whereas\u0026#34;, \u0026#34;wherever\u0026#34;, \u0026#34;whether\u0026#34;, \u0026#34;which\u0026#34;, \u0026#34;whichever\u0026#34;, \u0026#34;while\u0026#34;, \u0026#34;whilst\u0026#34;, \u0026#34;who\u0026#34;, \u0026#34;whoever\u0026#34;, \u0026#34;whole\u0026#34;, \u0026#34;whom\u0026#34;, \u0026#34;whomever\u0026#34;, \u0026#34;whose\u0026#34;, \u0026#34;will\u0026#34;, \u0026#34;with\u0026#34;, \u0026#34;within\u0026#34;, \u0026#34;without\u0026#34;, \u0026#34;would\u0026#34;, \u0026#34;yet\u0026#34;, \u0026#34;you\u0026#34;, \u0026#34;your\u0026#34;, \u0026#34;yours\u0026#34;, \u0026#34;yourself\u0026#34;, \u0026#34;yourselves\u0026#34;] if __name__ == \u0026#39;__main__\u0026#39;: # è·å–æ•°æ® documents, classes = load_books_data(getdata.data_folder) # æå–ç‰¹å¾è¯ extractor = CountVectorizer(vocabulary=function_words) # å‚æ•°å­—å…¸ parameters = {\u0026#39;kernel\u0026#39;: (\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;), \u0026#39;C\u0026#39;: [1, 10]} svr = SVC() # ä½¿ç”¨ç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°å€¼ grid = GridSearchCV(svr, parameters) # ä½¿ç”¨åŠŸèƒ½è¯åˆ†ç±» pipeline1 = Pipeline([(\u0026#39;feature_extraction\u0026#39;, extractor), (\u0026#39;clf\u0026#39;, grid)]) scores = cross_val_score(pipeline1, documents, classes, scoring=\u0026#39;f1_macro\u0026#39;) print(np.mean(scores)) Output:\n0.7738985477640941 Score: 0.813  N å…ƒè¯­æ³• N å…ƒè¯­æ³•ç”±ä¸€ç³»åˆ—çš„ N ä¸ªä¸ºä¸€ç»„çš„å¯¹è±¡ç»„æˆï¼ŒN ä¸ºæ¯ç»„å¯¹è±¡çš„ä¸ªæ•°\nInput:\n# ç”¨Nå…ƒè¯­æ³•åˆ†ç±» pipeline = Pipeline([(\u0026#39;feature_extraction\u0026#39;, CountVectorizer(analyzer=\u0026#39;char\u0026#39;, ngram_range=(3, 3))), # é•¿åº¦ä¸º3çš„Nå…ƒè¯­æ³• (\u0026#39;classifier\u0026#39;, grid) ]) scores = cross_val_score(pipeline, documents, classes, scoring=\u0026#39;f1_macro\u0026#39;) print(\u0026#34;Score: {:.3f}\u0026#34;.format(np.mean(scores))) Output:\nScore: 0.813  å®‰ç„¶é‚®ä»¶æ•°æ®é›†  è¯»å–æ•°æ®é›† æ¸…æ´—æ•°æ® ç»„è£…æµæ°´çº¿ ä½¿ç”¨ F å€¼è¯„ä¼°  # -*- coding: utf-8 -*- import os from email.parser import Parser # é‚®ä»¶è§£æå™¨ from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.utils import check_random_state # éšæœºçŠ¶æ€å®ä¾‹ from sklearn.metrics import confusion_matrix from matplotlib import pyplot as plt import numpy as np import quotequail enron_data_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), \u0026#34;maildir\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: p = Parser() def get_enron_corpus(num_authors=10, data_folder=enron_data_folder, min_docs_author=10, max_docs_author=100, random_state=None): random_state = check_random_state(random_state) # éšæœºå¯¹å¾—åˆ°çš„é‚®ç®±åˆ—è¡¨è¿›è¡Œæ’åº # os.listdirå‡½æ•°æ¯æ¬¡è¿”å›ç»“æœä¸ä¸€å®šç›¸åŒï¼Œåœ¨ä½¿ç”¨è¯¥å‡½æ•°å‰å…ˆæ’åºï¼Œä»è€Œä¿æŒè¿”å›ç»“æœçš„ä¸€è‡´æ€§ email_addresses = sorted(os.listdir(data_folder)) random_state.shuffle(email_addresses) documents = [] classes = [] author_num = 0 authors = {} # éå†é‚®ç®±æ–‡ä»¶å¤¹ï¼ŒæŸ¥æ‰¾å®ƒä¸‹é¢åå­—ä¸­å«æœ‰â€œsentâ€çš„è¡¨ç¤ºå‘ä»¶ç®±çš„å­æ–‡ä»¶å¤¹ for user in email_addresses: users_email_folder = os.path.join(data_folder, user) mail_folders = [os.path.join(users_email_folder, subfolder) for subfolder in os.listdir(users_email_folder) if \u0026#34;sent\u0026#34; in subfolder] try: # è·å–å­æ–‡ä»¶å¤¹ä¸­çš„æ¯ä¸€å°é‚®ä»¶ï¼Œè·³è¿‡å…¶ä¸­çš„å­æ–‡ä»¶å¤¹ authored_emails = [open(os.path.join(mail_folder, email_filename), encoding=\u0026#39;cp1252\u0026#39;).read() for mail_folder in mail_folders for email_filename in os.listdir(mail_folder)] except IsADirectoryError: continue # è·å¾—è‡³å°‘åå°é‚®ä»¶ if len(authored_emails) \u0026lt; min_docs_author: continue # æœ€å¤šè·å–å‰100å°é‚®ä»¶ if len(authored_emails) \u0026gt; max_docs_author: authored_emails = authored_emails[:max_docs_author] # è§£æé‚®ä»¶ï¼Œè·å–é‚®ä»¶å†…å®¹ contents = [p.parsestr(email)._payload for email in authored_emails] documents.extend(contents) # å°†å‘ä»¶äººæ·»åŠ åˆ°ç±»åˆ—è¡¨ä¸­ï¼Œæ¯å°é‚®ä»¶æ·»åŠ ä¸€æ¬¡ classes.extend([author_num] * len(authored_emails)) # è®°å½•æ”¶ä»¶äººç¼–å·ï¼Œå†æŠŠç¼–å·+1 authors[user] = author_num author_num += 1 # æ”¶ä»¶äººæ•°é‡è¾¾åˆ°è®¾ç½®çš„å€¼è·³å‡ºå¾ªç¯ if author_num \u0026gt;= num_authors or author_num \u0026gt;= len(email_addresses): break # è¿”å›é‚®ä»¶æ•°æ®é›†ä»¥åŠæ”¶ä»¶äººå­—å…¸ return documents, np.array(classes), authors documents, classes, authors = get_enron_corpus(data_folder=enron_data_folder, random_state=14) # ç§»é™¤é‚®ä»¶çš„å›å¤ä¿¡æ¯ def remove_replies(email_contents): r = quotequail.unwrap(email_contents) if r is None: return email_contents if \u0026#39;text_top\u0026#39; in r: return r[\u0026#39;text_top\u0026#39;] # å­—å…¸rä¸­å­˜åœ¨text_topï¼Œè¿”å›å®ƒçš„å€¼ elif \u0026#39;text\u0026#39; in r: return r[\u0026#39;text\u0026#39;] return email_contents documents = [remove_replies(document) for document in documents] parameters = {\u0026#39;kernel\u0026#39;: (\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;), \u0026#39;C\u0026#39;: [1, 10]} svr = SVC() grid = GridSearchCV(svr, parameters) pipeline = Pipeline([(\u0026#39;feature_extraction\u0026#39;, CountVectorizer(analyzer=\u0026#39;char\u0026#39;, ngram_range=(3, 3))), (\u0026#39;classifier\u0026#39;, grid) ]) scores = cross_val_score(pipeline, documents, classes, scoring=\u0026#39;f1_macro\u0026#39;) print(\u0026#34;Score: {:.3f}\u0026#34;.format(np.mean(scores))) Output:\nScore: 0.664   ä»æµæ°´çº¿ä¸­è·å¾—æœ€å¥½çš„å‚æ•°ç»„åˆ\nInput:\ntraining_documents, test_documents, y_train, y_test = train_test_split(documents, classes, random_state=14) pipeline.fit(training_documents, y_train) y_pred = pipeline.predict(test_documents) print(pipeline.named_steps[\u0026#39;classifier\u0026#39;].best_params_) Output:\n{'C': 10, 'kernel': 'rbf'}   ç»˜åˆ¶æ··æ·†çŸ©é˜µæŸ¥çœ‹åˆ†ç±»æƒ…å†µ\nInput:\ncm = confusion_matrix(y_test, y_pred) cm = cm / cm.astype(np.float).sum(axis=1) sorted_authors = sorted(authors.keys(), key=lambda x: authors[x]) plt.figure(figsize=(20, 20)) plt.imshow(cm, cmap=\u0026#39;Blues\u0026#39;) tick_marks = np.arange(len(sorted_authors)) plt.xticks(tick_marks, sorted_authors) plt.yticks(tick_marks, sorted_authors) plt.ylabel(\u0026#39;Actual\u0026#39;) plt.xlabel(\u0026#39;Predicted\u0026#39;) plt.show() Output:\n 9.1 \nç¬¬åç«  è¿™ä¸¤å¤©åœ¨é¼“æ£ jupyterlabï¼Œä¸€å¼€å§‹åœ¨æœåŠ¡å™¨ä¸Šå»ºäº†ä¸€ä¸ª lab ç¯å¢ƒï¼Œå¯æ¯æ¬¡è¿æ¥éƒ½è¦ç™»ä¸Šå‡ åˆ†é’Ÿï¼Œä¸çŸ¥é“æ˜¯æœåŠ¡å™¨ CPU ä¸è¡Œè¿˜æ˜¯ç½‘ç»œä¸è¡Œã€‚ç„¶ååˆåœ¨æœ¬åœ°é¼“æ£ï¼Œåœ¨ debian è£… nodejs å’Œ npm çš„æ—¶å€™æŠŠç³»ç»Ÿä¾èµ–æå´©äº†ï¼Œäºæ˜¯ç‹ ä¸‹å¿ƒæ¥é‡è£…äº†ç”µè„‘ã€‚ã€‚ã€‚å‘ç”Ÿçš„äº‹æƒ…å¤ªå¤šï¼Œå¿ƒç´¯ã€‚ã€‚\næ˜¨å¤©é‡è£…äº† Ubuntuï¼Œæäº†ä¸‹ç¾åŒ–ï¼Œå®‰è£…äº†å¿…é¡»çš„è½¯ä»¶ï¼ˆåˆ«è¯´ Ubuntu è¿˜æŒºå¥½ç”¨ï¼ŒçœŸé¦™ï¼‰\næˆ‘ä¿è¯è¿™æ˜¯æœ€åä¸€å¥åæ§½äº†ï¼Œä¸€å®š\næœ¬ç« ä»‹ç»å¦‚ä½•å¯¹æ–°é—»è¯­æ–™è¿›è¡Œèšç±»ï¼Œä»¥å‘ç°å…¶ä¸­çš„è¶‹åŠ¿å’Œä¸»é¢˜ã€‚\nè·å–æ–°é—»æ–‡ç«  è¿™ä¸€ç« çš„æ•°æ®é›†æ˜¯ä» reddit è·å¾—çš„ç½‘é¡µé“¾æ¥ï¼Œreddit çš„ app å®¡æ ¸æœºåˆ¶ä¸æ˜¯å¾ˆä¸¥æ ¼(?)å› æ­¤æˆ‘ç»ˆäºæ‹¿åˆ°äº†å¢™å¤–çš„ apiï¼Œä½¿ç”¨ requests ä¸‹è½½åˆè´¹äº†ä¸€ç•ªåŠŸå¤«ï¼Œä½¿ç”¨ä¹¦ä¸Šæºç çš„ url ä¸‹è½½æ€»æ˜¯ 403 é”™è¯¯ï¼Œç ”ç©¶äº†å¥½åŠå¤© reddit çš„ apiï¼Œå‘ç° reddit çš„ url æ”¹æˆäº†(new, top, \u0026hellip;)ï¼Œä¿®æ”¹ä¹‹åæ€»ç®—å®Œæˆäº†é“¾æ¥çš„ç´¢å¼•\nInput:\n# get_links.py # -*- coding: utf-8 -*- import json import os import requests import getpass import time # éœ€è¦çš„ä¸€äº›å‡­è¯ CLIENT_ID = \u0026#34;xxxxxxxxxxx\u0026#34; CLIENT_SECRET = \u0026#34;xxxxxxxxxxx\u0026#34; USER_AGENT = \u0026#34;python:xxxxxxxxx (by /u/xxxxxxxxx)\u0026#34; USERNAME = \u0026#34;xxxxxxxx\u0026#34; PASSWORD = \u0026#34;xxxxxxxxxxxxxx\u0026#34; # requestsä½¿ç”¨ä»£ç† proxies = {\u0026#34;http\u0026#34;: \u0026#34;socks5://xxxxxx\u0026#34;, \u0026#34;https\u0026#34;: \u0026#34;socks5://xxxxxx\u0026#34;} def login(username, password): if password is None: password = getpass.getpass( \u0026#34;Enter reddit password for user {}: \u0026#34;.format(username) ) headers = {\u0026#34;User-Agent\u0026#34;: USER_AGENT} # ä½¿ç”¨å‡­æ®è®¾ç½®èº«ä»½éªŒè¯å¯¹è±¡ client_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET) post_data = {\u0026#34;grant_type\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;username\u0026#34;: username, \u0026#34;password\u0026#34;: password} response = requests.post( \u0026#34;https://www.reddit.com/api/v1/access_token\u0026#34;, proxies=proxies, auth=client_auth, data=post_data, headers=headers, ) return response.json() if __name__ == \u0026#34;__main__\u0026#34;: # è°ƒç”¨loginè·å–token # token = login(USERNAME, PASSWORD) # print(token) token = { \u0026#34;access_token\u0026#34;: \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;xxxxx\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, \u0026#34;scope\u0026#34;: \u0026#34;*\u0026#34;, } def get_links(subreddit, token, n_pages=5): # å­˜æ”¾é“¾æ¥ä¿¡æ¯ stories = [] after = None for page_number in range(n_pages): # è¿›è¡Œè°ƒç”¨ä¹‹å‰ç­‰å¾…ï¼Œä»¥é¿å…è¶…è¿‡APIé™åˆ¶ print(\u0026#34;ç­‰å¾…2s...\u0026#34;) time.sleep(2) # è®¾ç½®æ ‡å¤´è¿›è¡Œè°ƒç”¨ headers = { \u0026#34;Authorization\u0026#34;: \u0026#34;bearer {}\u0026#34;.format(token[\u0026#34;access_token\u0026#34;]), \u0026#34;User-Agent\u0026#34;: USER_AGENT, } # topä¸ºæœ€çƒ­é“¾æ¥ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥æ¢æˆnew url = \u0026#34;https://oauth.reddit.com/r/{}/top?limit=100\u0026#34;.format(subreddit) if after: url += \u0026#34;\u0026amp;after={}\u0026#34;.format(after) while True: try: response = requests.get( url, proxies=proxies, headers=headers, timeout=10 ) result = response.json() # è·å–ä¸‹ä¸€ä¸ªå¾ªç¯çš„cursor after = result[\u0026#34;data\u0026#34;][\u0026#34;after\u0026#34;] except: print(\u0026#34;requestså‡ºé”™ç­‰å¾…...\u0026#34;) time.sleep(2) else: break # å°†æ‰€æœ‰æ–°é—»é¡¹æ·»åŠ åˆ°storyåˆ—è¡¨ä¸­ for story in result[\u0026#34;data\u0026#34;][\u0026#34;children\u0026#34;]: stories.append( ( story[\u0026#34;data\u0026#34;][\u0026#34;title\u0026#34;], story[\u0026#34;data\u0026#34;][\u0026#34;url\u0026#34;], story[\u0026#34;data\u0026#34;][\u0026#34;score\u0026#34;], ) ) return stories stories = get_links(\u0026#34;worldnews\u0026#34;, token) base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \u0026#34;raw\u0026#34;) # è¿™é‡Œæˆ‘å°†æ‰€æœ‰çš„é“¾æ¥éƒ½å­˜åœ¨äº†æ–‡ä»¶é‡Œï¼Œå› ä¸ºè·å–è¿™äº›ç½‘ç«™çš„å†…å®¹è¦å¾ˆä¹… with open(os.path.join(base_folder, \u0026#34;stories2.txt\u0026#34;), \u0026#34;w\u0026#34;) as f: for link in stories: f.write(json.dumps(list(link))) f.write(\u0026#34;\\n\u0026#34;) ä»ç½‘ç«™æŠ½å–æ–‡æœ¬ api/top æ€»å…±æœ‰ 500 ä¸ªç½‘ç«™ï¼Œæˆ‘åˆè·å–äº† api/new çš„ 490 ä¸ªï¼Œæ€»å…±ä¸‹è½½äº†åŠä¸ªå°æ—¶ï¼Œå¤±è´¥äº† 300ã€‚ã€‚ã€‚\næœ€åæˆåŠŸä¸‹è½½çš„ç½‘ç«™æ•°ä¸º 365\n# get_data.py # -*- coding: utf-8 -*- import hashlib import os import requests import json proxies = {\u0026#34;http\u0026#34;: \u0026#34;socks5://xxxxxxxxxxxx\u0026#34;, \u0026#34;https\u0026#34;: \u0026#34;socks5://xxxxxxxxxxxxx\u0026#34;} if __name__ == \u0026#34;__main__\u0026#34;: base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \u0026#34;raw\u0026#34;) # è¯»å–é“¾æ¥æ•°æ® with open(os.path.join(base_folder, \u0026#34;stories1.txt\u0026#34;), \u0026#34;r\u0026#34;) as f: temp = f.readlines() stories = [] for l in temp: stories.append(json.loads(l)) # è·å–ç½‘é¡µå†…å®¹ number_errors = 0 for title, url, score in stories: print(url) output_filename = hashlib.md5(url.encode()).hexdigest() fullpath = os.path.join(data_folder, output_filename + \u0026#34;.txt\u0026#34;) try: response = requests.get(url, proxies=proxies, timeout=10) data = response.text with open(fullpath, \u0026#34;w\u0026#34;) as outf: outf.write(data) except Exception as e: number_errors += 1 # è¾“å‡ºå‡ºé”™æ•°é‡ print(\u0026#34;å‡ºé”™ï¼š{}\u0026#34;.format(number_errors))  ä¸‹è½½ä¸‹æ¥çš„ç½‘é¡µå…¨æ˜¯ html æ–‡ä»¶ï¼Œè¦ä»ä¸­æå–å‡ºæœ‰ç”¨çš„ä¿¡æ¯ï¼Œè¿™é‡Œä½¿ç”¨è¾ƒä¸ºé€šç”¨çš„ lxml åº“ï¼Œå…¶å®ƒå¤„ç† html çš„åº“è¿˜æœ‰ BeautifulSoup ç­‰ã€‚\n# get_content.py # -*- coding: utf-8 -*- import os from lxml import html, etree if __name__ == \u0026#34;__main__\u0026#34;: base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \u0026#34;raw\u0026#34;) # è¾“å‡ºå˜æˆçº¯æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„ text_output_folder = os.path.join(base_folder, \u0026#34;textonly\u0026#34;) filenames = [ os.path.join(data_folder, filename) for filename in os.listdir(data_folder) ] # å­˜æ”¾ä¸å¯èƒ½åŒ…å«æ–°é—»å†…å®¹çš„èŠ‚ç‚¹ skip_node_types = [\u0026#34;script\u0026#34;, \u0026#34;head\u0026#34;, \u0026#34;style\u0026#34;, etree.Comment] # æŠŠhtmlæ–‡ä»¶è§£ææˆlxmlå¯¹è±¡ def get_text_from_file(filename): with open(filename, \u0026#34;r\u0026#34;) as inf: html_tree = html.parse(inf) return get_text_from_node(html_tree.getroot()) # æŠ½å–å­èŠ‚ç‚¹ä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œæœ€åè¿”å›æ‹¼æ¥åœ¨ä¸€èµ·çš„æ‰€æœ‰å­èŠ‚ç‚¹çš„æ–‡æœ¬ def get_text_from_node(node): if len(node) == 0: # æ²¡æœ‰å­èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›å†…å®¹ if node.text: return node.text else: return \u0026#34;\u0026#34; else: # æœ‰å­èŠ‚ç‚¹ï¼Œé€’å½’è°ƒç”¨å¾—åˆ°å†…å®¹ results = ( get_text_from_node(child) for child in node if child.tag not in skip_node_types ) result = str.join(\u0026#34;\\n\u0026#34;, (r for r in results if len(r) \u0026gt; 1)) # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ if len(result) \u0026gt;= 100: return result else: return \u0026#34;\u0026#34; for filename in os.listdir(data_folder): text = get_text_from_file(os.path.join(data_folder, filename)) with open(os.path.join(text_output_folder, filename), \u0026#34;w\u0026#34;) as outf: outf.write(text) æ–°é—»è¯­æ–™èšç±» k-means ç®—æ³•\nk-means èšç±»ç®—æ³•è¿­ä»£å¯»æ‰¾æœ€èƒ½å¤Ÿä»£è¡¨æ•°æ®çš„èšç±»è´¨å¿ƒç‚¹ã€‚ç®—æ³•å¼€å§‹æ—¶ä½¿ç”¨ä»è®­ç»ƒæ•°æ®ä¸­éšæœºé€‰å–çš„å‡ ä¸ªæ•°æ®ç‚¹ä½œä¸ºè´¨å¿ƒç‚¹ã€‚k-means ä¸­çš„ k è¡¨ç¤ºå¯»æ‰¾å¤šå°‘ä¸ªè´¨å¿ƒç‚¹ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ç®—æ³•å°†ä¼šæ‰¾åˆ°çš„ç°‡çš„æ•°é‡ã€‚æ­¥éª¤ï¼š\n ä¸ºæ¯ä¸€ä¸ªæ•°æ®ç‚¹åˆ†é…ç°‡æ ‡ç­¾\nä¸ºæ¯ä¸ªä¸ªä½“è®¾ç½®ä¸€ä¸ªæ ‡ç­¾ï¼Œå°†å®ƒå’Œæœ€è¿‘çš„è´¨å¿ƒç‚¹è”ç³»èµ·æ¥ï¼Œæ ‡ç­¾ç›¸åŒçš„ä¸ªä½“å±äºåŒä¸€ä¸ªç°‡ æ›´æ–°å„ç°‡çš„è´¨å¿ƒç‚¹  æ¯æ¬¡æ›´æ–°è´¨å¿ƒç‚¹æ—¶ï¼Œæ‰€æœ‰è´¨å¿ƒç‚¹å°†ä¼šå°èŒƒå›´ç§»åŠ¨ï¼Œè¿™ä¼šè½»å¾®æ”¹å˜æ¯ä¸ªæ•°æ®ç‚¹åœ¨ç°‡å†…çš„ä½ç½®ï¼Œä»è€Œå¼•å‘ä¸‹ä¸€æ¬¡è¿­ä»£æ—¶è´¨å¿ƒç‚¹çš„å˜åŠ¨\n# -*- coding: utf-8 -*- import os from sklearn.cluster import KMeans # TfidfVectorizerå‘é‡åŒ–å·¥å…·ï¼Œæ ¹æ®è¯è¯­å‡ºç°åœ¨å¤šå°‘ç¯‡æ–‡ç« ä¸­ï¼Œå¯¹è¯è¯­è®¡æ•°è¿›è¡ŒåŠ æƒ # å‡ºç°åœ¨è¾ƒå¤šæ–‡æ¡£ä¸­çš„è¯è¯­æƒé‡è¾ƒä½ï¼ˆç”¨æ–‡æ¡£é›†æ•°é‡é™¤ä»¥è¯è¯­å‡ºç°åœ¨çš„æ–‡æ¡£çš„æ•°é‡ï¼Œç„¶åå–å¯¹æ•°ï¼‰ from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.pipeline import Pipeline from collections import Counter from scipy.sparse import csr_matrix # ç¨€ç–çŸ©é˜µ import numpy as np from scipy.sparse.csgraph import minimum_spanning_tree # è®¡ç®—æœ€å°ç”Ÿæˆæ ‘MST from scipy.sparse.csgraph import connected_components # è¿é€šåˆ†æ”¯ from sklearn.base import BaseEstimator, ClusterMixin from sklearn.cluster import MiniBatchKMeans from sklearn.feature_extraction.text import HashingVectorizer base_folder = os.path.dirname(os.path.abspath(__file__)) data_folder = os.path.join(base_folder, \u0026#34;raw\u0026#34;) text_output_folder = os.path.join(base_folder, \u0026#34;textonly\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # åˆ†ç°‡çš„æ•°é‡ n_clusters = 10 pipeline = Pipeline( [ (\u0026#34;feature_extraction\u0026#34;, TfidfVectorizer(max_df=0.4)), # ç‰¹å¾æŠ½å–ï¼Œå¿½ç•¥å‡ºç°åœ¨40%æ–‡æ¡£ä¸­çš„è¯è¯­ï¼ˆåˆ é™¤åŠŸèƒ½è¯ï¼‰ (\u0026#34;clusterer\u0026#34;, KMeans(n_clusters=n_clusters)), # è°ƒç”¨k-meansç®—æ³• ] ) documents = [ open(os.path.join(text_output_folder, filename)).read() for filename in os.listdir(text_output_folder) ] # ä¸ä¸ºfitå‡½æ•°æŒ‡å®šç›®æ ‡ç±»åˆ«ï¼Œè¿›è¡Œè®­ç»ƒ pipeline.fit(documents) # ä½¿ç”¨è®­ç»ƒè¿‡çš„ç®—æ³•é¢„æµ‹ # labelsåŒ…å«æ¯ä¸ªæ•°æ®ç‚¹çš„ç°‡æ ‡ç­¾ï¼Œæ ‡ç­¾ç›¸åŒçš„æ•°æ®ç‚¹å±äºåŒä¸€ä¸ªç°‡ï¼Œæ ‡ç­¾æœ¬èº«æ²¡æœ‰å«ä¹‰ labels = pipeline.predict(documents) # ä½¿ç”¨Counterç±»æŸ¥çœ‹æ¯ä¸ªç°‡çš„æ•°æ®ç‚¹æ•°é‡ c = Counter(labels) for cluster_number in range(n_clusters): print( \u0026#34;Cluster {}contains {}samples\u0026#34;.format(cluster_number, c[cluster_number]) ) Output:\nCluster 0 contains 2 samples Cluster 1 contains 4 samples Cluster 2 contains 1 samples Cluster 3 contains 2 samples Cluster 4 contains 329 samples Cluster 5 contains 7 samples Cluster 6 contains 2 samples Cluster 7 contains 13 samples Cluster 8 contains 3 samples Cluster 9 contains 2 samples   èšç±»åˆ†æä¸»è¦æ˜¯æ¢ç´¢æ€§åˆ†æï¼Œå› æ­¤å¾ˆéš¾æœ‰æ•ˆåœ°è¯„ä¼°ç»“æœçš„å¥½åï¼Œå¦‚æœæœ‰æµ‹è¯•é›†ï¼Œå¯ä»¥å¯¹å…¶åˆ†ææ¥è¯„ä»·æ•ˆæœ\nå¯¹äº k-means ç®—æ³•ï¼Œå¯»æ‰¾æ–°è´¨å¿ƒç‚¹çš„æ ‡å‡†æ˜¯ï¼Œæœ€å°åŒ–æ¯ä¸ªæ•°æ®ç‚¹åˆ°æœ€è¿‘è´¨å¿ƒç‚¹çš„è·ç¦»ã€‚è¿™å«ä½œç®—æ³•çš„æƒ¯æ€§æƒé‡ï¼ˆinertiaï¼‰ï¼Œä»»ä½•ç»è¿‡è®­ç»ƒçš„ KMeans å®ä¾‹éƒ½æœ‰è¯¥å±æ€§\nä¸‹é¢å°† n_clusters ä¾æ¬¡å– 2 åˆ° 20 ä¹‹é—´çš„å€¼ï¼Œæ¯å–ä¸€ä¸ªå€¼ï¼Œk-means ç®—æ³•è¿è¡Œ 10 æ¬¡ã€‚æ¯æ¬¡è¿è¡Œç®—æ³•éƒ½è®°å½•æƒ¯æ€§æƒé‡ã€‚\nInput:\n# æƒ¯æ€§æƒé‡ï¼Œè¿™ä¸ªå€¼æ²¡æœ‰æ„ä¹‰ï¼Œä½†æ˜¯å¯ä»¥ç”¨æ¥ç¡®å®šn_clusters print(pipeline.named_steps[\u0026#34;clusterer\u0026#34;].inertia_) print() inertia_scores = [] n_clusters_values = list(range(2, 20)) for n_clusters in n_clusters_values: # å½“å‰çš„æƒ¯æ€§æƒé‡ç»„ cur_inertia_scores = [] X = TfidfVectorizer(max_df=0.4).fit_transform(documents) for i in range(10): km = KMeans(n_clusters=n_clusters).fit(X) cur_inertia_scores.append(km.inertia_) inertia_scores.append(cur_inertia_scores) print(\u0026#34;{}: {}\u0026#34;.format(n_clusters, np.mean(cur_inertia_scores))) Output:\n291.45747555507467 2 : 310.72961350285766 3 : 305.7904332223444 4 : 302.18859768191396 5 : 300.28785590112705 6 : 297.48005120447067 7 : 294.226862724111 8 : 292.340968109182 9 : 291.18707107605024 10 : 289.46981977256536 11 : 287.9333326469133 12 : 285.0561596766078 13 : 284.33745019948356 14 : 282.71178879028537 15 : 280.94991762471807 16 : 279.9555799316599 17 : 278.3825941905214 18 : 274.94616060558434 19 : 275.0297854253871   å°†ä¸Šè¡¨ä½œå›¾\nInput:\nimport plotly data = plotly.graph_objs.Scatter( x=list(range(18)), y=[ 310.73, 305.79, 302.18, 300.28, 297.48, 294.22, 292.34, 291.18, 289.46, 287.93, 285.05, 284.33, 282.71, 280.94, 279.95, 278.38, 274.94, 275.02, ], ) fig = plotly.graph_objs.Figure(data) fig.show() Output:\n 10.1 \n æ ¹æ®ä¸Šå›¾å¯ä»¥å‘ç°åœ¨ n_clusters=9 å’Œ 15 æ—¶æ‹ç‚¹æ¯”è¾ƒæ˜æ˜¾ï¼Œè¿™é‡Œä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œæˆ‘ä»¬æŒ‰ç…§ä¹¦ä¸Šé€‰æ‹© 6\nInput:\n# è®¾ç½®n_clusterså€¼ä¸º6ï¼Œ é‡æ–°è¿è¡Œç®—æ³• n_clusters = 6 pipeline = Pipeline( [ (\u0026#34;feature_extraction\u0026#34;, TfidfVectorizer(max_df=0.4)), (\u0026#34;clusterer\u0026#34;, KMeans(n_clusters=n_clusters)), ] ) pipeline.fit(documents) labels = pipeline.predict(documents) # è·å–ç‰¹å¾çš„æ‰€å¯¹åº”çš„è¯ terms = pipeline.named_steps[\u0026#34;feature_extraction\u0026#34;].get_feature_names() # ç»Ÿè®¡6ä¸ªç°‡ä¸­æ¯ä¸ªç°‡çš„å…ƒç´ ä¸ªæ•° c = Counter(labels) for cluster_number in range(n_clusters): print( \u0026#34;Cluster {}contains {}samples\u0026#34;.format(cluster_number, c[cluster_number]) ) print(\u0026#34; Most important terms\u0026#34;) centroid = pipeline.named_steps[\u0026#34;clusterer\u0026#34;].cluster_centers_[cluster_number] most_important = centroid.argsort() for i in range(5): # æ’åˆ—æ˜¯éé™åºæ’åˆ— term_index = most_important[-(i + 1)] # è¾“å‡ºåºå·ï¼Œè¯è¯­ï¼Œå¾—åˆ† print( \u0026#34; {0}{1}(score: {2:.4f})\u0026#34;.format( i + 1, terms[term_index], centroid[term_index] ) ) Output:\nCluster 0 contains 15 samples Most important terms 1 games (score: 0.2351) 2 olympic (score: 0.1921) 3 athletes (score: 0.1555) 4 ioc (score: 0.1383) 5 tokyo (score: 0.1365) Cluster 1 contains 48 samples Most important terms 1 she (score: 0.0442) 2 her (score: 0.0409) 3 masks (score: 0.0381) 4 monday (score: 0.0298) 5 23 (score: 0.0294) Cluster 2 contains 150 samples Most important terms 1 you (score: 0.0342) 2 measures (score: 0.0246) 3 would (score: 0.0246) 4 country (score: 0.0233) 5 our (score: 0.0231) Cluster 3 contains 14 samples Most important terms 1 your (score: 0.1922) 2 you (score: 0.1833) 3 robot (score: 0.1644) 4 unusual (score: 0.1505) 5 box (score: 0.1478) Cluster 4 contains 128 samples Most important terms 1 india (score: 0.0222) 2 et (score: 0.0189) 3 tablet (score: 0.0156) 4 app (score: 0.0140) 5 2020 (score: 0.0129) Cluster 5 contains 10 samples Most important terms 1 cache (score: 0.2858) 2 found (score: 0.2672) 3 server (score: 0.2484) 4 error (score: 0.2358) 5 mod_security (score: 0.1450)   ä¸Šé¢ä»£ç åœ¨æµæ°´çº¿æœ€åä¸€æ­¥çš„ k-means å®ä¾‹ä¸Šè°ƒç”¨è½¬æ¢æ–¹æ³•ã€‚å¾—åˆ°çš„çŸ©é˜µæœ‰å…­ä¸ªç‰¹å¾ï¼Œæ•°æ®é‡è·Ÿæ–‡æ¡£çš„é•¿åº¦ç›¸åŒï¼Œshape=(365,6)\nInput:\n# ç”¨K-meansç®—æ³•è½¬åŒ–ç‰¹å¾ X = pipeline.transform(documents) èšç±»èåˆ èšç±»ç®—æ³•ä¹Ÿå¯ä»¥è¿›è¡Œèåˆï¼Œè¿™æ ·åšçš„ä¸»è¦åŸå› æ˜¯ï¼Œèåˆåå¾—åˆ°çš„ç®—æ³•èƒ½å¤Ÿå¹³æ»‘ç®—æ³•å¤šæ¬¡è¿è¡Œæ‰€å¾—åˆ°çš„ä¸åŒç»“æœã€‚å¤šæ¬¡è¿è¡Œ k-means ç®—æ³•å¾—åˆ°çš„ç»“æœå› æœ€åˆé€‰æ‹©çš„è´¨å¿ƒç‚¹ä¸åŒè€Œä¸åŒã€‚å¤šæ¬¡è¿è¡Œç®—æ³•ï¼Œç»¼åˆè€ƒè™‘æ‰€å¾—åˆ°çš„å¤šä¸ªç»“æœï¼Œå¯ä»¥å‡å°‘æ³¢åŠ¨ã€‚èšç±»èåˆæ–¹æ³•è¿˜å¯ä»¥é™ä½å‚æ•°é€‰æ‹©å¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚å¤§å¤šæ•°èšç±»ç®—æ³•å¯¹å‚æ•°é€‰æ‹©å¾ˆæ•æ„Ÿ,å‚æ•°ç¨æœ‰ä¸åŒå°†å¸¦æ¥ä¸åŒçš„èšç±»ç»“æœ\næœ€åŸºæœ¬çš„èåˆæ–¹æ³•æ˜¯å¯¹æ•°æ®è¿›è¡Œå¤šæ¬¡èšç±»ï¼Œæ¯æ¬¡éƒ½è®°å½•å„ä¸ªæ•°æ®ç‚¹çš„ç°‡æ ‡ç­¾ã€‚ç„¶åè®¡ç®—æ¯ä¸¤ä¸ªæ•°æ®ç‚¹è¢«åˆ†åˆ°åŒä¸€ä¸ªç°‡çš„æ¬¡æ•°ã€‚è¿™å°±æ˜¯è¯æ®ç´¯ç§¯ç®—æ³•ï¼ˆEvidence Accumulation Clusteringï¼ŒEACï¼‰çš„ç²¾é«“\n ç¬¬ä¸€æ­¥ï¼Œä½¿ç”¨ k-means ç­‰ä½æ°´å¹³çš„èšç±»ç®—æ³•å¯¹æ•°æ®é›†è¿›è¡Œå¤šæ¬¡èšç±»ï¼Œè®°å½•æ¯ä¸€æ¬¡è¿­ä»£ä¸¤ä¸ªæ•°æ®ç‚¹å‡ºç°åœ¨åŒä¸€ç°‡çš„é¢‘ç‡ï¼Œå°†ç»“æœä¿å­˜åˆ°å…±åçŸ©é˜µï¼ˆcoassociationï¼‰ä¸­ ç¬¬äºŒæ­¥ï¼Œä½¿ç”¨å¦å¤–ä¸€ç§èšç±»ç®—æ³•â€”â€”åˆ†çº§èšç±»å¯¹ç¬¬ä¸€æ­¥å¾—åˆ°çš„å…±åçŸ©é˜µè¿›è¡Œèšç±»åˆ†æã€‚åˆ†çº§èšç±»ä¸€ä¸ªæ¯”è¾ƒæœ‰è¶£çš„ç‰¹æ€§æ˜¯ï¼Œå®ƒç­‰ä»·äºå¯»æ‰¾ä¸€æ£µæŠŠæ‰€æœ‰èŠ‚ç‚¹è¿æ¥åˆ°ä¸€èµ·çš„æ ‘ï¼Œå¹¶æŠŠæƒé‡ä½çš„è¾¹å»æ‰ã€‚  Input:\n# éå†æ‰€æœ‰æ ‡ç­¾ï¼Œè®°å½•å…·æœ‰ç›¸åŒæ ‡ç­¾çš„ä¸¤ä¸ªæ•°æ®ç‚¹çš„ä½ç½®ï¼Œåˆ›å»ºå…±åçŸ©é˜µ def create_coassociation_matrix(labels): rows = [] cols = [] # labelsç§ç±» unique_labels = set(labels) for label in unique_labels: # æ‰¾å‡ºlabelå€¼ç›¸åŒçš„æ•°æ®ç‚¹ indices = np.where(labels == label)[0] # è®°å½•ä»–ä»¬çš„ä½ç½®ï¼šå¦‚1ã€3ç‚¹çš„æ•°æ®å‡ä¸º1ï¼Œå³1å’Œ1ç›¸åŒï¼Œ1å’Œ3ç›¸åŒï¼Œ3å’Œ1ç›¸åŒï¼Œ3å’Œ3ç›¸åŒ # è¡Œå’Œåˆ—å‡å¢åŠ äº†4ä¸ªindices*indicesä¸ªæ•°å­— for index1 in indices: for index2 in indices: rows.append(index1) cols.append(index2) # è¿”å›ç»™å®šshapeå’Œtypeçš„å€¼å…¨ä¸º1çš„çŸ©é˜µ data = np.ones((len(rows),)) # åˆ›å»ºç¨€ç–çŸ©é˜µæ»¡è¶³ï¼ša[rows[k], cols[k]] = data[k] return csr_matrix((data, (rows, cols)), dtype=\u0026#34;float\u0026#34;) # ä½¿ç”¨æ ‡ç­¾ç”Ÿæˆå…±åçŸ©é˜µ C = create_coassociation_matrix(labels) # è¿™é‡Œä¹¦ä¸Šè¯´å¤šè¾“å…¥å‡ æ¬¡Cçœ‹çœ‹ç»“æœï¼Œæˆ‘æ²¡æœ‰ç”¨notebookï¼Œä½†æ˜¯ä½¿ç”¨printè¾“å‡ºæ˜¯ä¸€æ ·çš„ï¼Œå› æ­¤æ²¡æœ‰ææ‡‚ä¹¦ä¸Šçš„å«ä¹‰ print(C) print((365 ** 2 - create_coassociation_matrix(labels).nnz) / 365 ** 2) mst = minimum_spanning_tree(C) mst = minimum_spanning_tree(-C) pipeline.fit(documents) labels2 = pipeline.predict(documents) C2 = create_coassociation_matrix(labels2) C_sum = (C + C2) / 2 mst = minimum_spanning_tree(-C_sum) # åˆ é™¤ä½äºé˜ˆå€¼çš„è¾¹ mst.data[mst.data \u0026gt; -1] = 0 number_of_clusters, labels = connected_components(mst) Output:\n(0, 0)\t1.0 (0, 1)\t1.0 (0, 2)\t1.0 (0, 3)\t1.0 (0, 4)\t1.0 (0, 5)\t1.0 (0, 6)\t1.0 (0, 7)\t1.0 (0, 8)\t1.0 (0, 9)\t1.0 (0, 10)\t1.0 (0, 11)\t1.0 (0, 12)\t1.0 (0, 13)\t1.0 :\t: (364, 350)\t1.0 (364, 351)\t1.0 (364, 352)\t1.0 (364, 353)\t1.0 (364, 354)\t1.0 (364, 355)\t1.0 (364, 356)\t1.0 (364, 357)\t1.0 (364, 358)\t1.0 (364, 359)\t1.0 (364, 360)\t1.0 (364, 361)\t1.0 (364, 362)\t1.0 (364, 363)\t1.0 (364, 364)\t1.0 0.11092512666541565   ä»å›¾çš„ç†è®ºè§’åº¦çœ‹ï¼Œç”Ÿæˆæ ‘ä¸ºæ‰€æœ‰èŠ‚ç‚¹éƒ½è¿æ¥åˆ°ä¸€èµ·çš„å›¾ã€‚æœ€å°ç”Ÿæˆæ ‘ï¼ˆMinimum Spanning Treeï¼ŒMSTï¼‰å³æ€»æƒé‡æœ€ä½çš„ç”Ÿæˆæ ‘ã€‚ç»“åˆæˆ‘ä»¬çš„åº”ç”¨æ¥è®²ï¼Œå›¾ä¸­çš„èŠ‚ç‚¹å¯¹åº”æ•°æ®é›†ä¸­çš„ä¸ªä½“ï¼Œè¾¹çš„æƒé‡å¯¹åº”ä¸¤ä¸ªé¡¶ç‚¹è¢«åˆ†åˆ°åŒä¸€ç°‡çš„æ¬¡æ•°â€”â€”ä¹Ÿå°±æ˜¯å…±åçŸ©é˜µæ‰€è®°å½•çš„å€¼ã€‚\nçŸ©é˜µ C ä¸­ï¼Œå€¼è¶Šé«˜è¡¨ç¤ºä¸€ç»„æ•°æ®ç‚¹è¢«åˆ†åˆ°åŒä¸€ç°‡çš„æ¬¡æ•°è¶Šå¤šâ€”â€”è¿™ä¸ªå€¼è¡¨ç¤ºç›¸ä¼¼åº¦ã€‚ç›¸åï¼Œminimum_spanning_tree å‡½æ•°çš„è¾“å…¥ä¸ºè·ç¦»ï¼Œé«˜çš„å€¼åè€Œè¡¨ç¤ºç›¸ä¼¼åº¦è¶Šå°ã€‚è¿™é‡Œåˆç”¨åˆ°äº†ä¸€æ¬¡å–å\nInput:\nmst = minimum_spanning_tree(C) # å¯¹Cå–åå†è®¡ç®—æœ€å°ç”Ÿæˆæ ‘ mst = minimum_spanning_tree(-C) # åˆ›å»ºé¢å¤–çš„æ ‡ç­¾ pipeline.fit(documents) labels2 = pipeline.predict(documents) C2 = create_coassociation_matrix(labels2) C_sum = (C + C2) / 2 # ç”Ÿæˆé˜ˆå€¼ä¸å…¨ä¸º1å’Œ0çš„æœ€å°ç”Ÿæˆæ ‘ mst = minimum_spanning_tree(-C_sum) # åˆ é™¤ä½äºé˜ˆå€¼çš„è¾¹ mst.data[mst.data \u0026gt; -1] = 0 number_of_clusters, labels = connected_components(mst) print(number_of_clusters) print(labels) Output:\n2 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0, 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0, 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   k-means ç®—æ³•ä¸è€ƒè™‘ç‰¹å¾çš„æƒé‡ï¼Œå®ƒå¯»æ‰¾çš„æ˜¯åœ†å½¢ç°‡ï¼ˆcircular clustersï¼‰\nè¯æ®ç´¯ç§¯ç®—æ³•çš„å·¥ä½œåŸç†ä¸ºé‡æ–°æŠŠç‰¹å¾æ˜ å°„åˆ°æ–°ç©ºé—´ï¼Œæ¯æ¬¡è¿è¡Œ k-means ç®—æ³•éƒ½ç›¸å½“äºä½¿ç”¨è½¬æ¢å™¨å¯¹ç‰¹å¾è¿›è¡Œä¸€æ¬¡è½¬æ¢ã€‚\nè¯æ®ç´¯ç§¯ç®—æ³•åªå…³å¿ƒæ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»è€Œä¸æ˜¯å®ƒä»¬åœ¨åŸæ¥ç‰¹å¾ç©ºé—´çš„ä½ç½®ã€‚å¯¹äºæ²¡æœ‰è§„èŒƒåŒ–è¿‡çš„ç‰¹å¾ï¼Œä»ç„¶å­˜åœ¨é—®é¢˜ã€‚å› æ­¤ï¼Œç‰¹å¾è§„èŒƒå¾ˆé‡è¦ï¼Œæ— è®ºå¦‚ä½•éƒ½è¦åšï¼ˆæˆ‘ä»¬ç”¨ tf-idf è§„èŒƒç‰¹å¾å€¼ï¼Œä»è€Œä½¿ç‰¹å¾å…·æœ‰ç›¸åŒçš„å€¼åŸŸï¼‰\nInput:\n# åˆ›å»ºè¯æ®ç´¯ç§¯ç®—æ³•ç±» class EAC(BaseEstimator, ClusterMixin): def __init__( self, n_clusterings=10, cut_threshold=0.5, n_clusters_range=(3, 10) ): self.n_clusterings = n_clusterings # k-meansç®—æ³•è¿è¡Œæ¬¡æ•° self.cut_threshold = cut_threshold # ç”¨æ¥åˆ é™¤è¾¹çš„é˜ˆå€¼ self.n_clusters_range = n_clusters_range # æ¯æ¬¡è¿è¡Œk-meansç®—æ³•è¦æ‰¾åˆ°çš„ç°‡çš„æ•°é‡ def fit(self, X, y=None): # è¿›è¡ŒæŒ‡å®šæ¬¡æ•°çš„å…±åçŸ©é˜µç´¯åŠ  C = sum( ( create_coassociation_matrix(self._single_clustering(X)) for _ in range(self.n_clusterings) ) ) mst = minimum_spanning_tree(-C) mst.data[mst.data \u0026gt; -self.cut_threshold] = 0 self.n_components, self.labels_ = connected_components(mst) return self # è¿›è¡Œä¸€æ¬¡é›†ç¾¤ def _single_clustering(self, X): # åœ¨ç»™å®šèŒƒå›´ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªé›†ç¾¤æ•° n_clusters = np.random.randint(*self.n_clusters_range) km = KMeans(n_clusters=n_clusters) # è¿”å›ç”±k-meansè®¡ç®—å¾—åˆ°çš„ç°‡æ ‡ç­¾ return km.fit_predict(X) pipeline = Pipeline( [(\u0026#34;feature_extraction\u0026#34;, TfidfVectorizer(max_df=0.4)), (\u0026#34;clusterer\u0026#34;, EAC())] ) pipeline.fit(documents) number_of_clusters, labels = ( pipeline[\u0026#34;clusterer\u0026#34;].n_components, pipeline[\u0026#34;clusterer\u0026#34;].labels_, ) print(number_of_clusters) print(labels) Output:\n1 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  æ€»æ„Ÿè§‰æœ‰ä»€ä¹ˆé—®é¢˜ã€‚ã€‚ã€‚ (ï¿£Îµ(#ï¿£)â˜†â•°â•®o(ï¿£çš¿ï¿£///))\nçº¿ä¸Šå­¦ä¹  çº¿ä¸Šå­¦ä¹ æ˜¯æŒ‡ç”¨æ–°æ•°æ®å¢é‡åœ°æ”¹è¿›æ¨¡å‹ã€‚æ”¯æŒçº¿ä¸Šå­¦ä¹ çš„ç®—æ³•å¯ä»¥å…ˆç”¨ä¸€æ¡æˆ–å°‘é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œéšç€æ›´å¤šæ–°æ•°æ®çš„æ·»åŠ ï¼Œæ›´æ–°æ¨¡å‹ã€‚\nçº¿ä¸Šå­¦ä¹ ä¸æµå¼å­¦ä¹ ï¼ˆstreaming-based learningï¼‰æœ‰å…³ï¼Œä½†æœ‰å‡ ä¸ªé‡è¦çš„ä¸åŒç‚¹ã€‚çº¿ä¸Šå­¦ä¹ èƒ½å¤Ÿé‡æ–°è¯„ä¼°å…ˆå‰åˆ›å»ºæ¨¡å‹æ—¶æ‰€ç”¨åˆ°çš„æ•°æ®ï¼Œè€Œå¯¹äºåè€…ï¼Œæ‰€æœ‰æ•°æ®éƒ½åªä½¿ç”¨ä¸€æ¬¡ã€‚\nscikit-learn æä¾›äº† MiniBatchKMeans ç®—æ³•ï¼Œå¯ä»¥ç”¨å®ƒæ¥å®ç°çº¿ä¸Šå­¦ä¹ åŠŸèƒ½ã€‚è¿™ä¸ªç±»å®ç°äº† partial_fit å‡½æ•°ï¼Œæ¥æ”¶ä¸€ç»„æ•°æ®ï¼Œæ›´æ–°æ¨¡å‹ã€‚è°ƒç”¨fit()å°†ä¼šåˆ é™¤ä¹‹å‰çš„è®­ç»ƒç»“æœï¼Œé‡æ–°æ ¹æ®æ–°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚\nInput:\n# ä½¿ç”¨TfIDFVectorizerä»æ•°æ®é›†ä¸­æŠ½å–ç‰¹å¾ï¼Œåˆ›å»ºçŸ©é˜µX n_clusters = 6 vec = TfidfVectorizer(max_df=0.4) X = vec.fit_transform(documents) mbkm = MiniBatchKMeans(random_state=14, n_clusters=3) batch_size = 10 # éšæœºä»XçŸ©é˜µä¸­é€‰æ‹©æ•°æ®ï¼Œæ¨¡æ‹Ÿæ¥è‡ªå¤–éƒ¨çš„æ–°æ•°æ® for iteration in range(int(X.shape[0] / batch_size)): start = batch_size * iteration end = batch_size * (iteration + 1) mbkm.partial_fit(X[start:end]) # è·å–æ•°æ®é›†èšç±»ç»“æœ labels = mbkm.predict(X) c = Counter(labels) for cluster_number in range(n_clusters): print( \u0026#34;Cluster {}contains {}samples\u0026#34;.format(cluster_number, c[cluster_number]) ) Output:\nCluster 0 contains 2 samples Cluster 1 contains 362 samples Cluster 2 contains 1 samples Cluster 3 contains 0 samples Cluster 4 contains 0 samples Cluster 5 contains 0 samples   ç”±äº TfIDFVectorizer ä¸æ˜¯åœ¨çº¿ç®—æ³•ï¼Œå› æ­¤æ— æ³•åœ¨æµæ°´çº¿ä¸­ä½¿ç”¨\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ HashingVectorizer ç±»ï¼Œå®ƒå·§å¦™åœ°ä½¿ç”¨æ•£åˆ—ç®—æ³•æå¤§åœ°é™ä½äº†è®¡ç®—è¯è¢‹æ¨¡å‹æ‰€éœ€çš„å†…å­˜å¼€é”€ï¼Œå°†æ•°æ®çš„å†…å®¹è½¬æ¢æˆæ•£åˆ—å€¼\nInput:\nclass PartialFitPipeline(Pipeline): def partial_fit(self, X, y=None): Xt = X # ç»è¿‡æœ€åä¸€æ­¥ä¹‹å‰çš„æ‰€æœ‰æ­¥è½¬æ¢ for name, transform in self.steps[:-1]: Xt = transform.transform(Xt) #ã€€è°ƒç”¨MiniBatchKMeansçš„partial_fitå‡½æ•° return self.steps[-1][1].partial_fit(Xt, y=y) pipeline = PartialFitPipeline( [ (\u0026#34;feature_extraction\u0026#34;, HashingVectorizer()), (\u0026#34;clusterer\u0026#34;, MiniBatchKMeans(random_state=14, n_clusters=3)), ] ) batch_size = 10 for iteration in range(int(len(documents) / batch_size)): start = batch_size * iteration end = batch_size * (iteration + 1) pipeline.partial_fit(documents[start:end]) labels = pipeline.predict(documents) c = Counter(labels) for cluster_number in range(n_clusters): print( \u0026#34;Cluster {}contains {}samples\u0026#34;.format(cluster_number, c[cluster_number]) ) Output:\nCluster 0 contains 4 samples Cluster 1 contains 76 samples Cluster 2 contains 285 samples Cluster 3 contains 0 samples Cluster 4 contains 0 samples Cluster 5 contains 0 samples  è¿™ä¸€ç« çš„å†…å®¹æ¯”è¾ƒå¤šï¼Œä¹Ÿå­¦äº†æŒºä¹…ï¼Œè™½ç„¶ä¸­é—´ç»“æœè·Ÿä¹¦ä¸Šçš„å·®çš„æœ‰ç‚¹å¤šã€‚ã€‚å¯èƒ½æ˜¯å› ä¸ºæœ€è¿‘æ–°å† è‚ºç‚å§(ï¿£_,ï¿£ )\nç¬¬åä¸€ç«  æœ¬ç« ä»‹ç»å¦‚ä½•ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“\næ·±åº¦ç¥ç»ç½‘ç»œ æ·±åº¦ç¥ç»ç½‘ç»œå’Œç¬¬ 8 ç« ä¸­çš„åŸºæœ¬ç¥ç»ç½‘ç»œçš„å·®åˆ«åœ¨äºè§„æ¨¡å¤§å°ã€‚è‡³å°‘åŒ…å«ä¸¤å±‚éšå«å±‚çš„ç¥ç»ç½‘ç»œè¢«ç§°ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œã€‚ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒå…¶å®å°±æ˜¯ä¸€ç³»åˆ—çŸ©é˜µè¿ç®—ï¼Œä¸¤ä¸ªç½‘ç»œä¹‹é—´è¿æ¥çš„æƒé‡å¯ä»¥ç”¨çŸ©é˜µæ¥è¡¨ç¤ºã€‚å…¶ä¸­è¡Œè¡¨ç¤ºå‰ä¸€å±‚ç¥ç»å…ƒï¼Œåˆ—è¡¨ç¤ºåä¸€å±‚ç¥ç»å…ƒï¼Œä¸€ä¸ªç¥ç»ç½‘ç»œå°±å¯ä»¥ç”¨ä¸€ç»„è¿™æ ·çš„çŸ©é˜µæ¥è¡¨ç¤ºã€‚é™¤äº†ç¥ç»å…ƒå¤–ï¼Œæ¯å±‚å¢åŠ ä¸€ä¸ªåç½®é¡¹ï¼Œå®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ç¥ç»å…ƒï¼Œæ°¸è¿œå¤„äºæ¿€æ´»çŠ¶æ€ï¼Œå¹¶ä¸”è·Ÿä¸‹ä¸€å±‚çš„æ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½æœ‰è¿æ¥ã€‚\nç¥ç»ç½‘ç»œä½¿ç”¨å·ç§¯å±‚ï¼ˆä¸€èˆ¬æ¥è¯´ï¼Œä»…å·ç§¯ç¥ç»ç½‘ç»œåŒ…å«è¯¥å±‚ï¼‰å’Œæ± åŒ–å±‚ï¼ˆpooling layerï¼‰ï¼Œæ± åŒ–å±‚æ¥æ”¶æŸä¸ªåŒºåŸŸæœ€å¤§è¾“å‡ºå€¼ï¼Œå¯ä»¥é™ä½å›¾åƒä¸­çš„å¾®å°å˜åŠ¨å¸¦æ¥çš„å™ªéŸ³ï¼Œå‡å°‘ï¼ˆdown-sampleï¼Œé™é‡‡æ ·ï¼‰ä¿¡æ¯é‡ï¼Œè¿™æ ·åç»­å„å±‚æ‰€éœ€å·¥ä½œé‡ä¹Ÿä¼šç›¸åº”å‡å°‘ã€‚\nä½¿ç”¨ Iris æ•°æ®é›†è¿›è¡Œå¯¹æ¯”å®éªŒ\nInput:\nimport numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from keras.layers import Dense from keras.models import Sequential from matplotlib import pyplot as plt iris = load_iris() X = iris.data.astype(np.float32) y_true = iris.target.astype(np.int32) # é¢„å¤„ç†æ•°æ®é›† y_onehot = OneHotEncoder().fit_transform(y_true.reshape(-1, 1)) y_onehot = y_onehot.astype(np.int64).todense() X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, random_state=14) input_layer_size, hidden_layer_size, output_layer_size = 4, 6, 3 # éšå«å±‚ hidden_layer = Dense(output_dim=hidden_layer_size, input_dim=input_layer_size, activation=\u0026#39;relu\u0026#39;) # è¾“å‡ºå±‚ output_layer = Dense(output_layer_size, activation=\u0026#39;sigmoid\u0026#39;) # åˆ›å»ºé¡ºåºæ¨¡å‹ model = Sequential(layers=[hidden_layer, output_layer]) # ä¸ºè®­ç»ƒç¥ç»ç½‘ç»œé…ç½®æ¨¡å‹ # æŸå¤±å‡½æ•°è®¾ç½®ä¸ºå‡æ–¹è¯¯å·®ï¼Œä¼˜åŒ–å™¨è®¾ç½®ä¸ºadam(äºšå½“)å³éµå¾ªåŸå§‹æ–‡ä»¶ä¸­çš„é»˜è®¤å‚æ•°ï¼ŒæŒ‡å®šç²¾åº¦è¡¡é‡æ ‡å‡† model.compile(loss=\u0026#39;mean_squared_error\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) # å½“ä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†é€šè¿‡äº†ç¥ç»ç½‘ç»œä¸€æ¬¡å¹¶ä¸”è¿”å›äº†ä¸€æ¬¡ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºä¸€æ¬¡epoch # ä¸ºæ¨¡å‹è®­ç»ƒå›ºå®šçš„epochï¼ˆæ•°æ®é›†ä¸Šçš„è¿­ä»£ï¼‰ # è¾“å‡ºæ¨¡å¼ã€‚0ä¸è¾“å‡ºï¼Œ1æ¯ä¸ªepochä¸€ä¸ªè¿›åº¦æ¡ï¼Œ2ä¸€è¡Œæ¯ä¸ªepochã€‚ history = model.fit(X_train, y_train, nb_epoch=100, verbose=2) # è®°å½•äº†è¿ç»­å‡ ä¸ªepochçš„è®­ç»ƒæŸå¤±å€¼å’Œåº¦é‡å€¼ï¼Œä»¥åŠéªŒè¯æŸå¤±å€¼å’ŒéªŒè¯åº¦é‡å€¼(å¦‚æœé€‚ç”¨çš„è¯) history.history # ä½œå›¾ï¼Œç»˜åˆ¶å‡ºepochå’Œlosså…³ç³»å›¾ plt.figure(figsize=(10, 10)) plt.plot(history.epoch, history.history[\u0026#39;loss\u0026#39;]) plt.xlabel(\u0026#34;Epoch\u0026#34;) plt.ylabel(\u0026#34;Loss\u0026#34;) plt.show() # ä¸ºè¾“å…¥æ ·æœ¬ç”Ÿæˆè¾“å‡ºé¢„æµ‹ï¼Œè®¡ç®—æ˜¯åˆ†æ‰¹è¿›è¡Œçš„ # è¿”å›çš„æ˜¯æ•°å€¼[0.9356668, 0.20588416, 0.00021186471],ä»£è¡¨æ ·æœ¬å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ y_pred = model.predict(X_test) # è¿”å›ä¸€ä¸²é¢„æµ‹ç»“æœï¼Œæ ·æœ¬å±äºå“ªä¸€ä¸ªç±»åˆ« y_pred = model.predict_classes(X_test) y_pred = model.predict_classes(X_test) print(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred)) Output:\n iris_1 \n precision recall f1-score support 0 1.00 1.00 1.00 17 1 1.00 0.08 0.14 13 2 0.40 1.00 0.57 8 accuracy 0.68 38 macro avg 0.80 0.69 0.57 38 weighted avg 0.87 0.68 0.62 38   é‡å¤ä¸Šé¢çš„æ“ä½œï¼Œè¿™æ¬¡è¿è¡Œ 1000 æ­¥ï¼Œå¯¹æ¯”å®éªŒç»“æœ\nInput:\nhidden_layer = Dense(output_dim=hidden_layer_size, input_dim=input_layer_size, activation=\u0026#39;relu\u0026#39;) output_layer = Dense(output_layer_size, activation=\u0026#39;sigmoid\u0026#39;) model = Sequential(layers=[hidden_layer, output_layer]) model.compile(loss=\u0026#39;mean_squared_error\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) history = model.fit(X_train, y_train, nb_epoch=1000, verbose=False) plt.figure(figsize=(12, 8)) plt.plot(history.epoch, history.history[\u0026#39;loss\u0026#39;]) plt.xlabel(\u0026#34;Epoch\u0026#34;) plt.ylabel(\u0026#34;Loss\u0026#34;) plt.show(\u0026#34;keras_on_iris_2.png\u0026#34;) y_pred = model.predict_classes(X_test) print(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred)) Output:\n iris_2 \n precision recall f1-score support 0 1.00 1.00 1.00 17 1 1.00 1.00 1.00 13 2 1.00 1.00 1.00 8 accuracy 1.00 38 macro avg 1.00 1.00 1.00 38 weighted avg 1.00 1.00 1.00 38  ä»ç»“æœå¯ä»¥çœ‹å‡ºï¼Œç»è¿‡ 100 æ­¥è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ­£ç¡®ç‡è¾¾åˆ°äº† 68%ï¼Œç»è¿‡ 1000 æ­¥è®­ç»ƒåæ­£ç¡®ç‡è¾¾åˆ°äº† 100%\n éªŒè¯ç è¯†åˆ«å®éªŒ\nInput:\nimport numpy as np from PIL import Image, ImageDraw, ImageFont from skimage import transform as tf from matplotlib import pyplot as plt from sklearn.utils import check_random_state from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from keras.layers import Dense from keras.models import Sequential from skimage.measure import label, regionprops def create_captcha(text, shear=0, size=(100, 30), scale=1): im = Image.new(\u0026#34;L\u0026#34;, size, \u0026#34;black\u0026#34;) draw = ImageDraw.Draw(im) font = ImageFont.truetype( \u0026#34;/home/saltfish/Programming/Python/data_mining/ch11/FiraCode-Medium.otf\u0026#34;, 22 ) draw.text((0, 0), text, fill=1, font=font) image = np.array(im) affine_tf = tf.AffineTransform(shear=shear) image = tf.warp(image, affine_tf) image = image / image.max() shape = image.shape # Apply scale shapex, shapey = (shape[0] * scale, shape[1] * scale) image = tf.resize(image, (shapex, shapey)) return image image = create_captcha(\u0026#34;FISH\u0026#34;, shear=0.5, scale=0.6) plt.imshow(image, cmap=\u0026#34;Greys\u0026#34;) Output:\n captcha_1 \n def segment_image(image): # æ ‡è®°æ‰¾åˆ°è¿é€šçš„éé»‘è‰²åƒç´ çš„å­å›¾åƒ labeled_image = label(image \u0026gt; 0.2, connectivity=1, background=0) subimages = [] # æ‹†åˆ†å­å›¾ for region in regionprops(labeled_image): # æå–å­å›¾ start_x, start_y, end_x, end_y = region.bbox subimages.append(image[start_x:end_x, start_y:end_y]) if len(subimages) == 0: # æœªæ‰¾åˆ°å­å›¾ï¼Œè¿”å›è¿™ä¸ªå›¾ç‰‡æœ¬èº« return [ image, ] return subimages subimages = segment_image(image) # é€‰å‡ºå››å¼ å°å›¾ç‰‡ f, axes = plt.subplots(1, len(subimages), figsize=(10, 3)) for i in range(len(subimages)): axes[i].imshow(subimages[i], cmap=\u0026#34;gray\u0026#34;) plt.show() Output:\n captcha_2 \n random_state = check_random_state(14) letters = list(\u0026#34;ABCDEFGHIJKLMNOPQRSTUVWXYZ\u0026#34;) assert len(letters) == 26 shear_values = np.arange(0, 0.8, 0.05) scale_values = np.arange(0.9, 1.1, 0.1) # éšæœºç”Ÿæˆä¸€ä¸ªå­—æ¯çš„å›¾ç‰‡ def generate_sample(random_state=None): random_state = check_random_state(random_state) letter = random_state.choice(letters) shear = random_state.choice(shear_values) scale = random_state.choice(scale_values) return ( create_captcha(letter, shear=shear, size=(30, 30), scale=scale), letters.index(letter), ) image, target = generate_sample(random_state) plt.imshow(image, cmap=\u0026#34;Greys\u0026#34;) print(\u0026#34;The target for this image is: {0}\u0026#34;.format(letters[target])) Output:\nThe target for this image is: L   captcha_3 \n Input:\n# ç”Ÿæˆæ•°æ®é›† dataset, targets = zip(*(generate_sample(random_state) for i in range(1000))) dataset = np.array( [tf.resize(segment_image(sample)[0], (20, 20)) for sample in dataset] ) dataset = np.array(dataset, dtype=\u0026#34;float\u0026#34;) targets = np.array(targets) onehot = OneHotEncoder() y = onehot.fit_transform(targets.reshape(targets.shape[0], 1)) y = y.todense() X = dataset.reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2])) X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9) hidden_layer = Dense(100, input_dim=X_train.shape[1]) output_layer = Dense(y_train.shape[1]) model = Sequential(layers=[hidden_layer, output_layer]) model.compile(loss=\u0026#34;mean_squared_error\u0026#34;, optimizer=\u0026#34;adam\u0026#34;, metrics=[\u0026#34;accuracy\u0026#34;]) model.fit(X_train, y_train, epochs=1000, verbose=False) y_pred = model.predict(X_test) print(f1_score(y_pred=y_pred.argmax(axis=1), y_true=y_test.argmax(axis=1), average=\u0026#34;macro\u0026#34;)) print(classification_report(y_pred=y_pred.argmax(axis=1), y_true=y_test.argmax(axis=1))) Output:\n1.0 precision recall f1-score support 0 1.00 1.00 1.00 4 1 1.00 1.00 1.00 4 2 1.00 1.00 1.00 3 3 1.00 1.00 1.00 10 4 1.00 1.00 1.00 3 5 1.00 1.00 1.00 3 6 1.00 1.00 1.00 1 7 1.00 1.00 1.00 3 8 1.00 1.00 1.00 5 9 1.00 1.00 1.00 3 10 1.00 1.00 1.00 3 11 1.00 1.00 1.00 6 12 1.00 1.00 1.00 3 13 1.00 1.00 1.00 5 14 1.00 1.00 1.00 4 15 1.00 1.00 1.00 6 16 1.00 1.00 1.00 1 17 1.00 1.00 1.00 3 18 1.00 1.00 1.00 2 19 1.00 1.00 1.00 3 20 1.00 1.00 1.00 5 21 1.00 1.00 1.00 7 22 1.00 1.00 1.00 4 23 1.00 1.00 1.00 2 24 1.00 1.00 1.00 2 25 1.00 1.00 1.00 5 accuracy 1.00 100 macro avg 1.00 1.00 1.00 100 weighted avg 1.00 1.00 1.00 100  ä½¿ç”¨ GPU ä¼˜åŒ– ä¸ºäº†è®©æˆ‘çš„ GPU èƒ½è·‘ç¨‹åºï¼Œå¯è´¹äº†æˆ‘å¥½å¤§åŠŸå¤«ï¼Œç»“æœæˆ‘è¿™ 960M çš„ 2G å†…å­˜è¿˜è·‘ä¸äº†å¤ªå¤§çš„ç¨‹åº/(ã„’ o ã„’)/~~\nç¬¬ 101 æ¬¡æƒ³å¿µæˆ‘çš„å°å¼æœºï¼Œå¯æ¶çš„ç—…æ¯’\né…ç½®çš„è¿‡ç¨‹è·Ÿ TensorFlow å®˜ç½‘ç»™çš„æ–¹æ³•æ²¡å•¥åŒºåˆ«ï¼Œåœ¨è¿™å°±ä¸å¤šè¯´äº†ï¼ˆå®˜ç½‘ç»™å‡ºçš„ NVIDIA æ˜¾å¡é©±åŠ¨ç‰ˆæœ¬æ˜¯ 430ï¼Œæˆ‘è¿™é‡Œæ˜¯ 440ï¼ŒCUDA ç‰ˆæœ¬æ˜¯ 10.2ï¼Œä¾ç„¶èƒ½è¿è¡Œç¨‹åºï¼Œå¯èƒ½åªéœ€è¦ development and runtime libraries æ­£ç¡®å®‰è£…å°±è¡Œï¼Ÿï¼‰\nä½¿ç”¨ tensorflow åœ¨æ‰§è¡Œ modle.compile() çš„æ—¶å€™éœ€è¦è¾ƒé•¿çš„æ—¶é—´ï¼Œè¿è¡Œæ—¶çš„é€Ÿåº¦è¿˜æ˜¯å¾ˆå¿«çš„\nåˆæ¬¡æ¥è§¦ç¥ç»ç½‘ç»œï¼Œä¸äº†è§£çš„ä¸œè¥¿å¤ªå¤šäº†ï¼Œè¿˜æ˜¯å…ˆå¤šåšå‡ ä¸ªè®­ç»ƒå†è¯´å§ã€‚ã€‚\nåº”ç”¨ ä¹¦ä¸Šä½¿ç”¨ CIFAR å›¾åƒæ•°æ®é›†çš„ä»£ç å¤ªè€äº†ï¼ˆåŸè°…æˆ‘å¤ªèœäº†è§£å†³ä¸äº†ä¾èµ–é—®é¢˜ï¼‰ï¼Œå› æ­¤æˆ‘è·Ÿç€ Tensorflow å®˜ç½‘çš„ä»£ç åšå®Œäº†è¿™ä¸ªå®éªŒ\næœè£…è¯†åˆ«\nInput:\n# -*- coding: utf-8 -*- from __future__ import absolute_import, division, print_function, unicode_literals import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt if __name__ == \u0026#34;__main__\u0026#34;: # --------åŠ è½½ã€äº†è§£ã€é¢„å¤„ç†æ•°æ®é›†-------- fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = [ \u0026#34;T-shirt/top\u0026#34;, \u0026#34;Trouser\u0026#34;, \u0026#34;Pullover\u0026#34;, \u0026#34;Dress\u0026#34;, \u0026#34;Coat\u0026#34;, \u0026#34;Sandal\u0026#34;, \u0026#34;Shirt\u0026#34;, \u0026#34;Sneaker\u0026#34;, \u0026#34;Bag\u0026#34;, \u0026#34;Ankle boot\u0026#34;, ] # æŸ¥çœ‹æ•°æ®é›† print( train_images.shape, # (60000ï¼Œ28ï¼Œ28) len(train_labels), # 60000 train_labels, # [9 0 0 ... 3 0 5] test_images.shape, # (10000, 28, 28) len(test_labels), # 10000 ) Output:\n(60000, 28, 28) 60000 [9 0 0 ... 3 0 5] (10000, 28, 28) 10000   Input:\n# æŸ¥çœ‹å›¾åƒ plt.figure() plt.imshow(train_images[0]) plt.colorbar() plt.grid(False) plt.show() # é¢„å¤„ç†æ ‡å‡†åŒ– train_images = train_images / 255.0 test_images = test_images / 255.0 # æŸ¥çœ‹æ•°æ®é›† plt.figure(figsize=(10, 10)) for i in range(25): plt.subplot(5, 5, i + 1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]]) plt.show() Output:\n Input:\n# --------å»ºç«‹æ¨¡å‹-------- # å»ºç«‹ç¥ç»ç½‘ç»œæ‰€éœ€è¦æ¨¡å‹çš„å„å±‚ # tf.keras.layers.Flattenå°†å›¾åƒçš„æ ¼å¼ä»äºŒç»´æ•°ç»„(28 * 28)è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„(28 * 28 = 784) # å¯ä»¥å°†è¿™ä¸ªå›¾å±‚çœ‹ä½œæ˜¯å›¾åƒä¸­å–æ¶ˆå †å çš„åƒç´ è¡Œï¼Œå¹¶å°†å®ƒä»¬æ’åˆ—èµ·æ¥ # è¿™ä¸ªå±‚æ²¡æœ‰å‚æ•°éœ€è¦å­¦ä¹ ; å®ƒåªæ˜¯é‡æ–°æ ¼å¼åŒ–æ•°æ®ã€‚ # # ç„¶åæ˜¯ä¸¤ä¸ªç¨ å¯†å±‚ï¼ˆå®Œå…¨è¿æ¥çš„å±‚ï¼‰ï¼Œä¸­é—´ä¸€å±‚æœ‰128ä¸ªèŠ‚ç‚¹ï¼Œ # æœ€åä¸€å±‚è¿”å›é•¿åº¦ä¸º10çš„å¯¹æ•°æ•°ç»„ã€‚æ¯ä¸ªç¥ç»å…ƒåŒ…å«ä¸€ä¸ªå¾—åˆ†ï¼ŒæŒ‡ç¤ºå½“å‰å›¾åƒå¯¹è¿™ä¸€ç±»çš„è¯„åˆ† model = keras.Sequential( [ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=\u0026#34;relu\u0026#34;), keras.layers.Dense(10), ] ) # --------ç¼–è¯‘æ¨¡å‹-------- # æŸå¤±å‡½æ•°ï¼šè¿™å¯ä»¥è¡¡é‡è®­ç»ƒæœŸé—´æ¨¡å‹çš„å‡†ç¡®ç¨‹åº¦ï¼Œå¸Œæœ›æœ€å°åŒ–è¿™ä¸ªå‡½æ•°ï¼Œä»¥ä¾¿å°†æ¨¡å‹â€œå¼•å¯¼â€åˆ°æ­£ç¡®çš„æ–¹å‘ # ä¼˜åŒ–å™¨ï¼šå¦‚ä½•åŸºäºå®ƒçœ‹åˆ°çš„æ•°æ®å’Œå®ƒçš„æŸå¤±å‡½æ•°æ›´æ–°æ¨¡å‹ # æŒ‡æ ‡ï¼šç”¨äºæ£€æµ‹è®­ç»ƒå’Œæµ‹è¯•æ­¥éª¤ã€‚ä¸‹é¢çš„ä¾‹å­ä½¿ç”¨ç²¾ç¡®åº¦ï¼Œå³æ­£ç¡®åˆ†ç±»çš„å›¾åƒçš„åˆ†æ•° model.compile( optimizer=\u0026#34;adam\u0026#34;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\u0026#34;accuracy\u0026#34;], ) # --------è®­ç»ƒæ¨¡å‹-------- model.fit(train_images, train_labels, epochs=10) # --------è¯„ä¼°è¡¨ç°-------- test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print(\u0026#34;\\nTest accuracy:\u0026#34;, test_acc) probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()]) # predictionæ˜¯ç”±10ä¸ªæ•°å­—ç»„æˆçš„æ•°ç»„ã€‚å®ƒä»¬è¡¨ç¤ºæ¨¡å‹å¯¹å›¾åƒå¯¹åº”äº10ç§ä¸åŒè¡£æœå„è‡ªçš„ç½®ä¿¡åº¦ predictions = probability_model.predict(test_images) print(predictions[0]) Output:\npciBusID: 0000:02:00.0 name: GeForce GTX 960M computeCapability: 5.0 coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s Epoch 1/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4919 - accuracy: 0.8271 Epoch 2/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.3758 - accuracy: 0.8648 Epoch 3/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3346 - accuracy: 0.8770 Epoch 4/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3099 - accuracy: 0.8860 Epoch 5/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2927 - accuracy: 0.8927 Epoch 6/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2807 - accuracy: 0.8962 Epoch 7/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2655 - accuracy: 0.9010 Epoch 8/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2548 - accuracy: 0.9044 Epoch 9/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2440 - accuracy: 0.9095 Epoch 10/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.2373 - accuracy: 0.9113 313/313 - 0s - loss: 0.3479 - accuracy: 0.8798 Test accuracy: 0.879800021648407 [1.3496768e-07 1.5826453e-10 1.7375668e-09 2.1999605e-10 5.5648923e-07 1.9829762e-03 1.9957926e-07 1.8424643e-04 9.3086570e-09 9.9783188e-01]  ä»è¾“å‡ºå¯ä»¥çœ‹å‡º loss å‡½æ•°æ­£åœ¨é€æ¸å‡å°ï¼Œè®­ç»ƒçš„å‡†ç¡®ç‡åœ¨ä¸æ–­çš„å¢åŠ ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æ‰€è¦çš„\nåœ¨è®­ç»ƒé›†ä¸­çš„å‡†ç¡®ç‡ä¸º 91.1%ï¼Œ è€Œåœ¨æµ‹è¯•é›†ä¸­åªæœ‰ 88%ï¼Œè¿™æ˜¯å‡ºç°äº†è¿‡æ‹Ÿåˆ(overfitting)ï¼Œå…³äºè¿‡æ‹Ÿåˆçš„è¯æ˜å’Œé¿å…è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼Œç­‰è¿‡å‡ å¤©å•ç‹¬å†™ä¸€ä¸ª post å­¦ä¹ ä¸€ä¸‹\n å®šä¹‰ä¸¤ä¸ªå‡½æ•°ç”¨æ¥ç»˜å›¾ï¼Œæ›´ç›´è§‚åœ°çœ‹å‡ºé¢„æµ‹ç»“æœ\nInput:\n# --------éªŒè¯æ¨¡å‹-------- # åˆ¶ä½œå›¾è¡¨æ¥è§‚å¯Ÿåä¸ªç±»åˆ«é¢„æµ‹çš„å®Œæ•´é›†åˆ def plot_image(i, predictions_array, true_label, img): predictions_array, true_label, img = predictions_array, true_label[i], img[i] plt.grid(False) plt.xticks([]) plt.yticks([]) plt.imshow(img, cmap=plt.cm.binary) predicted_label = np.argmax(predictions_array) if predicted_label == true_label: color = \u0026#34;blue\u0026#34; else: color = \u0026#34;red\u0026#34; # ç½®ä¿¡åº¦ç™¾åˆ†æ¯” plt.xlabel( \u0026#34;{}{:2.0f}% ({})\u0026#34;.format( class_names[predicted_label], 100 * np.max(predictions_array), class_names[true_label], ), color=color, ) # ç»˜åˆ¶ç½®ä¿¡åº¦æŸ±çŠ¶å›¾ def plot_value_array(i, predictions_array, true_label): predictions_array, true_label = predictions_array, true_label[i] plt.grid(False) plt.xticks(range(10)) plt.yticks([]) thisplot = plt.bar(range(10), predictions_array, color=\u0026#34;#777777\u0026#34;) plt.ylim([0, 1]) predicted_label = np.argmax(predictions_array) # é”™è¯¯çš„é¢„æµ‹æ ‡ç­¾ä¸ºçº¢è‰² thisplot[predicted_label].set_color(\u0026#34;red\u0026#34;) # æ­£ç¡®çš„æ ‡ç­¾ä¸ºè“è‰² thisplot[true_label].set_color(\u0026#34;blue\u0026#34;) i = 0 plt.figure(figsize=(6, 3)) plt.subplot(1, 2, 1) plot_image(i, predictions[i], test_labels, test_images) plt.subplot(1, 2, 2) plot_value_array(i, predictions[i], test_labels) plt.show() i = 12 plt.figure(figsize=(6, 3)) plt.subplot(1, 2, 1) plot_image(i, predictions[i], test_labels, test_images) plt.subplot(1, 2, 2) plot_value_array(i, predictions[i], test_labels) plt.show() num_rows = 5 num_cols = 3 num_images = num_rows * num_cols plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows)) for i in range(num_images): plt.subplot(num_rows, 2 * num_cols, 2 * i + 1) plot_image(i, predictions[i], test_labels, test_images) plt.subplot(num_rows, 2 * num_cols, 2 * i + 2) plot_value_array(i, predictions[i], test_labels) plt.tight_layout() plt.show() Output:\n ç¤ºèŒƒå¦‚ä½•ä½¿ç”¨æ¨¡å‹æ¥å¾—åˆ°é¢„æµ‹ç»“æœ\nInput:\n# --------ä½¿ç”¨æ¨¡å‹-------- img = test_images[1] print(img.shape) # è½¬æ¢æˆkerasæ”¯æŒçš„æ ¼å¼ img = np.expand_dims(img, 0) print(img.shape) # ä¸ºè¯¥å›¾åƒé¢„æµ‹ predictions_single = probability_model.predict(img) print(predictions_single) plot_value_array(1, predictions_single[0], test_labels) _ = plt.xticks(range(10), class_names, rotation=45) plt.show() # è¿”å›é¢„æµ‹çš„ç§ç±» print(\u0026#34;result: \u0026#34;, np.argmax(predictions_single[0])) Output:\n(28, 28) (1, 28, 28) [[3.3822223e-05 3.9569712e-13 9.9579656e-01 1.2699689e-10 3.9967773e-03 1.0960948e-12 1.7281482e-04 5.0896191e-17 7.9589724e-11 1.4832706e-12]] result: 2 # è¿™å¼ å›¾ç‰‡å¿˜äº†ä¿å­˜äº†   è¯•ç€å¢åŠ æ­¥æ•°è§‚å¯Ÿæ˜¯å¦èƒ½å¾—åˆ°æ›´å¥½çš„ç»“æœ\nInput:\n# ä»…ä¿®æ”¹è¿™ä¸€è¡Œä»£ç ï¼Œå…¶å®ƒä¸å˜ï¼Œé‡æ–°è¿è¡Œ model.fit(train_images, train_labels, epochs=100, verbose=0) Output:\n313/313 - 0s - loss: 0.8228 - accuracy: 0.8824 Test accuracy: 0.8823999762535095 [3.4202448e-25 0.0000000e+00 1.0021874e-24 0.0000000e+00 5.4433387e-31 1.8526166e-17 5.1352536e-34 4.5777732e-13 1.4344803e-28 1.0000000e+00] (28, 28) (1, 28, 28) [[2.1121348e-16 0.0000000e+00 1.0000000e+00 1.0520916e-32 5.9910987e-09 1.4628578e-34 1.1571613e-14 0.0000000e+00 6.4996830e-38 0.0000000e+00]] result: 2  åœ¨å¢åŠ åˆ° 100 æ­¥åï¼Œæœ€åä¸€æ­¥çš„è¾“å‡ºä¸º\nEpoch 100/100 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0570 - accuracy: 0.9790  åœ¨è®­ç»ƒé›†ä¸Šçš„ç²¾ç¡®åº¦è¾¾åˆ°äº† 97.9%ï¼Œè€Œåœ¨æµ‹è¯•é›†ä¸­ä¹Ÿåªè¾¾åˆ°äº† 88.2%ï¼Œæœ‰å¾®å°çš„è¿›æ­¥\nè¿™ä¸ªå®éªŒæ¡ç†æ¸…æ™°åœ°å±•ç¤ºäº†æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ­¥éª¤ï¼š\n åŠ è½½ã€äº†è§£ã€é¢„å¤„ç†æ•°æ®é›† å»ºç«‹æ¨¡å‹ å»ºç«‹æ¨¡å‹ è®­ç»ƒæ¨¡å‹ è¯„ä¼°è¡¨ç° éªŒè¯æ¨¡å‹ ä½¿ç”¨æ¨¡å‹åšé¢„æµ‹   è¿™æœ¬ä¹¦å°±å¿«çœ‹å®Œäº†ï¼Œæ­£æ„ä¸çŸ¥é“ä¸‹æœ¬ä¹¦çœ‹å•¥çš„æˆ‘åˆå‘ç°äº†ä¸€ä¸ªå­¦ä¹ å®åº“TensorFlowã€‚å°±å†³å®šæ˜¯ä½ äº†ï¼\nç¬”è®°æœ¬å¥½éš¾ç”¨ï¼Œè¿˜æ˜¯ pycharm é€‚åˆæˆ‘ã€‚ã€‚ã€‚ä½†è¿˜æ˜¯å¾—å­¦ç”¨ç¬”è®°æœ¬å•Šï½ï½\nç¬¬åäºŒç«  æœ¬ç« ä¸»è¦ä»‹ç»äº† python ä½¿ç”¨ MapReduce æ¥è¿›è¡Œå¤§æ•°æ®å¤„ç†\nMapReduce ä¾‹å­ MapReduce ä¸»è¦åˆ†ä¸ºä¸¤æ­¥ï¼šæ˜ å°„ï¼ˆMapï¼‰å’Œè§„çº¦ï¼ˆReduceï¼‰\nInput:\n# -*- coding: utf-8 -*- from collections import defaultdict from sklearn.datasets import fetch_20newsgroups from joblib import Parallel, delayed import timeit # è®¡ç®—documentsä¸­çš„å•è¯å‡ºç°è¯ç´  def map_word_count(document_id, document): counts = defaultdict(int) for word in document.split(): counts[word] += 1 for word in counts: yield word, counts[word] # å°†mapå¾—åˆ°çš„ç»“æœï¼Œå³æ¯ç¯‡æ–‡ç« ä¸­å•è¯å‡ºç°çš„æ¬¡æ•°æ•´åˆèµ·æ¥ # å¦‚æ–‡ç« 1ä¸­å•è¯\u0026#34;apple\u0026#34;å‡ºç°äº†2æ¬¡ï¼Œæ–‡ç« 2ä¸­å•è¯\u0026#34;apple\u0026#34;å‡ºç°äº†5æ¬¡ï¼Œåˆ™è¿”å›ç»“æœä¸º[\u0026#34;apple\u0026#34;:[2,5],...] def shuffle_words(results_generators): records = defaultdict(list) # éå†æ¯ä¸€ç¯‡æ–‡ç«  for results in results_generators: # éå†æ¯ä¸ªå•è¯ for word, count in results: records[word].append(count) # æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªå•è¯ for word in records: yield word, records[word] # å°†å•è¯æ‰€å¯¹åº”çš„åˆ—è¡¨å åŠ èµ·æ¥å¾—åˆ°å•è¯å‡ºç°æ¬¡æ•° def reduce_counts(word, list_of_counts): return word, sum(list_of_counts) if __name__ == \u0026#34;__main__\u0026#34;: dataset = fetch_20newsgroups(subset=\u0026#34;train\u0026#34;) documents = dataset.data start = timeit.default_timer() # ç”Ÿæˆå™¨ï¼Œè¾“å‡º(å•è¯ï¼Œå‡ºç°æ¬¡æ•°çš„é”®å€¼å¯¹) map_results = map(map_word_count, range(len(documents)), documents) shuffle_results = shuffle_words(map_results) reduce_results = [ reduce_counts(word, list_of_counts) for word, list_of_counts in shuffle_results ] end = timeit.default_timer() print(reduce_results[:5]) print(len(reduce_results)) print(\u0026#34;----------\u0026#34;, str(end - start)) Output:\npydev debugger: process 7540 is connecting [('From:', 11536), ('lerxst@wam.umd.edu', 2), (\u0026quot;(where's\u0026quot;, 3), ('my', 7679), ('thing)', 9)] 280308 ---------- 4.087287616999674   æ¥ä¸‹æ¥å¯¼å…¥ joblib åº“ï¼Œå°† map å·¥ä½œåˆ†é…å‡ºå»ï¼Œä½¿ç”¨ 4 ä¸ªè¿›ç¨‹è¿›è¡Œè®¡ç®—\nInput:\ndef map_word_count(document_id, document): counts = defaultdict(int) for word in document.split(): counts[word] += 1 return list(counts.items()) start = timeit.default_timer() map_results = Parallel(n_jobs=4)( delayed(map_word_count)(i, document) for i, document in enumerate(documents) ) shuffle_results = shuffle_words(map_results) reduce_results = [ reduce_counts(word, list_of_counts) for word, list_of_counts in shuffle_results ] end = timeit.default_timer() print(reduce_results[:5]) print(len(reduce_results)) print(\u0026#34;----------\u0026#34;, str(end - start)) Output:\npydev debugger: process 7566 is connecting pydev debugger: process 7556 is connecting pydev debugger: process 7552 is connecting pydev debugger: process 7561 is connecting [('From:', 11536), ('lerxst@wam.umd.edu', 2), (\u0026quot;(where's\u0026quot;, 3), ('my', 7679), ('thing)', 9)] 280308 ---------- 3.5958340090001  å¯ä»¥çœ‹åˆ°è¿è¡Œæ—¶é—´ç¡®å®å‡å°‘äº†ï¼ˆæ•°æ®é›†å¤ªå°‘äº†æ•ˆæœä¸æ€ä¹ˆæ ·ï¼‰\nMapReduce åº”ç”¨ ä¹¦ä¸­ä½¿ç”¨ blogs çš„æ•°æ®é›†ï¼Œæœ‰ 19320 ä¸ªäººçš„ blog ä¿¡æ¯\næ‰‹å¤´ä¸Šçš„ç”µè„‘é…ç½®æœ‰ç‚¹ä¸è¡Œäº†ï¼Œè·‘çš„å±å®è´¹åŠ²ï¼Œå°±æ”¾åœ¨è¿™äº†ï¼ˆå…¶å®æ˜¯è¿«ä¸åŠå¾…æƒ³å»åšåš tensorflow çš„ç»ƒä¹ äº†å˜¿å˜¿ï¼‰\næ¥ä¸‹æ¥çš„æ–¹å‘ ä¹¦ä¸­æ ¹æ®æ¯ä¸€ç« çš„å†…å®¹ï¼Œéƒ½æœ‰æ›´è¿›ä¸€æ­¥çš„å®è·µï¼Œæˆ‘ä¼šé€‰å‡ ä¸ªå•ç‹¬åšä¸€ä¸‹ç»ƒä¹ \nDone\nå‚è€ƒé“¾æ¥  python-3.8.2-doc pandas-doc numpy-doc scikit-learn tensorflow  ","date":"2020-03-09T00:00:00Z","image":"https://saltfishpr.github.io/p/data-mining/cover_hu070ea85b747ec3ac952c2c1e3418996b_251504_120x120_fill_q75_box_smart1.jpg","permalink":"https://saltfishpr.github.io/p/data-mining/","title":"Data Mining"},{"content":"ç¼–ç  â€ƒç¼–ç æ˜¯æ•°æ®ä»ä¸€ç§æ ¼å¼å˜ä¸ºå¦ä¸€ç§æ ¼å¼çš„è¿‡ç¨‹ã€‚é€šè¿‡ç¼–ç ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ•°æ®ä»¥ä¸åŒçš„æ ¼å¼ä¿å­˜å’Œè½¬ç§»ã€‚\nPython3 å­—ç¬¦ä¸²ç±»å‹ str(Unicode) é€šè¿‡ encode ç¼–ç åå½¢æˆ byte(äºŒè¿›åˆ¶æ•°æ®)ã€‚ äºŒè¿›åˆ¶æ•°æ®é€šè¿‡decodeè½¬æ¢æˆ Unicode å­—ç¬¦ã€‚\nç¤ºä¾‹ 1ï¼š\ns1 = \u0026#34;ä¸­æ–‡\u0026#34; s2 = s1.encode(\u0026#34;utf-8\u0026#34;) print(type(s1), s1) print(type(s2), s2) output 1:\n\u0026lt;class 'str'\u0026gt; ä¸­æ–‡ \u0026lt;class 'bytes'\u0026gt; b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' â€ƒæ–‡ä»¶ä¸­å­˜å‚¨çš„éƒ½æ˜¯ byte è¿™æ ·ä¸€ä¸ªä¸€ä¸ªäºŒè¿›åˆ¶æ•°ï¼Œæ‰€ä»¥åœ¨å°† str å­˜å…¥æ–‡ä»¶æˆ–ä»æ–‡ä»¶è¯»å–å†…å®¹æ—¶éœ€è¦æŒ‡æ˜ç¼–ç ç±»å‹ã€‚\nç»éªŒæ€»ç»“  å†™ python ç¨‹åºçš„æ—¶å€™ï¼ŒæŠŠç¼–ç å’Œè§£ç æ“ä½œæ”¾åœ¨ç•Œé¢çš„æœ€å¤–å›´æ¥åšï¼Œç¨‹åºçš„æ ¸å¿ƒéƒ¨åˆ†ä½¿ç”¨ Unicode å­—ç¬¦ç±»å‹(Python3 ä¸­çš„ str)ã€‚\n   åœ¨å†™æ–‡ä»¶æ—¶æ³¨æ˜ç¼–ç ç±»å‹\nwith open(file_path, \u0026#39;w\u0026#39;,encode=\u0026#39;utf-8\u0026#39;) as f: f.write(data)   åœ¨ä½¿ç”¨ requests åº“è·å–å“åº”ä¿¡æ¯æ—¶ï¼ŒæŒ‡æ˜ç¼–ç ç±»å‹\ndef send_req(url): headers = {} req = requests.get(url, headers) req.encoding = \u0026#34;utf-8\u0026#34; if req.ok: print(req.text) return req.text else: return Exception(\u0026#34;è®¿é—®å¤±è´¥\u0026#34;)   base64 ç¼–ç \nBase64 æ˜¯ç½‘ç»œä¸Šæœ€å¸¸è§çš„ç”¨äºä¼ è¾“ 8Bit å­—èŠ‚ç çš„ç¼–ç æ–¹å¼ä¹‹ä¸€ï¼Œå¯ç”¨äºåœ¨ HTTP ç¯å¢ƒä¸‹ä¼ é€’è¾ƒé•¿çš„æ ‡è¯†ä¿¡æ¯ã€‚é‡‡ç”¨ Base64 ç¼–ç å…·æœ‰ä¸å¯è¯»æ€§ï¼Œéœ€è¦è§£ç åæ‰èƒ½é˜…è¯»ã€‚\nå°†æ–‡ä»¶è½¬æ¢æˆ base64 ç¼–ç å½¢å¼è¿›è¡Œä¼ è¾“ã€‚\nimport base64 with open(file_path, \u0026#39;rb\u0026#39;) as f: res = base64.b64encode(f.read())   python3 ä¸­ str å’Œ bytes äº’æ¢å‡½æ•°\ndef to_str(bytes_or_str): if isinstance(bytes_or_str, bytes): value = bytes_or_str.decode(\u0026#39;utf-8\u0026#39;) else: value = bytes_or_str return value def to_bytes(bytes_or_str): if isinstance(bytes_or_str, str): value = bytes_or_str.encode(\u0026#39;utf-8\u0026#39;) else: value = bytes_or_str return value   å‚è€ƒèµ„æ–™ï¼š\n base64 ç¼–ç  Effective+Python  ","date":"2020-03-06T00:00:00Z","permalink":"https://saltfishpr.github.io/p/python-encode/","title":"Python Encode"},{"content":"Markdown å¸¸ç”¨å†™æ³•å¤§å…¨ åˆ†å‰²çº¿ï¼š\n---  æ ‡é¢˜ï¼š\n# ä¸€çº§æ ‡é¢˜  ## äºŒçº§æ ‡é¢˜  ### ä¸‰çº§æ ‡é¢˜  ###### å…­çº§æ ‡é¢˜ ä¸€çº§æ ‡é¢˜ äºŒçº§æ ‡é¢˜ ä¸‰çº§æ ‡é¢˜ å…­çº§æ ‡é¢˜  æ–œä½“æ–‡å­—\n_æ–œä½“_ æ–œä½“\nåŠ ç²—æ–‡å­—\n**åŠ ç²—** åŠ ç²—\n æ®µè½å’Œæ¢è¡Œï¼š\næ–°ä¸€æ®µè½éœ€è¦ç©ºä¸€è¡Œ æ–°çš„ä¸€æ®µ æˆ–è€…åœ¨æœ€ååŠ ä¸Šä¸¤ä¸ªç©ºæ ¼ æ¢è¡Œ æ–°ä¸€æ®µè½éœ€è¦ç©ºä¸€è¡Œ\næ–°çš„ä¸€æ®µ\næˆ–è€…åœ¨æœ€ååŠ ä¸Šä¸¤ä¸ªç©ºæ ¼\næ¢è¡Œ\n åˆ—è¡¨ï¼š\n- åˆ—è¡¨ - åˆ—è¡¨ - åˆ—è¡¨ 1. åˆ—è¡¨ 2. åˆ—è¡¨ 3. åˆ—è¡¨  åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨   åˆ—è¡¨ åˆ—è¡¨ åˆ—è¡¨  åœ¨åˆ—è¡¨ä¸­åˆ†æ®µæˆ–è€…æ¢è¡Œè¦åœ¨åé¢çš„æ®µå’Œè¡Œä¸­åŠ ä¸Š 4 ä¸ªç©ºæ ¼\n- é¡¹ç›®ä¸€ï¼Œæ®µè½ä¸€ é¡¹ç›®ä¸€ï¼Œæ®µè½äºŒ - é¡¹ç›®äºŒï¼Œç¬¬ä¸€è¡Œ é¡¹ç›®äºŒï¼Œç¬¬äºŒè¡Œ   é¡¹ç›®ä¸€ï¼Œæ®µè½ä¸€\né¡¹ç›®ä¸€ï¼Œæ®µè½äºŒ\n  é¡¹ç›®äºŒï¼Œç¬¬ä¸€è¡Œ\né¡¹ç›®äºŒï¼Œç¬¬äºŒè¡Œ\n   å¼•ç”¨\n\u0026gt; æˆ‘äº¦é£˜é›¶ä¹…ï¼Œåå¹´æ¥ï¼Œæ·±æ©è´Ÿå°½ï¼Œæ­»ç”Ÿå¸ˆå‹ã€‚ â€”â€”é¡¾è´è§‚  æˆ‘äº¦é£˜é›¶ä¹…ï¼Œåå¹´æ¥ï¼Œæ·±æ©è´Ÿå°½ï¼Œæ­»ç”Ÿå¸ˆå‹ã€‚ â€”â€”é¡¾è´è§‚\n åœ¨å¼•ç”¨ä¸­å¼•ç”¨\n\u0026gt; å¼•ç”¨ 1 \u0026gt; \u0026gt; \u0026gt; å¼•ç”¨ 2  å¼•ç”¨ 1\n å¼•ç”¨ 2\n  åˆ†æ®µ\n\u0026gt; å¼•ç”¨ \u0026gt; \u0026gt; å¼•ç”¨  å¼•ç”¨\nå¼•ç”¨\n æ¢è¡Œ\n\u0026gt; å¼•ç”¨ \u0026gt; å¼•ç”¨  å¼•ç”¨\nå¼•ç”¨\n  é“¾æ¥\n### æ–‡å†…é“¾æ¥  è¿™æ˜¯ä¸€ä¸ªæ–‡å†…é“¾æ¥çš„[ä¾‹å­](http://example.com/ \u0026#34;é¼ æ ‡æ‚¬æµ®æ­¤å¤„æ˜¾ç¤ºçš„æ ‡é¢˜\u0026#34;)ã€‚ [è¿™ä¸ª](http://example.net/)é“¾æ¥åœ¨é¼ æ ‡æ‚¬æµ®æ—¶æ²¡æœ‰æ ‡é¢˜ã€‚ [è¿™ä¸ª](/about/)é“¾æ¥æ˜¯æœ¬åœ°èµ„æºã€‚ ### å¼•ç”¨é“¾æ¥  è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨é“¾æ¥çš„[ä¾‹å­][id]ã€‚ [id]: http://example.com/ \u0026#34;é¼ æ ‡æ‚¬æµ®æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰\u0026#34; ### æ³¨æ„ï¼Œè¿™é‡Œçš„ id æ²¡æœ‰å¤§å°å†™åŒºåˆ†ï¼Œå¦‚æœçœç•¥ idï¼Œåˆ™å‰é¢æ–¹æ‹¬å·çš„å†…å®¹ä¼šè¢«ç”¨ä½œ idã€‚  æˆ‘å¸¸ç”¨çš„ç½‘ç«™åŒ…æ‹¬[Google][1]ï¼Œ[Yahoo][2]å’Œ[MSN][3]ã€‚ [1]: http://google.com/ \u0026#34;Google\u0026#34; [2]: http://search.yahoo.com/ \u0026#34;Yahoo Search\u0026#34; [3]: http://search.msn.com/ \u0026#34;MSN Search\u0026#34; ### ä¹Ÿå¯ä»¥è¿™æ ·å†™ï¼ŒæŒ‰é¡ºåºå¡«å…¥æ¡†ä¸­  æˆ‘å¸¸ç”¨çš„ç½‘ç«™åŒ…æ‹¬[Google][]ï¼Œ[Yahoo][]å’Œ[MSN][]ã€‚ [google]: http://google.com/ \u0026#34;Google\u0026#34; [yahoo]: http://search.yahoo.com/ \u0026#34;Yahoo Search\u0026#34; [msn]: http://search.msn.com/ \u0026#34;MSN Search\u0026#34; æ–‡å†…é“¾æ¥ è¿™æ˜¯ä¸€ä¸ªæ–‡å†…é“¾æ¥çš„ä¾‹å­ã€‚\nè¿™ä¸ªé“¾æ¥åœ¨é¼ æ ‡æ‚¬æµ®æ—¶æ²¡æœ‰æ ‡é¢˜ã€‚\nè¿™ä¸ªé“¾æ¥æ˜¯æœ¬åœ°èµ„æºã€‚\nå¼•ç”¨é“¾æ¥ è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨é“¾æ¥çš„ä¾‹å­ã€‚\næ³¨æ„ï¼Œè¿™é‡Œçš„ id æ²¡æœ‰å¤§å°å†™åŒºåˆ†ï¼Œå¦‚æœçœç•¥ idï¼Œåˆ™å‰é¢æ–¹æ‹¬å·çš„å†…å®¹ä¼šè¢«ç”¨ä½œ idã€‚ æˆ‘å¸¸ç”¨çš„ç½‘ç«™åŒ…æ‹¬Googleï¼ŒYahooå’ŒMSNã€‚\nä¹Ÿå¯ä»¥è¿™æ ·å†™ï¼ŒæŒ‰é¡ºåºå¡«å…¥æ¡†ä¸­ æˆ‘å¸¸ç”¨çš„ç½‘ç«™åŒ…æ‹¬Googleï¼ŒYahooå’ŒMSNã€‚\n æ’å…¥å›¾ç‰‡\n### æ–¹æ³•ä¸€  ![åœ°é“](/img/post-bg-2015.jpg \u0026#34;æ‚¬æµ®æ˜¾ç¤ºçš„æç¤º\u0026#34;) ### æ–¹æ³•äºŒ  ![åœ°é“][base64str] [base64str]: data:image/jpg;base64,xxxxxxxxxxxxxxx... æ–¹æ³•ä¸€  åœ°é“ \næ–¹æ³•äºŒ  åœ°é“ \n ä»£ç \nprint(\u0026#39;Hello,World\u0026#39;) print('Hello,World')   é“¾æ¥\n\u0026lt;https://www.baidu.com/\u0026gt; https://www.baidu.com/\n åœ¨ä¸å¸Œæœ›ç¬¦å·è¢«å½“æˆ markdown æ ‡è¯†ç¬¦æ—¶ï¼Œç”¨\\è½¬ä¹‰\n\\_ä¸æ˜¯æ–œä½“\\_ _ä¸æ˜¯æ–œä½“_\nKarmdowm æ‰©å±• è¡¨æ ¼\n| å·¦å¯¹é½ | ä¸­é—´å¯¹é½ | å³å¯¹é½ | | :----- | :------: | -----: | | å·¦ 1 | ä¸­ 1 | å³ 1 | | å·¦ 2 | ä¸­ 2 | å³ 3 |    å·¦å¯¹é½ ä¸­é—´å¯¹é½ å³å¯¹é½     å·¦ 1 ä¸­ 1 å³ 1   å·¦ 2 ä¸­ 2 å³ 3     è§’æ ‡\nè¯·å‚é˜…è„šæ³¨ 1. [^1] [^1]: è„šæ³¨ 1 å†…å®¹ã€‚ # å‡ºç°åœ¨æ–‡æœ« è¯·å‚é˜…è„šæ³¨ 2. [^2] [^2]: è„šæ³¨ 2 å†…å®¹ã€‚ # å‡ºç°åœ¨æ–‡æœ« è¯·å‚é˜…è„šæ³¨ 1. 1\nè¯·å‚é˜…è„šæ³¨ 2. 2\n ç¼©å†™\nå­¦ä¹  GFMã€‚ \\*[GFM]: GitHub Flavored Markdown å­¦ä¹  GFMã€‚\n*[GFM]: GitHub Flavored Markdown\nGitHub Flavored Markdown æ‰©å±• ä»£ç é«˜äº®\n```python print(\u0026#39;Hello,World!\u0026#39;) ``` print(\u0026#39;Hello,World!\u0026#39;)  åˆ é™¤çº¿\n~~åˆ é™¤å†…å®¹~~ åˆ é™¤å†…å®¹\nHTML æ‰©å±• ä¸‹åˆ’çº¿\n\u0026lt;u\u0026gt;ä¸‹åˆ’å†…å®¹\u0026lt;/u\u0026gt; ä¸‹åˆ’å†…å®¹ ä¸Šæ ‡\nS = Ï€r\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt; S = Ï€r2 ä¸‹æ ‡\nWater: H\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt;O Water: H2O\n é¦–è¡Œç¼©è¿›\n\u0026amp;emsp;å¼€å§‹å†…å®¹ â€ƒå¼€å§‹å†…å®¹\n å†…éƒ¨è·³è½¬\nç‚¹æ­¤[æ ‡ç­¾](#j1)è·³è½¬ã€‚ \u0026lt;a name=\u0026#34;é”šç‚¹\u0026#34; id=\u0026#34;j1\u0026#34; href=\u0026#34;https://saltfishpr.github.io/\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; ç‚¹æ­¤æ ‡ç­¾è·³è½¬ã€‚ id è¦åŒ¹é…ï¼Œ name å’Œ href éƒ½ä¸æ˜¯å¿…é¡»çš„\n æ’å…¥è§†é¢‘\n\u0026lt;div\u0026gt; \u0026lt;a href=\u0026#34;//player.bilibili.com/player.html?aid=93178052\u0026amp;cid=159088790\u0026amp;page=1\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;å°é¢å›¾ç‰‡è·¯å¾„\u0026#34; alt=\u0026#34;æ²¡æœ‰å›¾ç‰‡æ—¶æ˜¾ç¤ºæ­¤æ–‡å­—\u0026#34; width=\u0026#34;100%\u0026#34; frameborder=\u0026#34;no\u0026#34; framespacing=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;true\u0026#34; /\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;script\u0026gt; var em = document.getElementById(\u0026#34;video-iframe\u0026#34;); console.log(em.clientWidth); em.height = em.clientWidth * 0.75 \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt;  æ³¨é‡Š\n[^_^]: # (æ³¨é‡Šï¼Œä¸ä¼šåœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤ºã€‚) \u0026lt;div style=\u0026#39;display: none\u0026#39;\u0026gt; æ³¨é‡Šå†…å®¹ \u0026lt;/div\u0026gt; \u0026lt;!-- å¤šæ®µ æ³¨é‡Šï¼Œ ä¸ä¼šåœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤ºã€‚ --\u0026gt;  å‚è€ƒèµ„æ–™ï¼š\n https://www.jianshu.com/p/d7d6da4b7c60#fnref1 https://guides.github.com/features/mastering-markdown/ emoji ç›®å½•    è„šæ³¨ 1 å†…å®¹ã€‚\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n è„šæ³¨ 2 å†…å®¹ã€‚\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"2020-03-05T00:00:00Z","image":"https://saltfishpr.github.io/p/markdown-cheat-sheet/cover_hua7319bcb3da2aa1454b00ba432a7b2bd_509084_120x120_fill_box_smart1_3.png","permalink":"https://saltfishpr.github.io/p/markdown-cheat-sheet/","title":"Markdown Cheat Sheet"},{"content":"æ¨é€å¥½å‹çš„ b ç«™è§†é¢‘ï¼Œé¡ºä¾¿æµ‹è¯•åµŒå…¥å¼ç½‘é¡µï¼ˆç‹—å¤´)\n\r","date":"2020-03-04T00:00:00Z","permalink":"https://saltfishpr.github.io/p/%E5%88%B6%E4%BD%9C%E9%9B%B7%E8%9B%87%E9%BC%A0%E6%A0%87%E5%AE%8F/","title":"åˆ¶ä½œé›·è›‡é¼ æ ‡å®"},{"content":"This is the beginning of my blog.\n","date":"2020-03-03T00:00:00Z","permalink":"https://saltfishpr.github.io/p/hello-blog/","title":"Hello Blog"}]